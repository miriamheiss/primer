[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preceptor’s Primer for Bayesian Data Science: Using the Cardinal Virtues for Inference",
    "section": "",
    "text": "Welcome\n\n\n\n\n\n\n\n \nThis isn’t the book you’re looking for.\n\nFirst, the book is for students in my classes. Everything about the book is designed to make the experience of those students better. I hope that some of the material here may be useful to people outside of this class.\nSecond, the book changes all the time. It is as up-to-date as possible.\nThird, I am highly opinionated about what matters and what does not. You might not share my views."
  },
  {
    "objectID": "preamble.html#dedication",
    "href": "preamble.html#dedication",
    "title": "Preamble",
    "section": "Dedication",
    "text": "Dedication\n\n\n\n\n\nAnd what is romantic, Kay —\nAnd what is love?\nNeed we ask anyone to tell us these things?"
  },
  {
    "objectID": "preamble.html#acknowledgements",
    "href": "preamble.html#acknowledgements",
    "title": "Preamble",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work builds on the contributions of many people in the R and Open Source communities. In particular, I would like to acknowledge extensive material taken from Diez, Barr, and Çetinkaya-Rundel (2014), Grolemund and Wickham (2017), Irizarry (2019), Kim and Ismay (2019), Bryan (2019), Diez, Barr, and Çetinkaya-Rundel (2014), Downey (2012), Grolemund and Wickham (2017), Kuhn and Silge (2020), Timbers, Campbell, and Lee (2021), and Legler and Roback (2019).\nAlboukadel Kassambara, Andrew Tran, Thomas Mock and others kindly allowed for the re-use and/or modification of their work.\n\n\n\n\n\n\n\n\nThanks to contributions from students and colleagues at Harvard and elsewhere, as well as from random people I met on the internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, and Stephanie Yao.\n\n\n\n\n\n\n\n\nAlso, Becca Gill, Ajay Malik, Heather Li, Nosa Lawani, Stephanie Saab, Nuo Wen Lei, Anmay Gupta and Dario Anaya.\n\n\n\n\n\n\n\n\nAlso, Kevin Xu, Anmay Gupta, Sophia Zhu, Arghayan Jeiyasarangkan, Yuhan Wu, Ryan Southward, George Pentchev, Ahmet Atilla Colak, Mahima Malhotra, and Shreeram Patkar.\n\n\n\n\n\n\n\n\nAlso, Tejas Mundhe, Jackson Roe, Varun Dommeti, Soham Gunturu and Felix Cai.\nI would like to gratefully acknowledge funding from The Derek Bok Center for Teaching and Learning at Harvard University, via its Digital Teaching Fellows and Learning Lab Undergraduate Fellows programs.\n\n\n\n\n\nDavid Kane"
  },
  {
    "objectID": "preamble.html#license",
    "href": "preamble.html#license",
    "title": "Preamble",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\n\nBryan, Jenny. 2019. STAT 545: Data Wrangling, Exploration, and Analysis with r. https://stat545.com/.\n\n\nDiez, David M, Christopher D Barr, and Mine Çetinkaya-Rundel. 2014. Introductory Statistics with Randomization and Simulation. First. Scotts Valley, CA: CreateSpace Independent Publishing Platform. https://www.openintro.org/stat/textbook.php?stat_book=isrs.\n\n\nDowney, Allen. 2012. Think Bayes: Bayesian Statistics Made Simple. Green Tea Press.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. First. Sebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science: Data Analysis and Prediction Algorithms with r. First. Boca Raton, FL: CRC Press.\n\n\nKim, Albert Y., and Chester Ismay. 2019. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. First. Boca Raton, FL: CRC Press.\n\n\nKuhn, Max, and Julia Silge. 2020. Tidy Modeling with r.\n\n\nLegler, Julie, and Paul Roback. 2019. Broadening Your Statistical Horizons: Generalized Linear Models and Multilevel Models.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2021. Data Science: A First Introduction. https://ubc-dsci.github.io/introduction-to-datascience/."
  },
  {
    "objectID": "00-getting-started.html#summary",
    "href": "00-getting-started.html#summary",
    "title": "Getting Started",
    "section": "Summary",
    "text": "Summary\nYou should have done the following:\n\nInstalled the latest versions of R and RStudio.\nRun, from the Console:\n\n\noptions(pkgType = \"binary\")\n\n\nInstalled, from Github, the all.primer.tutorials package:\n\n\nremotes::install_github(\"PPBDS/all.primer.tutorials\")\n\n\nRun this command to set some RStudio options:\n\n\nall.primer.tutorials::prep_rstudio_settings()\n\n\nLearned some basic terminology that will help you when you read the rest of the Primer.\n\nLet’s get started."
  },
  {
    "objectID": "01-visualization.html#looking-at-data",
    "href": "01-visualization.html#looking-at-data",
    "title": "1  Visualization",
    "section": "\n1.1 Looking at data",
    "text": "1.1 Looking at data\nThis chapter focuses on ggplot2, one of the core packages in the Tidyverse, a collection of 25 or so packages which have a similar design philosophy. The tidyverse (note the small “t”) is an R package that contains the 9 key packages, most importantly ggplot2. In other words, there are two separate things called “tidyverse”: a capital “T” Tidyverse which refers to many packages which work well together and a small “t” tidyverse which is an R package which loads the key Tidyverse packages.\nTo access the datasets, help pages, and functions that we will use in this chapter, load the tidyverse:\n\nlibrary(tidyverse)\n\nThat one line of code loads all the packages associated with the tidyverse, packages which you will use in almost every data analysis. The first time you load the tidyverse, R will report which functions from the tidyverse conflict with functions in base R or with other packages you may have loaded. (We hide these and other messages in this book because they are ugly.)\n\nYou might get this error message:\nError in library(tidyverse) : there is no package called ‘tidyverse’\nIf that happens, you need to install the package:\n\ninstall.packages(\"tidyverse\")\n\nThen, run library(tidyverse) again.\n\n1.1.1 Examining trains\n\nMost data comes to us in “spreadsheet”-type format. These datasets are called data frames or tibbles in R. Let’s explore the trains tibble from the primer.data package. This data comes from Enos (2014), which investigated attitudes toward immigration among Boston commuters.\n\nlibrary(primer.data)\ntrains\n\n# A tibble: 115 × 14\n   treat…¹ att_s…² att_end gender race  liberal party   age income line  station\n   <fct>     <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n 1 Treated      11      11 Female White FALSE   Demo…    31 135000 Fram… Grafton\n 2 Treated       9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n 3 Treated       3       5 Male   White TRUE    Demo…    63 135000 Fram… Grafton\n 4 Treated      11      11 Male   White FALSE   Demo…    45 300000 Fram… Grafton\n 5 Control       8       5 Male   White TRUE    Demo…    55 135000 Fram… Grafton\n 6 Treated      13      13 Female White FALSE   Demo…    37  87500 Fram… Grafton\n 7 Control      13      13 Female White FALSE   Repu…    53  87500 Fram… Grafton\n 8 Treated      10      11 Male   White FALSE   Demo…    36 135000 Fram… Grafton\n 9 Control      12      12 Female White FALSE   Demo…    54 105000 Fram… Grafton\n10 Treated       9      10 Male   White FALSE   Repu…    42 135000 Fram… Grafton\n# … with 105 more rows, 3 more variables: hisp_perc <dbl>,\n#   ideology_start <int>, ideology_end <int>, and abbreviated variable names\n#   ¹​treatment, ²​att_start\n\n\nLet’s unpack this output:\n\nA tibble is a specific kind of data frame. This particular data frame has 115 rows corresponding to different units, meaning people in this case.\nThe tibble also has 14 columns corresponding to variables which describe each unit or observation.\nWe see, by default, the top 10 rows and some of the columns. You can see more (or fewer) rows and columns by using the print() command:\n\n\nprint(trains, n = 15, width = 100)\n\n# A tibble: 115 × 14\n   treatment att_start att_end gender race  liberal party        age income\n   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl>\n 1 Treated          11      11 Female White FALSE   Democrat      31 135000\n 2 Treated           9      10 Female White FALSE   Republican    34 105000\n 3 Treated           3       5 Male   White TRUE    Democrat      63 135000\n 4 Treated          11      11 Male   White FALSE   Democrat      45 300000\n 5 Control           8       5 Male   White TRUE    Democrat      55 135000\n 6 Treated          13      13 Female White FALSE   Democrat      37  87500\n 7 Control          13      13 Female White FALSE   Republican    53  87500\n 8 Treated          10      11 Male   White FALSE   Democrat      36 135000\n 9 Control          12      12 Female White FALSE   Democrat      54 105000\n10 Treated           9      10 Male   White FALSE   Republican    42 135000\n11 Control          10       9 Female White FALSE   Democrat      33 105000\n12 Treated          11       9 Male   White FALSE   Democrat      50 250000\n13 Treated          13      13 Male   White FALSE   Republican    24 105000\n14 Control           6       7 Male   White TRUE    Democrat      40  62500\n15 Control           8       8 Male   White TRUE    Democrat      53 300000\n   line       station      hisp_perc ideology_start ideology_end\n   <chr>      <chr>            <dbl>          <int>        <int>\n 1 Framingham Grafton         0.0264              3            3\n 2 Framingham Southborough    0.0154              4            4\n 3 Framingham Grafton         0.0191              1            2\n 4 Framingham Grafton         0.0191              4            4\n 5 Framingham Grafton         0.0191              2            2\n 6 Framingham Grafton         0.0231              5            5\n 7 Framingham Grafton         0.0304              5            5\n 8 Framingham Grafton         0.0247              4            4\n 9 Framingham Grafton         0.0247              4            3\n10 Framingham Grafton         0.0259              4            4\n11 Framingham Grafton         0.0259              3            3\n12 Framingham Grafton         0.0259              5            4\n13 Framingham Grafton         0.0159              4            4\n14 Framingham Grafton         0.0159              1            1\n15 Framingham Southborough    0.0392              2            2\n# … with 100 more rows\n\n\nThe n argument to print() tells R the number of rows you want to see. width refers to the number of characters to print across the screen. Want to see every row and every column? Try:\n\nprint(trains, n = Inf, width = Inf)\n\nInf is an R object which means infinity.\n\n1.1.2 Exploring tibbles\nThere are many ways to get a feel for the data contained in a tibble.\n\n1.1.2.1 view()\n\nRun view(trains) in the Console in RStudio. Explore this tibble in the resulting pop up viewer.\nObserve that there are many different types of variables. Some of the variables are quantitative. These variables are numerical in nature. Other variables here, including gender and treatment, are categorical. Categorical variables take on one of a limited set of possible values.\n\n1.1.2.2 glimpse()\n\nWe can also explore a tibble by using glimpse().\n\nglimpse(trains)\n\nRows: 115\nColumns: 14\n$ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n$ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n$ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n$ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n$ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n$ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n$ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n$ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n$ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n$ hisp_perc      <dbl> 0.0264, 0.0154, 0.0191, 0.0191, 0.0191, 0.0231, 0.0304,…\n$ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n$ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\n\n\nWe see the first few values for each variable in a row after the variable name. In addition, the data type of the variable is given immediately after each variable’s name, inside < >.\ndbl refers to “double”, which is computer terminology for quantitative/numerical variables. int is for “integer.” fct is refers to a “factor,” a variable use for catagorical or nominal data. chr is for character data.\n\n1.1.2.3 summary()\n\nUsesummary() to get a sense of the distribution of the variables in the tibble.\n\nsummary(trains)\n\n   treatment    att_start         att_end          gender         \n Treated:51   Min.   : 3.000   Min.   : 3.000   Length:115        \n Control:64   1st Qu.: 7.000   1st Qu.: 7.000   Class :character  \n              Median : 9.000   Median : 9.000   Mode  :character  \n              Mean   : 9.191   Mean   : 9.139                     \n              3rd Qu.:11.000   3rd Qu.:11.000                     \n              Max.   :15.000   Max.   :15.000                     \n                                                                  \n     race            liberal           party                age       \n Length:115         Mode :logical   Length:115         Min.   :20.00  \n Class :character   FALSE:64        Class :character   1st Qu.:33.00  \n Mode  :character   TRUE :51        Mode  :character   Median :43.00  \n                                                       Mean   :42.37  \n                                                       3rd Qu.:52.00  \n                                                       Max.   :68.00  \n                                                                      \n     income           line             station            hisp_perc      \n Min.   : 23500   Length:115         Length:115         Min.   :0.01210  \n 1st Qu.: 87500   Class :character   Class :character   1st Qu.:0.01960  \n Median :135000   Mode  :character   Mode  :character   Median :0.03000  \n Mean   :141813                                         Mean   :0.03821  \n 3rd Qu.:135000                                         3rd Qu.:0.04290  \n Max.   :300000                                         Max.   :0.25940  \n                                                        NA's   :1        \n ideology_start   ideology_end \n Min.   :1.000   Min.   :1.00  \n 1st Qu.:2.000   1st Qu.:2.00  \n Median :3.000   Median :3.00  \n Mean   :2.757   Mean   :2.73  \n 3rd Qu.:4.000   3rd Qu.:3.50  \n Max.   :5.000   Max.   :5.00"
  },
  {
    "objectID": "01-visualization.html#basic-plots",
    "href": "01-visualization.html#basic-plots",
    "title": "1  Visualization",
    "section": "\n1.2 Basic Plots",
    "text": "1.2 Basic Plots\nThere are three essential components to a plot:\n\n\ndata: the dataset containing the variables of interest.\n\ngeom: the geometric object to display, e.g., scatterplot, line, bar.\n\naes: aesthetic attributes of the geometric object. The most important are the names of the variables that should be on the x and y axes. Additional attributes include color and size. Aesthetic attributes are mapped to variables in the dataset.\n\nConsider a basic scatterplot using data from Enos (2014) for 115 Boston commuters.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\n\n\n\n\nNotice how data and aes are specified in the call to ggplot(), followed by our choice of geom.\nPlots are composed of layers, combined using the + sign. The most essential layer specifies which type of geometric object we want the plot to use. In our graph above, the geom we used is geom_point().\nThe + sign comes at the end of the code line and not at the beginning. When adding layers to a plot, start a new line after the + so that the code for each layer is on a new line.\n\n1.2.1 geom_point()\n\nScatterplots, also called bivariate plots, allow you to visualize the relationship between two numerical variables.\nRecall our scatterplot from above.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\n\n\n\n\nLet’s break down this code, piece-by-piece.\n\nThe data argument is set to trains via data = trains.\nThe aesthetic mapping is set via mapping = aes(x = age, y = income). Here, we map age to the x axis and income to the y axis.\nThe geometric object is specified using geom_point(), telling R we want a scatterplot. We added a layer using the + sign.\n\nIf we do not specify the geometric object, we have a blank plot:\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income))\n\n\n\n\nIn addition to mapping variables to the x and y axes, we can also map variables to color.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point()\n\n\n\n\n\nWe use the function labs() to add a plot title, axis labels, subtitles, and captions to our graph. By default, R simply uses the names of variables for axes and legends. Add better titles and labels.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nNote that, as with geom’s, we add a layer using + when creating labs()for our plot. In general, every plot should give a title and axes labels. You should also add a subtitle, the purpose of which is to give a short “main point” of the graphic. What do you want the viewer to notice? You should also provide the source for the data, usually via the caption argument.\nLet’s now take a tour of some of the more useful geoms.\n\n1.2.2 Color and Fill\n\nBefore we take a look at the other useful geoms, let’s first talk about color and fill. These are two arguments we can use to change the color of geoms we have.\nLet’s take a look at this plot below.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nNotice that the plot is the exact same as the plot in the previous section, with one key change: we’ve added colors that separate the commuters by party. How did we do this? You’ll notice in the mapping = aes() function, we’ve added another argument. This argument, color =, can change the color of the dots. In this case, we changed the color by party.\nYou can also change the dots to all being a different.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point(color = \"steelblue\") +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nBecause we set the color argument in the call to geom_point() directly, all the points are steel blue. (There are hundreds of other colors we could have chosen.) Note the difference between setting color as an aesthetic (with aes()) and as an argument to geom_point().\nThere is another argument you can use, called fill. The key difference between fill and color is that fill defines the color with which a geom is filled, whereas color sets the outline of a geom.\n\nggplot(data = trains,\n       mapping = aes(x = race)) +\n  geom_bar(color = \"pink\")\n\n\n\n\nYou can see here that color has only changed the very outline of the bars. If we used fill:\n\nggplot(data = trains,\n       mapping = aes(x = race)) +\n  geom_bar(fill = \"pink\")\n\n\n\n\n\n1.2.3 geom_jitter()\n\nConsider a different scatter plot using the trains data.\n\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nThe problem with this display is “overplotting.” Because attitudes are measured as integers, we do not know if a given point represents just one person or a dozen. There are two methods we can use to address overplotting: transparency and jitter.\nMethod 1: Changing the transparency\nWe can change the transparency/opacity of the points by using the alpha argument within geom_point(). The alpha argument can be set to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1.\n\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point(alpha = 0.2) +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nNote that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but only changing the default setting of alpha.\nMethod 2: Jittering the points\nWe can also decide to jitter the points on the plot. We do this by replacing geom_point() with geom_jitter(). Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged.\nIn order to specify how much jitter to add, we use the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. It is important to add just enough jitter to break any overlap in the points, but not to the extent where you alter the original pattern.\n\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_jitter() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nWhen deciding whether to jitter a scatterplot or use the alpha argument to geom_point(), know that there is no single right answer. We suggest you play around with both methods to see which one better emphasizes the point you are trying to make.\n\n1.2.4 geom_line()\n\nLinegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory, predictive, or independent variable, is of a sequential nature. In other words, there is an inherent ordering to the variable.\n\n\n\n\n\nThe most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots.\nLet’s plot the median duration of unemployment in the United States over the last 50 years.\n\nggplot(data = economics,\n       mapping = aes(x = date, y = uempmed)) +\n  geom_line() +\n  labs(title = \"Unemployment Duration in the United States: 1965 -- 2015\",\n       subtitle = \"Dramatic increase in duration after the Great Recesssion\",\n       x = \"Date\",\n       y = \"Median Duration in Weeks\",\n       caption = \"Source: FRED Economic Data\")\n\n\n\n\nAlmost every aspect of the code used to create this plot is identical to our scatter plots, except for the geom we used.\n\n1.2.5 geom_histogram()\n\n\nA histogram is a plot that visualizes the distribution of a numerical value.\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nWe draw a bar whose height indicates the corresponding count.\n\nLet’s consider the income variable from the the trains tibble. Pay attention to how we have changed the two arguments to ggplot(). We have removed data = and mapping =. The code still works because R functions allow for passing in arguments by position. The first argument to ggplot() is the data. We don’t need to tell R that trains is the value for data. R assumes that it is because we passed it in as the first argument. Similarly, the second argument to ggplot() is mapping, so R assumes that aes(x = income) is the value we want for mapping because it is the second item passed in.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote the message printed above:\n\nstat_bin() using bins = 30. Pick better value with binwidth.\n\nYou would get the same message if you ran this code yourself. Try it!\nThe message is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is the default value. Unless you override this default number of bins with a number you specify, R will choose 30 by default. Because this is an important aspect of making a histogram, R insists on informing you with this message. You make this message go away by specifying the bin number yourself, as you should always do.\nLet’s specify bins and also add some labels.\n\n# Note that we can use either single or double quotes when creating items like\n# titles and axis labels. In this case, we use single quotes for the subtitle\n# because we want to put the word middle to print within double quotes in the\n# actual plot.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = 'Why are there so few people with \"middle\" incomes?',\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nUnlike scatterplots and linegraphs, there is now only one variable being mapped in aes(). Here, that variable is income. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram().\nWe can use the fill argument to change the color of the actual bins. Let’s set fill to “steelblue”.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = 'Why are there so few people with \"middle\" incomes?',\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nWe can also adjust the number of bins in our histogram in one of two ways:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nIn this data, however, there are not many unique values for income, so neither approach will have much effect. Replace income with age if you want to experiment with these options.\n\n1.2.6 geom_bar()\n\ngeom_bar() visualizes the distribution of a categorical variable. This is a simpler task than creating a histogram, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot.\n\nggplot(data = trains, \n       mapping = aes(x = race)) +\n  geom_bar()\n\n\n\n\n\n1.2.6.1 Two categorical variables\nAnother use of barplots is to visualize the joint distribution of two categorical variables. (See Chapter 5 for the definition of a joint distribution.) Let’s look at race, as well as treatment, in the trains data by using the fill argument inside the aes() aesthetic mapping. Recall the fill aesthetic corresponds to the color used to fill the bars.\n\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar()\n\n\n\n\nThis is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of people of different races within each region.\nAn alternative to stacked barplots are side-by-side barplots, also known as dodged barplots. The code to create a side-by-side barplot includes a position = \"dodge\" argument added inside geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead.\n\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nWhites are over-represented in the Control group even though the treatment was assigned at random.\n\n1.2.7 geom_col()\n\ngeom_col() is similar to geom_bar(), except that geom_col() requires you to calculate the number of observations in each category ahead of time. geom_bar() does the calculation for you. See an example below.\n\ntrains |> \n  summarize(count = n(),\n            .by = c(race, treatment)) |> \n  ggplot(mapping = aes(x = race,\n                       y = count,\n                       fill = treatment)) +\n   geom_col(position = \"dodge\")\n\n\n\n\nYou will learn to filter the data later in this chapter. However, what is key here is there was a y-variable supplied in geom_col() but there was not in geom_bar(). geom_col() gives you more control over the data that is being presented compared to geom_bar(), which may come in useful in some circumstances.\n\n1.2.7.1 No pie charts!\nOne of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. Robbins (2013) argues that we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another. Do not use pie charts.\n\n1.2.8 geom_smooth()\n\nWe can add trend lines to the plots we create using the geom_smooth() function.\nRecall the following scatterplot from our previous work.\n\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nWe can add a trend line to our graph by adding the layer geom_smooth(). Including trend lines allow us to visualize the relationship between att_start and att_end.\n\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nNote the message. R is telling us that we need to specify the method and formula argument, just the way it told us to provide the bins argument when we used geom_histogram() before.\nLet’s add the argument method = \"lm\", where “lm” stands for linear model. This causes the fitted line to be straight rather than curved. Let’s also add the argument formula = y ~ x. We make the argument y ~ x since R doesn’t know what model it is estimating, and gives us a warning that y is a function of x. By specifying this relationship, the warning disappears as R is sure of the model it is estimating. Again, R was not giving us an error before. It was simply telling us what options it was using since we did not specify the options ourselves.\nAlways include enough detail in your code to make those messages disappear.\n\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x)\n\n\n\n\nNotice the gray section surrounding the line we plotted. This area is called the confidence interval, which is set to 95% by default. We will learn about confidence intervals in Chapter 5. You can make the shaded area disappear by adding se = FALSE as another argument to geom_smooth().\n\n1.2.9 geom_density()\n\nRecall our plot from the geom_histogram() section.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = 'Why are there so few people with \"middle\" incomes?',\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nChange geom_histogram() to geom_density() to make a density plot, which is a smoothed version of the histogram.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_density() +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = 'Why are there so few people with \"middle\" incomes?',\n       x = \"Income\",\n       y = NULL,\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nThe values on the y-axis are scaled so that the total area under the curve equals one.\n\n1.2.10 Conclusion\nThe geoms we talked about are some of the most commonly used ones while using R. However, there are an endless list of options for you to pick from. We’ve listed a couple more here, however, feel free to check out this documentation here if you want to learn more!\n\n\ngeom_boxplot(): this creates a box and whiskers plot, which visualizes 5 summary statistics.\n\ngeom_dotplot(): this is similar to a bar graph, except with stacked dots on top of each other rather than bars.\n\ngeom_map(): this is a geom option you have to turn polygons to an actual map. However, there is another method that we will teach you in Maps that also allows you to create maps.\n\ngeom_text(): this geom allows you to add text onto your plots. We will use an example of this in the Visualization Case Studies tutorial!"
  },
  {
    "objectID": "01-visualization.html#tidyverse",
    "href": "01-visualization.html#tidyverse",
    "title": "1  Visualization",
    "section": "\n1.3 Tidyverse",
    "text": "1.3 Tidyverse\nGoing forward, most ggplot() code will omit the data = and mapping = explicit naming of arguments while relying on the default ordering. Most of the time, we include argument names and, as a rule, you should to. But we create so many plots in The Primer that these omissions are unlikely to cause problems.\n\n1.3.1 Data wrangling\nWe can’t use all the beautiful plots that we learned in the previous chapter until we have “wrangled” the data into a convenient shape. Key wrangling functions include:\n\nfilter(): to pick out the rows we want to keep from a tibble.\nselect(): to pick out the columns we want to keep from a tibble.\narrange(): to sort the rows in a tibble, in either ascending or descending order.\nmutate(): to create new columns.\nsummarize(): to create a new tibble comprised of summary statistics for one (or more) rows, depending on the use of the .by argument.\n\n1.3.2 The pipe operator: |>\n\nThe pipe operator (|>) allows us to combine multiple operations in R into a single sequential chain of actions. Much like how the + sign has to come at the end of the line when constructing plots — because we are building the plot layer-by-layer — the pipe operator |> has to come at the end of the line because we are building a data wrangling pipeline step-by-step. If you do not include the pipe operator, R assumes the next line of code is unrelated to the layers you built and you will get an error.\n\n1.3.3 filter() rows\n\n\n\n\nfilter() reduces the rows in a tibble.\n\n\n\n\nPlease read the filter section from R for Data Science (2e).\n\n1.3.4 select variables\n\n\n\n\nselect() reduces the number of columns in a tibble.\n\n\n\n\nUsing the filter() function we were able to pick out specific rows (observations) from the tibble. The select() function allows us to pick specific columns (variables) instead.\nPlease read the select section from R for Data Science (2e).\n\n1.3.5 arrange() rows\narrange() allows us to sort/reorder a tibble’s rows according to the values of a specific variable. Unlike filter() or select(), arrange() does not remove any rows or columns from the tibble.\nPlease read the arrange section from R for Data Science (2e).\n\n\n1.3.6 mutate() variables\n\n\n\n\n`mutate() adds a column to a tibble.\n\n\n\n\nmutate() takes existing columns and creates a new column.\nPlease read the mutate section from R for Data Science (2e).\n\n1.3.6.1 if_else()\n\nif_else() is often used within calls to mutate(). It has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE.\nImagine that we want to create a new variable old, which is TRUE when age > 50 and FALSE otherwise.\n\ntrains |> \n  select(age) |> \n  mutate(old = if_else(age > 50, TRUE, FALSE))\n\n# A tibble: 115 × 2\n     age old  \n   <int> <lgl>\n 1    31 FALSE\n 2    34 FALSE\n 3    63 TRUE \n 4    45 FALSE\n 5    55 TRUE \n 6    37 FALSE\n 7    53 TRUE \n 8    36 FALSE\n 9    54 TRUE \n10    42 FALSE\n# … with 105 more rows\n\n\nAnother function similar to if_else(), is dplyr::case_when(). case_when() is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Note that there is a different version of if_else() in base R: ifelse(). This works exactly the same as the dplyr version but is somewhat less robust. For this reason, we prefer the if_else() version.\n\n1.3.7 summarize() tibbles\nWe often need to calculate summary statistics, things like the mean (also called the average) and the median (the middle value). Other examples of summary statistics include the sum, the minimum, the maximum, and the standard deviation.\nThe function summarize() allows us to calculate these statistics on individual columns from a tibble. Example:\n\ntrains |> \n  summarize(mn_age = mean(age), \n            sd_age = sd(age))\n\n# A tibble: 1 × 2\n  mn_age sd_age\n   <dbl>  <dbl>\n1   42.4   12.2\n\n\nThe mean() and sd() summary functions go inside the summarize() function. The summarize() function takes in a tibble and returns a tibble with only one row corresponding to the summary statistics. Remember: Tibbles go in and tibbles come out.\nmean()\nThe mean, or average, is the most commonly reported measure of the center of a distribution. The mean is the sum of all of the data elements divided by the number of elements. If we have \\(N\\) data points, the mean is given by:\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_N}{N}\\]\nmedian()\nThe median is another commonly reported measure of the center of a distribution, calculated by first sorting the vector of values from smallest to largest. The middle element in the sorted list is the median. If the middle falls between two values, then the median is the mean of those two middle values. The median and the mean are the two most common measures of the center of a distribution. The median is more stable, less affected by outliers. There is no widely accepted symbol for the median, although \\(\\tilde{x}\\) is not uncommon. If we have \\(n\\) data points, and \\(n\\) is even, the median is given by:\n\\[m(x) = {\\frac{1}{2}}{(x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1})}\\] If \\(n\\) is odd, then the median is given by:\nsd()\nThe standard deviation (sd) of a distribution is a measure of its variation around the mean.\n\\[\\text{sd} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}}\\]\nmad()\nThe scaled median absolute deviation (mad) is a measure of variation around the median. It is not as popular as the standard deviation. The formula for calculating mad is a bit mysterious.\n\\[\\text{mad} = 1.4826 \\times \\text{median}(abs(x - \\tilde{x}))\\]\nThe basic idea for both sd and mad is that we need a measure of variation around the center of the distribution of the variable. sd uses the mean, \\(\\bar{x}\\), as its estimate of the center while mad uses the median, \\(\\tilde{x}\\). Because mad uses the absolute difference, as opposed to the squared difference, it is more robust to outliers. The 1.4826 multiplier causes the mad and the sd to be identical in the (important) case of standard normal distributions, a topic we will introduce in Chapter 2.\nquantile()\nThe quantile of a distribution is the value of that distribution which occupies a specific percentile location in the sorted list of values.\nI.e., the 5th percentile distribution is the point below which 5% of the data falls. The 95th percentile is, similarly, the point below which 95% of the data falls. The 50th percentile, the median, splits the data into two separate, and equal, parts. The minimum is at the 0th percentile. The maximum is at the 100th percentile.\nTherefore, the value of the 5th percentile would be the quantile of the 5th percentile. Say the dataset consisted of numbers from 0-100. Therefore, the 5th percentile would at 5, so 5 would be the quantile.\nLet’s take a look at the poverty variable in the kenya tibble from the primer.data package. poverty is the percentage of residents in each community with incomes below the poverty line. Let’s first confirm that quantile() works by comparing its output with that from simpler functions.\n\nc(min(kenya$poverty), median(kenya$poverty), max(kenya$poverty))\n\n[1] 0.1810048 0.4315779 0.8989494\n\nquantile(kenya$poverty, probs = c(0, 0.5, 1))\n\n       0%       50%      100% \n0.1810048 0.4315779 0.8989494 \n\n\nThe probs argument allows us to specify the percentile(s) we want. Two of the most important percentiles are the 2.5th and 97.5th because they define the 95% interval, a central range which includes 95% of the values.\n\nquantile(kenya$poverty, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.2225586 0.6588897 \n\n\nThe interval between these two percentiles includes 95% of all the values in the distribution. Depending on the context, this interval is sometimes called a “confidence interval” or “uncertainty interval” or “compatibility interval.” Different percentile ranges create intervals of different widths.\nIf there is an NA value in the variable, any statistical function like mean() will return NA. You can fix this by using na.rm = TRUE within the statistical function.\n\n1.3.7.1 Summaries by a group\nInstead of calculating statistics for the entire tibble, we often want to do so “by” group. For example, here is the mean age for both genders in the data.\n\ntrains |> \n  summarize(mn_age = mean(age),\n            .by = gender)\n\n# A tibble: 2 × 2\n  gender mn_age\n  <chr>   <dbl>\n1 Female   41.0\n2 Male     43.5\n\n\nThe .by argument can take more than one variable.\n\ntrains |> \n  summarize(mn_age = mean(age),\n            .by = c(gender, race))\n\n# A tibble: 8 × 3\n  gender race     mn_age\n  <chr>  <chr>     <dbl>\n1 Female White      40.7\n2 Male   White      43.8\n3 Female Asian      40.7\n4 Female Hispanic   43.5\n5 Male   Asian      39.7\n6 Male   Hispanic   44.7\n7 Female Black      44.5\n8 Male   Black      44  \n\n\nOlder code often uses group_by() to achieve the same result.\n\ntrains |> \n  group_by(gender) |> \n  summarize(mn_age = mean(age))\n\n# A tibble: 2 × 2\n  gender mn_age\n  <chr>   <dbl>\n1 Female   41.0\n2 Male     43.5\n\n\nDo not use group_by() unless you have an extremely good reason to do so. Use the .by argument, most commonly with `summarize(), although it also works with other Tidyverse functions.\n\n1.3.8 Other useful commands\n\nslice() returns specific rows from a tibble:\n\ntrains |> \n  slice(2:5)\n\n# A tibble: 4 × 14\n  treatm…¹ att_s…² att_end gender race  liberal party   age income line  station\n  <fct>      <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n1 Treated        9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n2 Treated        3       5 Male   White TRUE    Demo…    63 135000 Fram… Grafton\n3 Treated       11      11 Male   White FALSE   Demo…    45 300000 Fram… Grafton\n4 Control        8       5 Male   White TRUE    Demo…    55 135000 Fram… Grafton\n# … with 3 more variables: hisp_perc <dbl>, ideology_start <int>,\n#   ideology_end <int>, and abbreviated variable names ¹​treatment, ²​att_start\n\n\nUnlike filter(), slice() relies on the order that the rows are currently in. It is often handy to return a random sample of the rows:\n\ntrains |> \n  slice_sample(n = 4)\n\n# A tibble: 4 × 14\n  treatm…¹ att_s…² att_end gender race  liberal party   age income line  station\n  <fct>      <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n1 Control       11      11 Male   White FALSE   Repu…    42 135000 Fran… Norfolk\n2 Control       13      13 Male   Asian TRUE    Demo…    60 135000 Fran… Norwoo…\n3 Control        9       6 Male   White FALSE   Demo…    23 300000 Fran… Norwoo…\n4 Control       13       9 Male   White TRUE    Demo…    67 135000 Fram… Natick \n# … with 3 more variables: hisp_perc <dbl>, ideology_start <int>,\n#   ideology_end <int>, and abbreviated variable names ¹​treatment, ²​att_start\n\n\nWe can use [] — the “single bracket” operator — to pull out one or more columns, but the variables names must be quoted.\n\ntrains[\"age\"] |> \n  print(n = 3)\n\n# A tibble: 115 × 1\n    age\n  <int>\n1    31\n2    34\n3    63\n# … with 112 more rows\n\n\nAnd\n\ntrains[c(\"age\", \"party\")] |> \n  print(n = 3)\n\n# A tibble: 115 × 2\n    age party     \n  <int> <chr>     \n1    31 Democrat  \n2    34 Republican\n3    63 Democrat  \n# … with 112 more rows\n\n\nThis behavior is, more or less, the same as with select(). However, [] is part of “base R,” those parts of the R language which do not require extra packages like the Tidyverse. For the most part, you should use select().\nThe $ operator allows us to extract a single variable from a tibble and return it as a vector. If you need to do something like this within a pipe, use pull().\n\ntrains$age\n\n  [1] 31 34 63 45 55 37 53 36 54 42 33 50 24 40 53 50 33 33 32 57 41 36 43 25 41\n [26] 33 44 46 41 28 36 37 38 48 20 52 38 45 55 38 45 44 36 29 42 43 54 39 31 50\n [51] 60 67 54 44 50 20 57 25 60 44 35 54 52 47 60 47 22 56 50 21 29 45 46 42 23\n [76] 29 60 41 30 61 21 46 53 45 46 63 21 31 35 22 68 27 22 30 59 56 32 35 23 60\n[101] 50 31 43 30 54 52 52 50 37 27 55 42 68 52 50"
  },
  {
    "objectID": "01-visualization.html#advanced-plots",
    "href": "01-visualization.html#advanced-plots",
    "title": "1  Visualization",
    "section": "\n1.4 Advanced Plots",
    "text": "1.4 Advanced Plots\n\n\n\n\n\nGood visualizations teach. When you construct a plot, decide what message you want to convey. Here are some functions which may be helpful.\n\n1.4.1 Plot objects\nPlots are R objects, just like tibbles. We can create them, print them and save them. Up until now, we have just “spat” them out in an R code chunk. Nothing wrong with that! Indeed, this is the most common approach to plotting in R. Sometimes, however, it is handy to work with a plot object. Consider:\n\ntrain_plot <- ggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\n\nThis is the same code as our first example with geom_point(). train_plot is an R object. This code does not print anything out. In order to make this plot appear, we need to print it out explicitly:\n\ntrain_plot\n\n\n\n\nRecall that typing the name of an object is the same thing as using print(). Now that we have this object, we can display it whenever we want.\nBut, sometimes, we want a permanent copy of the plot, saved to our computer. That is the purpose of ggsave():\n\nggsave(filename = \"enos_trains.jpg\", \n       plot = train_plot)\n\nggsave() uses the suffix of the provided filename to determine the type of image to save. Because we use “enos_trains.jpg”, the file is saved in JPEG format. If we had used “enos_trains.png”, the file would have been saved as a PNG. We can display a saved file by using knitr::include_graphics(). For example:\n\nknitr::include_graphics(\"enos_trains.jpg\")\n\nThis code displays the image in an Rmd, assuming that the file “enos_trains.jpg” is located in the current working directory. A common scenario is that we create an image and store it in a directory named figures/ and then use that figure in more than one Rmd.\n\n1.4.2 Faceting\nFaceting splits a visualization into parts, one for each value of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose contents will differ.\nBefore we proceed, let’s create a subset of the tibble gapminder from the gapminder package. (You may need to install the gapminder package for this code to work. Refer back to the introduction if you need a refresher on how to do so.)\n\nlibrary(gapminder)\ngapminder_filt <- gapminder |> \n      filter(year == 2007, continent != \"Oceania\")\n\nLet’s plot our filtered data using geom_point()\n\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point()\n\n\n\n\nIt is difficult to compare the continents despite the colors. It would be much easier if we could “split” this scatterplot by the 4 continents. In other words, we would create plots of gdpPercap and lifeExp for each continent separately. We do this by using the function facet_wrap() with the argument ~ continent. In facet_wrap, you must always put the tilde (~) in front of the variable you wish to wrap it by.\n\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent)\n\n\n\n\nThis is much better! We can specify the number of rows and columns in the grid by using the nrow argument inside of facet_wrap(). Let’s get all continents in a row by setting nrow to 1. Let’s also add a trend line geom_smooth() to our faceted plot.\n\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x, \n              se = FALSE)\n\n\n\n\nAs expected, we can see a positive correlation between economic development and life expectancy on all continents.\n\n1.4.3 Stats\nConsider the following histogram.\n\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp))+ \n  geom_histogram(bins = 20, \n                 color = \"white\")\n\n\n\n\nRecall that the y-aesthetic of a histogram — the count of the observations in each bin — gets computed automatically. We can use the after_stat() argument within geom_histogram() to generate percent values as our y-aesthetic. after_stat() allows us to control the values of the variables calculated specifically for this specific aesthetic layer.\n\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 20) +\n  labs(y = \"Percentage\")\n\n\n\n\n\n1.4.4 Axis Limits and Scales\n\n1.4.4.1 coord_cartesian()\n\nWe can also manipulate the limits of the axes by using xlim() and ylim() within a call to coord_cartesian(). For example, assume that we are only interested in countries with a GDP per capita from 0 to 30,000. Recall that, because data is the first argument and mapping is the second to ggplot(), we don’t actually have to name the arguments. We can just provide them, as long as they are in the correct order.\n\nggplot(gapminder_filt, \n       aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  coord_cartesian(xlim = c(0, 30000))\n\n\n\n\nWe can see that the GDP per capita on the x-axis is now only shown from 0 to 30,000.\n\n1.4.4.2 scale_x and scale_y\n\nWe can also change the scaling of the axes. For example, it might be useful to display the axes on a logarithmic scale by using scale_x_log10() or scale_y_log10(). Also, note that we can (lazily!) not provide the explicit x and y argument names to aes() as long as we provide the values in the right order: x comes before y.\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10()\n\n\n\n\n\nBeyond scale_x_log10(), there are other ways to change the scales. We will cover scale_y_continuous and scale_x_continuous in this section.\nThere are two major uses for scale_x_continuous, and that is to change the breaks and the labels. Take the graph below.\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\n\n\n\n\nWe only want there to be breaks on the y-axis every 20 years instead of every 10. We also want to add dollar signs to the x-axis, and to have breaks every 20,000 dollars. Let’s fix the graph!\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) +\n  scale_y_continuous(breaks = c(40, 60, 80)) +\n  scale_x_continuous(labels = scales::dollar_format(),\n                     breaks = c(0, 20000, 40000))\n\n\n\n\nLet’s break this down. We used the breaks argument to create the breaks on the scale. We used the c() function to specify what breaks we wanted. Then, we used labels to modify the labels on the x-axis. The :: allows us to extract a function from a specific package, if the function exists in multiple packages. So, we specifically extract the dollar_format function from the scales package to change the labels of the x-axis.\nThere is another function called scale_x_discrete/scale_y_discrete. This function is similar enough to scale_y/x_continuous that we will not give it its own section. The only difference in the usage of the discrete vs. continuous function is that the discrete function is applied to discrete variables. Discrete variables are those that are countable (i.e. the number of tables in a room) with nothing in between, whereas continuous variables have infinite possibilities (i.e. height has an infinite number of possible values).\n\n1.4.5 Text\nRecall we use labs() to add labels and titles to our plots. We can also change labels inside the plots using geom_text().\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\") +\n  geom_text(aes(label = country), \n            size = 2, \n            color = \"black\", \n            check_overlap = TRUE)\n\n\n\n\nLet’s breakdown the code within geom_text(). We included a new aesthetic called label. This defines the character variable which will be used as the basis for the labels. We set label to country so each point corresponds to the country it represents. We set the text font by setting size to 2, and we set the text color by using color. Finally, we included the argument check_overlap = TRUE to make sure the names of the countries were legible.\n\n1.4.6 Themes\nThemes can be used to change the overall appearance of a plot without much effort. We add themes as layers to our plots. You can find an overview of the different themes in ggplot here.\nConsider the following faceted scatterplot.\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) \n\n\n\n\nNote the use of the breaks argument to scale_x_log10(). This specifies the location of labels on the x-axis. We can also use the labels argument if we want to change their appearence. These tricks work in the entire family of scale_* functions.\nLet’s now add a theme to our faceted scatterplot. We will use the theme theme_economist(), from the ggthemes package, to make our plot look like the plots in the The Economist.\n\nlibrary(ggthemes)\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) +\n  theme_economist()\n\n\n\n\nThis looks pretty good. However, notice the legend on the top of our graph. It crowds our graph and takes away from the most important part: the data. We can use theme() to customize the non-data parts of our plots such as background, gridlines, and legends. Let’s de-clutter the graph by removing our legend. We can do this by using the legend.position argument and setting it to “none”.\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000),\n                  labels = scales::dollar_format(accuracy = 1)) + \n    theme_economist() +\n    theme(legend.position = \"none\")\n\n\n\n\nGreat. Now our graph is easier to visualize.\n\n1.4.6.1 theme()\nThe theme() function also offers a wide selection of functions for manually changing individual elements. We will cover the most widely used ones here, but the vast majority will be listed in that link.\nThere are two key elements in the theme() function.\n\n\nTheme elements: these specify the non-data elements you can control. For example, panel.border is the element that controls the border of the grid area.\n\nElement function: this describes the visual properties of the element. There are four main element functions, element_blank(), element_rect(), element_line(), element_text().\n\n\nelement_blank(): this hides the element from the theme.\n\nelement_line(): this modifies elements that are plot lines, grid lines, axes, etc.\n\nelement_text(): this changes the text elements of the plot, like titles, captions, etc.\n\nelement_rect(): rectangle elements control the background of plots, legends, etc.\n\n\n\nLet’s look at an example of how we might want to modify a graph. Take this graph below.\n\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) +\n  geom_line() +\n  labs(title = \"Unemployed Population in the United States: 1965 - 2015\",\n       subtitle = \"Dramatic spike during the Great Recesssion\",\n       x = \"Year\",\n       y = \"Number of Unemployed (in thousands)\",\n       caption = \"Source: FRED Economic Data\")\n\n\n\n\nYou have a couple of problems with this graph. Firstly, you want the title to be bold. You want there to be a light blue background. You also don’t particularly care for there to be so many grid lines from the x-axis. Let’s change this!\n\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) +\n  geom_line() +\n  labs(title = \"Unemployed Population in the United States: 1965 - 2015\",\n       subtitle = \"Dramatic spike during the Great Recesssion\",\n       x = \"Year\",\n       y = \"Number of Unemployed (in thousands)\",\n       caption = \"Source: FRED Economic Data\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        panel.background = element_rect(fill = \"lightblue\"),\n        panel.grid.minor.x = element_blank())\n\n\n\n\nThere! Within the theme() function, we can change the plot title to bold, the background to blue, and get rid of the “minor” grid lines. theme() is a very versatile function. S"
  },
  {
    "objectID": "01-visualization.html#going-further",
    "href": "01-visualization.html#going-further",
    "title": "1  Visualization",
    "section": "\n1.5 Going further",
    "text": "1.5 Going further\nThere are so many more plots you can make with R that we have not shown you yet. For example, you can create cool animations with the gganimate package:\n\n\n\n\n\nYou can make your plots interactive, using the plotly package:\n\n\n\n\n\n\nYou can also create maps from census data, like this plot below."
  },
  {
    "objectID": "01-visualization.html#summary-1",
    "href": "01-visualization.html#summary-1",
    "title": "1  Visualization",
    "section": "\n1.6 Summary",
    "text": "1.6 Summary\nTibbles are rectangular stores of data. They are a specific type of data frame, so we will use both terms interchangeably.\nYou need to practice every day.\nDo not use pie charts.\nShield my eyes from your ugly messages and warning.\nEach step in the pipe starts with a tibble and then, once it is done, usually produces a tibble. It is tibbles all the way down!\nWhen your R code is behaving in a weird way, especially when it is “losing” rows, the problem is often solved by using ungroup() in the pipeline.\nThe two most important attributes of a distribution are its center and its variation around that center.\nIn this chapter, we first looked at basic coding terminology and concepts that we deal with when programming with R. We then learned about the three basic components that make up each plot: data, mapping, and one or more geoms. The ggplot2 package offers a wide range of geoms that we can use to create different types of plots. Next, we examined the “super package” tidyverse, which includes helpful tools for visualization. It also offers features for importing and manipulating data, which is the main topic of Chapter 2. Lastly, we explored advanced plotting features such as axis scaling, faceting, and themes.\nRecall the plot we began the chapter with:\n\n\n\n\n\nYou now know enough to make plots like this by yourself.\nA beautiful plot is just a collection of steps, each simple enough on its own. We have taught you (some of) these steps. Time to start walking on your own.\n\n\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on Exclusionary Attitudes.” Proceedings of the National Academy of Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111.\n\n\nRobbins, Naomi. 2013. Creating More Effective Graphs. First. New York, NY: Chart House."
  },
  {
    "objectID": "02-wrangling.html#tibbles",
    "href": "02-wrangling.html#tibbles",
    "title": "2  Wrangling",
    "section": "\n2.1 Tibbles",
    "text": "2.1 Tibbles\n\nTibbles are a kind of data frame, useful for storing data in which we have the same number of observations for each variable. We can use the tibble() function to create tibbles. Tibbles are composed of columns, each of which is a variable, and rows, each of which is a “unit” or an “observation.” Furthermore, each column (i.e., each variable) can be of a different type: character, integer, factor, double, date and so on.\n\n2.1.1 tibble()\n\n\ntibble(a = 2.1, b = \"Hello\", c = TRUE, d = 9L)\n\n# A tibble: 1 × 4\n      a b     c         d\n  <dbl> <chr> <lgl> <int>\n1   2.1 Hello TRUE      9\n\n\nIn our code, we specify the column names as , a, b, and c. Under each variable, we give a different value. Under each variable name, the data type is specified for the data within that column. A tibble can consist of one or more atomic vectors, the most important types of which are double, character, logical, and integer. The tibble above includes a variable of each type. The “L” in “9L” tells R that we want d to be an integer rather than the default, which would be a double. When you print out a tibble, the variable type is shown below the variable name, as indicated by <dbl>, <chr>, and so on.\nVariables should not begin with a number (like 54abc) or include spaces (like my var). If you insist on using variable names such, you must include backticks around each name when you reference it.\n\ntibble(`54abc` = 1, `my var` = 2, c = 3)\n\n# A tibble: 1 × 3\n  `54abc` `my var`     c\n    <dbl>    <dbl> <dbl>\n1       1        2     3\n\n\nIf we did not include the backticks, R would give us an error.\n\ntibble(54abc = 1, my var = 2, c = 3)\n\nError: <text>:1:10: unexpected symbol\n1: tibble(54abc\n             ^\n\n\nIn the real world, you may come across datasets with dirty column names such as 54abc or my var.\nIt is sometimes easier to use the function tribble() to create tibbles.\n\ntribble(\n  ~ var1, ~ `var 2`, ~ myvar,\n  1,           3,      5,\n  4,           6,      8,\n)\n\n# A tibble: 2 × 3\n   var1 `var 2` myvar\n  <dbl>   <dbl> <dbl>\n1     1       3     5\n2     4       6     8\n\n\nThe tildes — as in ~ var1 — specify which row has the column names. The formatting makes it easier, relative to specifying raw vectors, to see which values are from the same observation.\nas_tibble() is a handy function for transforming other R objects, especially matrices, into tibbles."
  },
  {
    "objectID": "02-wrangling.html#lists",
    "href": "02-wrangling.html#lists",
    "title": "2  Wrangling",
    "section": "\n2.2 Lists",
    "text": "2.2 Lists\n\n\n\n\n\nSubsetting a list, visually.\n\n\n\n\nEarlier, we briefly introduced lists. Lists are a type of vector that is a step up in complexity from atomic vectors, because lists can contain other lists. This makes them suitable for representing hierarchical or tree-like structures. You create a list with the function list():\n\nx <- list(1, 2, 3)\nx\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n\nA very useful tool for working with lists is str() because it focuses on displaying the structure, not the contents.\n\nstr(x)\n\nList of 3\n $ : num 1\n $ : num 2\n $ : num 3\n\nx_named <- list(a = 1, b = 2, c = 3)\nstr(x_named)\n\nList of 3\n $ a: num 1\n $ b: num 2\n $ c: num 3\n\n\nUnlike atomic vectors, list() can contain objects of several types.\n\ny <- list(\"a\", 1L, 1.5, TRUE)\nstr(y)\n\nList of 4\n $ : chr \"a\"\n $ : int 1\n $ : num 1.5\n $ : logi TRUE\n\n\nLists can even contain other lists!\n\nz <- list(list(1, 2), list(3, 4))\nstr(z)\n\nList of 2\n $ :List of 2\n  ..$ : num 1\n  ..$ : num 2\n $ :List of 2\n  ..$ : num 3\n  ..$ : num 4\n\n\n\n2.2.1 Visualizing lists\nTo explain more complicated list manipulation functions, it’s helpful to have a visual representation of lists. For example, take these three lists:\n\nx1 <- list(c(1, 2), c(3, 4))\nx2 <- list(list(1, 2), list(3, 4))\nx3 <- list(1, list(2, list(3)))\n\nThey are structured as follows:\n\n\n\n\n\nThere are three principles:\n\nLists have rounded corners. Atomic vectors have square corners.\nChildren are drawn inside their parent, and have a slightly darker background to make it easier to see the hierarchy.\nThe orientation of the children (i.e. rows or columns) isn’t important, so I’ll pick a row or column orientation to either save space or illustrate an important property in the example.\n\n2.2.2 Subsetting\nThere are three ways to subset a list, which I’ll illustrate with a list named a:\n\na <- list(a = 1:3, b = \"a string\", c = pi, d = list(-1, -5))\n\n[ ] extracts a sub-list. The result will always be a list.\n\nstr(a[1:2])\n\nList of 2\n $ a: int [1:3] 1 2 3\n $ b: chr \"a string\"\n\nstr(a[4])\n\nList of 1\n $ d:List of 2\n  ..$ : num -1\n  ..$ : num -5\n\n\nLike with vectors, you can subset with a logical, integer, or character vector.\n[[ ]] extracts a single component from a list. It removes a level of hierarchy from the list.\n\nstr(a[[1]])\n\n int [1:3] 1 2 3\n\nstr(a[[4]])\n\nList of 2\n $ : num -1\n $ : num -5\n\n\n$ is a shorthand for extracting named elements of a list. It works similarly to [[ ]] except that you don’t need to use quotes.\n\na$a\n\n[1] 1 2 3\n\na[[\"a\"]]\n\n[1] 1 2 3\n\n\nThe distinction between [ ] and [[ ]] is really important for lists, because [[ ]] drills down into the list while [ ] returns a new, smaller list. Compare the code and output above with the visual representation."
  },
  {
    "objectID": "02-wrangling.html#characters",
    "href": "02-wrangling.html#characters",
    "title": "2  Wrangling",
    "section": "\n2.3 Characters",
    "text": "2.3 Characters\n\n\n\n\nReal data is nasty.\n\n\n\n\n\nSo far, our tibbles have been clean and wholesome, like gapminder and trains. Real data is nasty. You will bring data into R from the outside world and discover there are problems. We will now discuss common remedial tasks for cleaning and transforming character data, also known as strings. A string is one or more characters that are enclosed inside a pair of matching ‘single’ or “double quotes”.\n\nWe will use the fruit data, a vector with the names of different fruits, from the stringr package, which is automatically loaded when we issue library(tidyverse). Although we can manipulate character vectors directly, it is much more common, in real world situations, to work with vectors which are in a tibble.\n\n\ntbl_fruit <- tibble(fruit = fruit)\n\n\n2.3.1 Character vectors\n\ntbl_fruit |> \n  slice_sample(n = 8)\n\n# A tibble: 8 × 1\n  fruit       \n  <chr>       \n1 grapefruit  \n2 cantaloupe  \n3 watermelon  \n4 cherimoya   \n5 mulberry    \n6 elderberry  \n7 nectarine   \n8 blackcurrant\n\n\n\nNote that slice_sample() selects a random set of rows from the tibble. The argument n, as shown above, will display that many rows. Use the argument prop to return a specific percentage of all the rows in the tibble.\nstr_detect() determines if a character vector matches a pattern. It returns a logical vector that is the same length as the input. Recall logicals are either TRUE or FALSE.\nWhich fruit names actually include the letter “c?”\n\ntbl_fruit |> \n  mutate(fruit_in_name = str_detect(fruit, pattern = \"c\")) \n\n# A tibble: 80 × 2\n   fruit        fruit_in_name\n   <chr>        <lgl>        \n 1 apple        FALSE        \n 2 apricot      TRUE         \n 3 avocado      TRUE         \n 4 banana       FALSE        \n 5 bell pepper  FALSE        \n 6 bilberry     FALSE        \n 7 blackberry   TRUE         \n 8 blackcurrant TRUE         \n 9 blood orange FALSE        \n10 blueberry    FALSE        \n# … with 70 more rows\n\n\n\nstr_length() counts characters in your strings. Note this is different from the length() of the character vector itself.\n\ntbl_fruit |> \n  mutate(name_length = str_length(fruit)) \n\n# A tibble: 80 × 2\n   fruit        name_length\n   <chr>              <int>\n 1 apple                  5\n 2 apricot                7\n 3 avocado                7\n 4 banana                 6\n 5 bell pepper           11\n 6 bilberry               8\n 7 blackberry            10\n 8 blackcurrant          12\n 9 blood orange          12\n10 blueberry              9\n# … with 70 more rows\n\n\nstr_sub() returns a portion of a string. This demonstration will return the first three letters of each fruit with the arguments fruit, 1, and 3.\n\ntbl_fruit |> \n  mutate(first_three_letters = str_sub(fruit, 1, 3)) \n\n# A tibble: 80 × 2\n   fruit        first_three_letters\n   <chr>        <chr>              \n 1 apple        app                \n 2 apricot      apr                \n 3 avocado      avo                \n 4 banana       ban                \n 5 bell pepper  bel                \n 6 bilberry     bil                \n 7 blackberry   bla                \n 8 blackcurrant bla                \n 9 blood orange blo                \n10 blueberry    blu                \n# … with 70 more rows\n\n\nstr_c() combines a character vector of length to a single string. This is similar to the normal c() function for creating a vector.\n\ntbl_fruit |> \n  mutate(name_with_s = str_c(fruit, \"s\")) \n\n# A tibble: 80 × 2\n   fruit        name_with_s  \n   <chr>        <chr>        \n 1 apple        apples       \n 2 apricot      apricots     \n 3 avocado      avocados     \n 4 banana       bananas      \n 5 bell pepper  bell peppers \n 6 bilberry     bilberrys    \n 7 blackberry   blackberrys  \n 8 blackcurrant blackcurrants\n 9 blood orange blood oranges\n10 blueberry    blueberrys   \n# … with 70 more rows\n\n\nstr_replace() replaces a pattern within a string.\n\ntbl_fruit |> \n  mutate(capital_A = str_replace(fruit, \n                                 pattern = \"a\", \n                                 replacement = \"A\")) \n\n# A tibble: 80 × 2\n   fruit        capital_A   \n   <chr>        <chr>       \n 1 apple        Apple       \n 2 apricot      Apricot     \n 3 avocado      Avocado     \n 4 banana       bAnana      \n 5 bell pepper  bell pepper \n 6 bilberry     bilberry    \n 7 blackberry   blAckberry  \n 8 blackcurrant blAckcurrant\n 9 blood orange blood orAnge\n10 blueberry    blueberry   \n# … with 70 more rows\n\n\n\n2.3.2 Regular expressions with stringr\nSometimes, your string tasks cannot be expressed in terms of a fixed string, but can be described in terms of a pattern. Regular expressions, also know as “regexes,” are the standard way to specify these patterns. In regexes, specific characters and constructs take on special meaning in order to match multiple strings.\nTo explore regular expressions, we will use the str_detect() function, which reports TRUE for any string which matches the pattern, and then filter() to see all the matches. For example, here are all the fruits which include a “w” in their name.\n\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"w\"))\n\n# A tibble: 4 × 1\n  fruit     \n  <chr>     \n1 honeydew  \n2 kiwi fruit\n3 strawberry\n4 watermelon\n\n\nIn the code below, the first metacharacter is the period . , which stands for any single character, except a newline (which, by the way, is represented by \\n). The regex b.r will match all fruits that have an “b”, followed by any single character, followed by “r”. Regexes are case sensitive.\n\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"b.r\"))\n\n# A tibble: 15 × 1\n   fruit      \n   <chr>      \n 1 bilberry   \n 2 blackberry \n 3 blueberry  \n 4 boysenberry\n 5 cloudberry \n 6 cranberry  \n 7 cucumber   \n 8 elderberry \n 9 goji berry \n10 gooseberry \n11 huckleberry\n12 mulberry   \n13 raspberry  \n14 salal berry\n15 strawberry \n\n\nAnother common metacharacter is *, generally pronounced “star.” It means zero or more characters of whatever proceeded it. Consider:\n\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"b.*r\"))\n\n# A tibble: 19 × 1\n   fruit       \n   <chr>       \n 1 bell pepper \n 2 bilberry    \n 3 blackberry  \n 4 blackcurrant\n 5 blood orange\n 6 blueberry   \n 7 boysenberry \n 8 breadfruit  \n 9 cloudberry  \n10 cranberry   \n11 cucumber    \n12 elderberry  \n13 goji berry  \n14 gooseberry  \n15 huckleberry \n16 mulberry    \n17 raspberry   \n18 salal berry \n19 strawberry  \n\n\nNote that addition of fruits like “bell pepper” and “breadfruit.” The b.*r means that we only require a “b” and an “r,” separated by zero or more characters. Bell pepper and breadfruit match this pattern while they did not match b.r.\nAnchors can be included to express where the expression must occur within the string. The ^ indicates the beginning of string and $ indicates the end.\n\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"^w\"))\n\n# A tibble: 1 × 1\n  fruit     \n  <chr>     \n1 watermelon\n\n\n\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"o$\"))\n\n# A tibble: 5 × 1\n  fruit    \n  <chr>    \n1 avocado  \n2 mango    \n3 pamelo   \n4 pomelo   \n5 tamarillo"
  },
  {
    "objectID": "02-wrangling.html#factors",
    "href": "02-wrangling.html#factors",
    "title": "2  Wrangling",
    "section": "\n2.4 Factors",
    "text": "2.4 Factors\nFactors are categorical variables that only take on a specified set of values. To manipulate factors we will use the forcats package, a core package in the Tidyverse.\nIt is easy to make factors with either factor(), as.factor() or parse_factor().\n\ntibble(X = letters[1:3]) |> \n  mutate(fac_1 = factor(X)) |> \n  mutate(fac_2 = as.factor(X)) |> \n  mutate(fac_3 = parse_factor(X))\n\n# A tibble: 3 × 4\n  X     fac_1 fac_2 fac_3\n  <chr> <fct> <fct> <fct>\n1 a     a     a     a    \n2 b     b     b     b    \n3 c     c     c     c    \n\n\nWhich of those three options is best depends on the situation. factor() is useful when you are creating a factor from nothing. as.factor() is best for simple transformations, especially of character variables, as in this example. parse_factor() is the most modern and powerful of the three.\n\nLet’s use gapminder$continent as an example. Note that str() is a useful function for getting detailed information about an object.\n\nstr(gapminder$continent)\n\n Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n\nlevels(gapminder$continent)\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\" \n\nnlevels(gapminder$continent)\n\n[1] 5\n\nclass(gapminder$continent)\n\n[1] \"factor\"\n\n\nTo get a frequency table as a tibble, from a tibble, use count(). To get a similar result from a free-range factor, use fct_count().\n\ngapminder |> \n  count(continent)\n\n# A tibble: 5 × 2\n  continent     n\n  <fct>     <int>\n1 Africa      624\n2 Americas    300\n3 Asia        396\n4 Europe      360\n5 Oceania      24\n\nfct_count(gapminder$continent)\n\n# A tibble: 5 × 2\n  f            n\n  <fct>    <int>\n1 Africa     624\n2 Americas   300\n3 Asia       396\n4 Europe     360\n5 Oceania     24\n\n\n\n2.4.1 Dropping unused levels\nRemoving all the rows corresponding to a specific factor level does not remove the level itself. These unused levels can come back to haunt you later, e.g., in figure legends.\nWatch what happens to the levels of country when we filter gapminder to a handful of countries.\n\nnlevels(gapminder$country)\n\n[1] 142\n\nh_gap <- gapminder |>\n  filter(country %in% c(\"Egypt\", \"Haiti\", \n                        \"Romania\", \"Thailand\", \n                        \"Venezuela\"))\nnlevels(h_gap$country)\n\n[1] 142\n\n\nEven though h_gap only has data for a handful of countries, we are still schlepping around all levels from the original gapminder tibble.\nHow can you get rid of them? The base function droplevels() operates on all the factors in a data frame or on a single factor. The function fct_drop() operates on a single factor variable.\n\nh_gap_dropped <- h_gap |> \n  droplevels()\nnlevels(h_gap_dropped$country)\n\n[1] 5\n\n# Use fct_drop() on a free-range factor\n\nh_gap$country |>\n  fct_drop() |>\n  levels()\n\n[1] \"Egypt\"     \"Haiti\"     \"Romania\"   \"Thailand\"  \"Venezuela\"\n\n\n\n2.4.2 Change the order of the levels\nBy default, factor levels are ordered alphabetically.\n\ngapminder$continent |>\n  levels()\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\" \n\n\nWe can also order factors by:\n\nFrequency: Make the most common level the first and so on.\nAnother variable: Order factor levels according to a summary statistic for another variable.\n\nLet’s order continent by frequency using fct_infreq().\n\ngapminder$continent |> \n  fct_infreq() |>\n  levels()\n\n[1] \"Africa\"   \"Asia\"     \"Europe\"   \"Americas\" \"Oceania\" \n\n\nWe can also have the frequency print out backwards using fct_rev().\n\ngapminder$continent |> \n  fct_infreq() |>\n  fct_rev() |> \n  levels()\n\n[1] \"Oceania\"  \"Americas\" \"Europe\"   \"Asia\"     \"Africa\"  \n\n\nThese two bar charts of frequency by continent differ only in the order of the continents. Which do you prefer? We show the code for just the second one.\n\n\n\n\n\n\ngapminder |> \n  mutate(continent = fct_infreq(continent)) |> \n  mutate(continent = fct_rev(continent)) |> \n  ggplot(aes(x = continent)) +\n    geom_bar() +\n    coord_flip()\n\n\n\n\nLet’s now order country by another variable, forwards and backwards. This other variable is usually quantitative and you will order the factor according to a grouped summary. The factor is the grouping variable and the default summarizing function is median() but you can specify something else.\n\n# Order countries by median life expectancy\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp) |> \n  levels() |> \n  head()\n\n[1] \"Sierra Leone\"  \"Guinea-Bissau\" \"Afghanistan\"   \"Angola\"       \n[5] \"Somalia\"       \"Guinea\"       \n\n# Order according to minimum life exp instead of median\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp, min) |> \n  levels() |> \n  head()\n\n[1] \"Rwanda\"       \"Afghanistan\"  \"Gambia\"       \"Angola\"       \"Sierra Leone\"\n[6] \"Cambodia\"    \n\n# Backwards!\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp, \n            .desc = TRUE) |> \n  levels() |> \n  head()\n\n[1] \"Iceland\"     \"Japan\"       \"Sweden\"      \"Switzerland\" \"Netherlands\"\n[6] \"Norway\"     \n\n\nWhy do we reorder factor levels? It often makes plots much better! When plotting a factor against a numeric variable, it is generally a good idea to order the factors by some function of the numeric variable. Alphabetic ordering is rarely best.\nCompare the interpretability of these two plots of life expectancy in the Aericas in 2007. The only difference is the order of the country factor. Which is better?\n\ngapminder |> \n  filter(year == 2007, \n         continent == \"Americas\") |> \n  ggplot(aes(x = lifeExp, y = country)) + \n    geom_point()\n\n\n\n\n\ngapminder |> \n  filter(year == 2007, \n         continent == \"Americas\") |> \n  ggplot(aes(x = lifeExp, \n             y = fct_reorder(country, lifeExp))) + \n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.3 Recode the levels\nUse fct_recode() to change the names of the levels for a factor.\n\ni_gap <- gapminder |> \n  filter(country %in% c(\"United States\", \"Sweden\", \n                        \"Australia\")) |> \n  droplevels()\n\ni_gap$country |> \n  levels()\n\n[1] \"Australia\"     \"Sweden\"        \"United States\"\n\ni_gap$country |>\n  fct_recode(\"USA\" = \"United States\", \"Oz\" = \"Australia\") |> \n  levels()\n\n[1] \"Oz\"     \"Sweden\" \"USA\""
  },
  {
    "objectID": "02-wrangling.html#date-times",
    "href": "02-wrangling.html#date-times",
    "title": "2  Wrangling",
    "section": "\n2.5 Date-Times",
    "text": "2.5 Date-Times\nWe will manipulate date-times using the lubridate package because lubridate makes it easier to work with dates and times in R. lubridate is not part of the core tidyverse because you only need it when you’re working with dates/times.\n\n\n\n\n\nThere are three types of date/time data that refer to an instant in time:\n\nA date. Tibbles print this as <date>.\nA time within a day. Tibbles print this as <time>.\nA date-time is a date plus a time. It uniquely identifies an instant in time (typically to the nearest second). Tibbles print this as <dttm>.\n\nYou should always use the simplest possible data type that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we’ll come back to at the end of the chapter.\nTo get the current date or date-time you can use today() or now():\n\ntoday()\n\n[1] \"2023-03-02\"\n\nnow()\n\n[1] \"2023-03-02 13:13:54 EST\"\n\n\nOtherwise, there are three ways you’re likely to create a date/time:\n\nFrom a string.\nFrom individual date-time components.\nFrom an existing date/time object.\n\nThey work as follows:\n\n2.5.1 From strings\nDate/time data often comes as strings. The lubridate functions automatically work out the format once you specify the order of the component. First, figure out that you want the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” accordingly. That gives you the name of the lubridate function that will parse your date. For example:\n\nymd(\"2017-01-31\")\n\n[1] \"2017-01-31\"\n\nmdy(\"January 31st, 2017\")\n\n[1] \"2017-01-31\"\n\ndmy(\"31-Jan-2017\")\n\n[1] \"2017-01-31\"\n\n\nThese functions also take unquoted numbers. This is the most concise way to create a single date/time object, as you might need when filtering date/time data. ymd() is short and unambiguous:\n\nymd(20170131)\n\n[1] \"2017-01-31\"\n\n\nymd() and friends create dates. To create a date-time, add an underscore and one or more of “h”, “m”, and “s” to the name of the parsing function:\n\nymd_hms(\"2017-01-31 20:11:59\")\n\n[1] \"2017-01-31 20:11:59 UTC\"\n\nmdy_hm(\"01/31/2017 08:01\")\n\n[1] \"2017-01-31 08:01:00 UTC\"\n\n\nYou can also force the creation of a date-time from a date by supplying a timezone:\n\nymd(20170131, tz = \"UTC\")\n\n[1] \"2017-01-31 UTC\"\n\n\n\n2.5.2 From individual components\nInstead of a single string, sometimes you’ll have the individual components of the date-time spread across multiple columns. This is what we have in the flights data:\n\nflights |> \n  select(year, month, day, hour, minute)\n\n# A tibble: 336,776 × 5\n    year month   day  hour minute\n   <int> <int> <int> <dbl>  <dbl>\n 1  2013     1     1     5     15\n 2  2013     1     1     5     29\n 3  2013     1     1     5     40\n 4  2013     1     1     5     45\n 5  2013     1     1     6      0\n 6  2013     1     1     5     58\n 7  2013     1     1     6      0\n 8  2013     1     1     6      0\n 9  2013     1     1     6      0\n10  2013     1     1     6      0\n# … with 336,766 more rows\n\n\nTo create a date/time from this sort of input, use make_date() for dates, or make_datetime() for date-times:\n\nflights |> \n  select(year, month, day, hour, minute) |> \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n\n# A tibble: 336,776 × 6\n    year month   day  hour minute departure          \n   <int> <int> <int> <dbl>  <dbl> <dttm>             \n 1  2013     1     1     5     15 2013-01-01 05:15:00\n 2  2013     1     1     5     29 2013-01-01 05:29:00\n 3  2013     1     1     5     40 2013-01-01 05:40:00\n 4  2013     1     1     5     45 2013-01-01 05:45:00\n 5  2013     1     1     6      0 2013-01-01 06:00:00\n 6  2013     1     1     5     58 2013-01-01 05:58:00\n 7  2013     1     1     6      0 2013-01-01 06:00:00\n 8  2013     1     1     6      0 2013-01-01 06:00:00\n 9  2013     1     1     6      0 2013-01-01 06:00:00\n10  2013     1     1     6      0 2013-01-01 06:00:00\n# … with 336,766 more rows\n\n\n\n2.5.3 From other types\nYou may want to switch between a date-time and a date. That’s the job of as_datetime() and as_date():\n\nas_datetime(today())\n\n[1] \"2023-03-02 UTC\"\n\nas_date(now())\n\n[1] \"2023-03-02\"\n\n\nSometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date().\n\n\nas_datetime(60 * 60 * 10)\n\n[1] \"1970-01-01 10:00:00 UTC\"\n\nas_date(365 * 10 + 2)\n\n[1] \"1980-01-01\"\n\n\n\n2.5.4 Date-time components\nNow that you know how to get date-time data into R’s date-time data structures, let’s explore what you can do with them. This section will focus on the accessor functions that let you get and set individual components. The next section will look at how arithmetic works with date-times.\nYou can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second().\n\ndatetime <- ymd_hms(\"2016-07-08 12:34:56\")\nyear(datetime)\n\n[1] 2016\n\nmonth(datetime)\n\n[1] 7\n\nmday(datetime)\n\n[1] 8\n\nyday(datetime)\n\n[1] 190\n\nwday(datetime)\n\n[1] 6\n\n\nFor month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set both label = TRUE and abbr = FALSE to return the full name.\n\nmonth(datetime, label = TRUE)\n\n[1] Jul\n12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\n\nwday(datetime, label = TRUE, abbr = FALSE)\n\n[1] Friday\n7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\n\n\n\n2.5.5 Setting components\nYou can create a new date-time with update().\n\nupdate(datetime, year = 2020, month = 2, mday = 2, hour = 2)\n\n[1] \"2020-02-02 02:34:56 UTC\"\n\n\nIf values are too big, they will roll-over:\n\nymd(\"2015-02-01\") |> \n  update(mday = 30)\n\n[1] \"2015-03-02\"\n\nymd(\"2015-02-01\") |> \n  update(hour = 400)\n\n[1] \"2015-02-17 16:00:00 UTC\"\n\n\n\n2.5.6 Time zones\nTime zones are an enormously complicated topic because of their interaction with geopolitical entities. Fortunately, we don’t need to dig into all the details as they’re not all imperative for data analysis. You can see the complete list of possible timezones with the function OlsonNames(). Unless otherwise specified, lubridate always uses UTC (Coordinated Universal Time).\nIn R, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time:\n\n(x1 <- ymd_hms(\"2015-06-01 12:00:00\", tz = \"America/New_York\"))\n\n[1] \"2015-06-01 12:00:00 EDT\"\n\n(x2 <- ymd_hms(\"2015-06-01 18:00:00\", tz = \"Europe/Copenhagen\"))\n\n[1] \"2015-06-01 18:00:00 CEST\"\n\n(x3 <- ymd_hms(\"2015-06-02 04:00:00\", tz = \"Pacific/Auckland\"))\n\n[1] \"2015-06-02 04:00:00 NZST\""
  },
  {
    "objectID": "02-wrangling.html#combining-data",
    "href": "02-wrangling.html#combining-data",
    "title": "2  Wrangling",
    "section": "\n2.6 Combining Data",
    "text": "2.6 Combining Data\nThere are many ways to bring data together.\n\n\n\n\nCombining data is often tricky.\n\n\n\n\nThe bind_rows() function is used to combine all the rows from two or more tibbles.\n\ndata_1 <- tibble(x = 1:2,                \n                 y = c(\"A\", \"B\")) \n\ndata_2 <- tibble(x = 3:4,\n                 y = c(\"C\", \"D\")) \n\n\nbind_rows(data_1, data_2)\n\n# A tibble: 4 × 2\n      x y    \n  <int> <chr>\n1     1 A    \n2     2 B    \n3     3 C    \n4     4 D    \n\n\n\n2.6.1 Joins\nConsider two tibbles: superheroes and publishers.\n\nsuperheroes <- tibble::tribble(\n       ~name,   ~gender,     ~publisher,\n   \"Magneto\",   \"male\",       \"Marvel\",\n     \"Storm\",   \"female\",     \"Marvel\",\n    \"Batman\",   \"male\",       \"DC\",\n  \"Catwoman\",   \"female\",     \"DC\",\n   \"Hellboy\",   \"male\",       \"Dark Horse Comics\"\n  )\n\npublishers <- tibble::tribble(\n  ~publisher, ~yr_founded,\n        \"DC\",       1934L,\n    \"Marvel\",       1939L,\n     \"Image\",       1992L\n  )\n\nNote how easy it is to use tribble() from the tibble package to create a tibble on the fly using text which organized for easy entry and reading. Recall that a double colon — :: — is how we indicate that a function comes from a specific package.\n\n2.6.1.1 inner_join()\n\n\ninner_join(x, y): Returns all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned.\n\n\n\n\nInner join.\n\n\n\n\n\ninner_join(superheroes, publishers)\n\nJoining with `by = join_by(publisher)`\n\n\n# A tibble: 4 × 4\n  name     gender publisher yr_founded\n  <chr>    <chr>  <chr>          <int>\n1 Magneto  male   Marvel          1939\n2 Storm    female Marvel          1939\n3 Batman   male   DC              1934\n4 Catwoman female DC              1934\n\n\nWe lose Hellboy in the join because, although he appears in x = superheroes, his publisher Dark Horse Comics does not appear in y = publishers. The join result has all variables from x = superheroes plus yr_founded, from y.\nNote the message that we are ‘Joining, by = “publisher”’. Whenever joining, R checks to see if there are variables in common between the two tibbles and, if there are, uses them to join. However, it is concerned that you may not be aware that this is what it is doing, so R tells you. Such messages are both annoying and a signal that we have not made our code as robust as we should. Fortunately, we can specify precisely which variables we want to join by. Always do this.\n\ninner_join(superheroes, publishers, by = \"publisher\")\n\nby also takes a vector of key variables if you want to merge by multiple variables.\nNow compare this result to that of using inner_join() with the two datasets in opposite positions.\n\ninner_join(publishers, superheroes, by = \"publisher\")\n\nWarning in inner_join(publishers, superheroes, by = \"publisher\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\n# A tibble: 4 × 4\n  publisher yr_founded name     gender\n  <chr>          <int> <chr>    <chr> \n1 DC              1934 Batman   male  \n2 DC              1934 Catwoman female\n3 Marvel          1939 Magneto  male  \n4 Marvel          1939 Storm    female\n\n\nIn a way, this does illustrate multiple matches, if you think about it from the x = publishers direction. Every publisher that has a match in y = superheroes appears multiple times in the result, once for each match. In fact, we’re getting the same result as with inner_join(superheroes, publishers), up to variable order (which you should also never rely on in an analysis).\n\n2.6.1.2 full_join()\n\nfull_join(x, y): Returns all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing.\n\nfull_join(superheroes, publishers, by = \"publisher\")\n\n# A tibble: 6 × 4\n  name     gender publisher         yr_founded\n  <chr>    <chr>  <chr>                  <int>\n1 Magneto  male   Marvel                  1939\n2 Storm    female Marvel                  1939\n3 Batman   male   DC                      1934\n4 Catwoman female DC                      1934\n5 Hellboy  male   Dark Horse Comics         NA\n6 <NA>     <NA>   Image                   1992\n\n\nWe get all rows of x = superheroes plus a new row from y = publishers, containing the publisher Image. We get all variables from x = superheroes AND all variables from y = publishers. Any row that derives solely from one table or the other carries NAs in the variables found only in the other table.\nBecause full_join() returns all rows and all columns from both x and y, the result of full_join(x = superheroes, y = publishers) should match that of full_join(x = publishers, y = superheroes).\n\n2.6.1.3 left_join()\n\nleft_join(x, y): Returns all rows from x, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned.\n\nleft_join(superheroes, publishers, by = \"publisher\")\n\n# A tibble: 5 × 4\n  name     gender publisher         yr_founded\n  <chr>    <chr>  <chr>                  <int>\n1 Magneto  male   Marvel                  1939\n2 Storm    female Marvel                  1939\n3 Batman   male   DC                      1934\n4 Catwoman female DC                      1934\n5 Hellboy  male   Dark Horse Comics         NA\n\n\nWe basically get x = superheroes back, but with an additional variable yr_founded, which is unique to y = publishers. Hellboy, whose publisher does not appear in y = publishers, has an NA for yr_founded.\nNow compare this result to that of running left_join(x = publishers, y = superheroes). Unlike inner_join() and full_join() the order of the arguments has a significant effect on the resulting tibble.\n\nleft_join(publishers, superheroes, by = \"publisher\")\n\nWarning in left_join(publishers, superheroes, by = \"publisher\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\n# A tibble: 5 × 4\n  publisher yr_founded name     gender\n  <chr>          <int> <chr>    <chr> \n1 DC              1934 Batman   male  \n2 DC              1934 Catwoman female\n3 Marvel          1939 Magneto  male  \n4 Marvel          1939 Storm    female\n5 Image           1992 <NA>     <NA>  \n\n\nWe get a similar result as with inner_join(), but the publisher Image survives in the join, even though no superheroes from Image appear in y = superheroes. As a result, Image has NAs for name and gender.\nThere is a similar function, right_join(x, y) that returns all rows from y, and all columns from x and y.\n\n2.6.1.4 semi_join()\n\nsemi_join(x, y): Returns all rows from x where there are matching values in y, keeping just columns from x. A semi join differs from an inner join because an inner join will return one row of x for each matching row of y, whereas a semi join will never duplicate rows of x. This is a filtering join.\n\nsemi_join(superheroes, publishers, by = \"publisher\")\n\n# A tibble: 4 × 3\n  name     gender publisher\n  <chr>    <chr>  <chr>    \n1 Magneto  male   Marvel   \n2 Storm    female Marvel   \n3 Batman   male   DC       \n4 Catwoman female DC       \n\n\nCompare the result of switching the values of the arguments.\n\nsemi_join(x = publishers, y = superheroes, by = \"publisher\")\n\n# A tibble: 2 × 2\n  publisher yr_founded\n  <chr>          <int>\n1 DC              1934\n2 Marvel          1939\n\n\nNow the effects of switching the x and y roles is more clear. The result resembles x = publishers, but the publisher Image is lost, since there are no observations where publisher == \"Image\" in y = superheroes.\n\n2.6.1.5 anti_join()\n\nanti_join(x, y): Return all rows from x where there are no matching values in y, keeping just columns from x.\n\nanti_join(superheroes, publishers, by = \"publisher\")\n\n# A tibble: 1 × 3\n  name    gender publisher        \n  <chr>   <chr>  <chr>            \n1 Hellboy male   Dark Horse Comics\n\n\nWe keep only Hellboy now.\nNow switch the arguments and compare the result.\n\nanti_join(publishers, superheroes, by = \"publisher\")\n\n# A tibble: 1 × 2\n  publisher yr_founded\n  <chr>          <int>\n1 Image           1992\n\n\nWe keep only publisher Image now (and the variables found in x = publishers).\n\n2.6.2 Example\nConsider the relationships among the tibbles in the nycflights13 package:\n\n\n\n\nRelationships among nycflights tables\n\n\n\n\nIn both the flights and airlines data frames, the key variable we want to join/merge/match the rows by has the same name: carrier. Let’s use inner_join() to join the two data frames, where the rows will be matched by the variable carrier\n\nflights |> \n  inner_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 20\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 10 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, name <chr>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nThis is our first example of using a join function with a “pipe.” flights is fed as the first argument to inner_join(). That is what the pipe does. The symbol is created by combining a vertical line | and a greater-than symbol ‘>’. Cmd-Shift-M is the shortcut key for inserting a pipe.\nThe above code is equivalent to:\n\ninner_join(flights, airlines, by = \"carrier\")\n\nThe airports data frame contains the airport codes for each airport:\n\nairports\n\n# A tibble: 1,458 × 8\n   faa   name                             lat    lon   alt    tz dst   tzone    \n   <chr> <chr>                          <dbl>  <dbl> <dbl> <dbl> <chr> <chr>    \n 1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n 2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n 3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n 4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n 5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n 6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n 7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n 8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n 9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n# … with 1,448 more rows\n\n\nHowever, if you look at both the airports and flights data frames, you’ll find that the airport codes are in variables that have different names. In airports the airport code is in faa, whereas in flights the airport codes are in origin and dest.\nIn order to join these two data frames by airport code, our inner_join() operation will use by = c(\"dest\" = \"faa\") thereby allowing us to join two data frames where the key variable has a different name.\n\nflights |> \n  inner_join(airports, by = c(\"dest\" = \"faa\"))\n\n# A tibble: 329,174 × 26\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      554        600      -6     812     837     -25 DL     \n 5  2013     1     1      554        558      -4     740     728      12 UA     \n 6  2013     1     1      555        600      -5     913     854      19 B6     \n 7  2013     1     1      557        600      -3     709     723     -14 EV     \n 8  2013     1     1      557        600      -3     838     846      -8 B6     \n 9  2013     1     1      558        600      -2     753     745       8 AA     \n10  2013     1     1      558        600      -2     849     851      -2 B6     \n# … with 329,164 more rows, 16 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, name <chr>, lat <dbl>, lon <dbl>,\n#   alt <dbl>, tz <dbl>, dst <chr>, tzone <chr>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nLet’s construct the chain of pipe operators |> that computes the number of flights from NYC to each destination, but also includes information about each destination airport:\n\nflights |>\n  summarize(num_flights = n(),\n            .by = dest) |>\n  arrange(desc(num_flights)) |>\n  inner_join(airports, by = c(\"dest\" = \"faa\")) |>\n  rename(airport_name = name)\n\n# A tibble: 101 × 9\n   dest  num_flights airport_name             lat    lon   alt    tz dst   tzone\n   <chr>       <int> <chr>                  <dbl>  <dbl> <dbl> <dbl> <chr> <chr>\n 1 ORD         17283 Chicago Ohare Intl      42.0  -87.9   668    -6 A     Amer…\n 2 ATL         17215 Hartsfield Jackson At…  33.6  -84.4  1026    -5 A     Amer…\n 3 LAX         16174 Los Angeles Intl        33.9 -118.    126    -8 A     Amer…\n 4 BOS         15508 General Edward Lawren…  42.4  -71.0    19    -5 A     Amer…\n 5 MCO         14082 Orlando Intl            28.4  -81.3    96    -5 A     Amer…\n 6 CLT         14064 Charlotte Douglas Intl  35.2  -80.9   748    -5 A     Amer…\n 7 SFO         13331 San Francisco Intl      37.6 -122.     13    -8 A     Amer…\n 8 FLL         12055 Fort Lauderdale Holly…  26.1  -80.2     9    -5 A     Amer…\n 9 MIA         11728 Miami Intl              25.8  -80.3     8    -5 A     Amer…\n10 DCA          9705 Ronald Reagan Washing…  38.9  -77.0    15    -5 A     Amer…\n# … with 91 more rows\n\n\n\"ORD\" is the airport code of Chicago O’Hare airport and \"FLL\" is the code for the main airport in Fort Lauderdale, Florida, which can be seen in the airport_name variable."
  },
  {
    "objectID": "02-wrangling.html#tidy-data",
    "href": "02-wrangling.html#tidy-data",
    "title": "2  Wrangling",
    "section": "\n2.7 Tidy data",
    "text": "2.7 Tidy data\nConsider the first five rows from the drinks data frame from the fivethirtyeight package:\n\n\n# A tibble: 5 × 5\n  country     beer_servings spirit_servings wine_servings total_litres_of_pure…¹\n  <chr>               <int>           <int>         <int>                  <dbl>\n1 Afghanistan             0               0             0                    0  \n2 Albania                89             132            54                    4.9\n3 Algeria                25               0            14                    0.7\n4 Andorra               245             138           312                   12.4\n5 Angola                217              57            45                    5.9\n# … with abbreviated variable name ¹​total_litres_of_pure_alcohol\n\n\nAfter reading the help file by running ?drinks, you’ll see that drinks is a data frame that contains the results from a survey of the average number of servings of beer, spirits, and wine consumed in 193 countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi’s article: “Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?”.\nLet’s apply some of the data wrangling verbs on the drinks data frame:\n\n\nfilter() the drinks data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, then\n\n\nselect() all columns except total_litres_of_pure_alcohol by using the - sign, then\n\n\nrename() the variables beer_servings, spirit_servings, and wine_servings to beer, spirit, and wine, respectively.\n\nWe will save the resulting data frame in drinks_smaller.\n\ndrinks_smaller <- drinks |>\n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) |>\n  select(-total_litres_of_pure_alcohol) |>\n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\n\n2.7.1 Definition of “tidy” data\nWhat does it mean for your data to be “tidy”? While “tidy” has a clear English meaning of “organized,” the word “tidy” in data science using R means that your data follows a standardized format.\n“Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\n\n\n\n2.7.2 Converting to “tidy” data\nIn this book so far, you’ve only seen data frames that were already in “tidy” format. Furthermore, for the rest of this book, you’ll mostly only see data frames that are already tidy as well. This is not always the case, however, with all datasets in the world. If your original data frame is in wide (non-“tidy”) format and you would like to use the ggplot2 or dplyr packages, you will first have to convert it to “tidy” format. To do so, we recommend using the pivot_longer() function in the tidyr package (Wickham, Vaughan, and Girlich 2023).\nGoing back to our drinks_smaller data frame from earlier:\n\ndrinks_smaller\n\n# A tibble: 4 × 4\n  country       beer spirit  wine\n  <chr>        <int>  <int> <int>\n1 China           79    192     8\n2 Italy           85     42   237\n3 Saudi Arabia     0      5     0\n4 USA            249    158    84\n\n\nWe tidy it by using the pivot_longer() function from the tidyr package as follows:\n\ndrinks_smaller_tidy <- drinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy\n\n# A tibble: 12 × 3\n   country      type   servings\n   <chr>        <chr>     <int>\n 1 China        beer         79\n 2 China        spirit      192\n 3 China        wine          8\n 4 Italy        beer         85\n 5 Italy        spirit       42\n 6 Italy        wine        237\n 7 Saudi Arabia beer          0\n 8 Saudi Arabia spirit        5\n 9 Saudi Arabia wine          0\n10 USA          beer        249\n11 USA          spirit      158\n12 USA          wine         84\n\n\nLet’s dissect the arguments to pivot_longer().\n\nthe first argumentnames_to corresponds to the name of the variable in the new “tidy”/long data frame that will contain the column names of the original data. Observe how we set names_to = \"type\". In the resulting drinks_smaller_tidy, the column type contains the three types of alcohol beer, spirit, and wine. Since type is a variable name that doesn’t appear in drinks_smaller, we use quotation marks around it. You’ll receive an error if you just use names_to = type here.\nThe second argument values_to corresponds to the name of the variable in the new “tidy” data frame that will contain the values of the original data. Observe how we set values_to = \"servings\" since each numeric value in the beer, wine, and spirit columns of the drinks_smaller corresponds to a value of servings. In the resulting drinks_smaller_tidy, the column servings contains the 4 \\(\\times\\) 3 = 12 numerical values. Note again that servings doesn’t appear as a variable in drinks_smaller so it again needs quotation marks around it for the values_to argument.\nThe third argument cols is the columns in the drinks_smaller data frame you either want to or don’t want to “tidy.” Observe how we set this to -country indicating that we don’t want to “tidy” the country variable in drinks_smaller and rather only beer, spirit, and wine. Since country is a column that appears in drinks_smaller we don’t put quotation marks around it.\n\nThe third argument here of cols is a little nuanced, so let’s consider code that was written slightly differently but that produces the same output:\n\ndrinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\n\nNote that the third argument now specifies which columns we want to “tidy” with c(beer, spirit, wine), instead of the columns we don’t want to “tidy” using -country. We use the c() function to create a vector of the columns in drinks_smaller that we’d like to “tidy.” Note that since these three columns appear one after another in the drinks_smaller data frame, we can also do the following for the cols argument:\n\ndrinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)\n\nConverting “wide” format data to “tidy” format often confuses new R users. The only way to get comfortable with the pivot_longer() function is with practice, practice, and more practice using different datasets. For example, run ?pivot_longer and look at the examples in the bottom of the help file.\nIf, however, you want to convert a “tidy” data frame to “wide” format, you will need to use the pivot_wider() function instead. Run ?pivot_wider and look at the examples in the bottom of the help file for examples.\nYou can also view examples of both pivot_longer() and pivot_wider() on the tidyverse.org webpage. There’s a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a dataset in the weekly #TidyTuesday event that might serve as a nice place for you to find other data to explore and transform."
  },
  {
    "objectID": "02-wrangling.html#other-commands",
    "href": "02-wrangling.html#other-commands",
    "title": "2  Wrangling",
    "section": "\n2.8 Other commands",
    "text": "2.8 Other commands\n\n\n\n\n\n\n\n\nHere are some topics which will prove important later in the Primer.\n\n2.8.1 Random variables\n\n2.8.1.1 sample()\n\nThe most common distributions you will work with are empirical or frequency distributions, the values of age in the trains tibble, the values of poverty in the kenya tibble, and so on. But we can also create our own data by making “draws” from a distribution which we have concocted.\nConsider the distribution of the possible values from rolling a fair die. We can use the sample() function to create draws from this distribution, meaning it will change (or sometimes stay the same) for every subsequent draw.\n\n\n\n\ndie <- c(1, 2, 3, 4, 5, 6)\n\nsample(x = die, size = 1)\n\n[1] 3\n\n\nThis produces one “draw” from the distribution of the possible values of one roll of fair six-sided die.\nNow, suppose we wanted to roll this die 10 times. One of the arguments of the sample() function is replace. We must specify it as TRUE if values can appear more than once. Since, when rolling a die 10 times, we expect that a value like 3 can appear more than once, we need to set replace = TRUE.\n\nsample(x = die, size = 10, replace = TRUE)\n\n [1] 5 6 3 3 3 4 3 6 4 5\n\n\nIn other words, rolling a 1 on the first roll should not preclude you from rolling a 1 on a later roll.\nWhat if the die is not “fair,” meaning that some sides are more likely to appear than others? The final argument of the sample() function is the prob argument. It takes a vector (of the same length as the initial vector x) that contains all of the probabilities of selecting each one of the elements of x. Suppose that the probability of rolling a 1 was 0.5, and the probability of rolling any other value is 0.1. (These probabilities should sum to 1. If they don’t sample() will automatically re-scale them.)\n\nsample(x = die, \n       size = 10, \n       replace = TRUE, \n       prob = c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1))\n\n [1] 1 1 3 1 2 1 1 6 1 1\n\n\n\nRemember: There is no real data here. We have not actually rolled a die. We have just made some assumptions about what would happen if we were to roll a die. With those assumptions we have built an urn — a data generating mechanism — from which we can draw as many values as we like. Let’s roll the unfair die 10,000 times.\n\ntibble(result = sample(x = die, \n                       size = 10000, \n                       replace = TRUE, \n                       prob = c(0.5, rep(0.1, 5)))) |> \n  ggplot(aes(x = result)) +\n    geom_bar() +\n    labs(title = \"Distribution of Results of an Unfair Die\",\n         x = \"Result of One Roll\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 1:6,\n                       labels = as.character(1:6)) +\n    scale_y_continuous(labels = scales::comma_format())\n\n\n\n\nsample() is just one of many functions for creating draws — or, more colloquially, “drawing” — from a distribution. Three of the most important functions are: runif(), rbinom(), and rnorm().\n\n2.8.1.2 runif()\n\nConsider a “uniform” distribution. This is the case in which every outcome in the range of possible outcomes has the same chance of occurring. The function runif() (spoken as “r-unif”) enables us to draw from a uniform contribution. runif() has three arguments: n, min, and max. runif() will produce n draws from between min and max, with each value having an equal chance of occurring.\n\nrunif(n = 10, min = 4, max = 6)\n\n [1] 5.594800 4.981960 4.601993 5.738417 4.879622 5.155263 4.319479 5.627666\n [9] 4.800110 5.634794\n\n\nMathematically, we would notate:\n\\[y_i \\sim U(4, 6)\\]\nThis means that the each value for \\(y\\) is drawn from a uniform distribution between four and six.\n\n2.8.1.3 rbinom()\n\nConsider binomial distribution, the case in which the probability of some Boolean variable (for instance success or failure) is calculated for repeated, independent trials. One common example would be the probability of flipping a coin and landing on heads. The function rbinom() allows us to draw from a binomial distribution. This function takes three arguments, n, size, and prob.\nn is the number of values we seek to draw. size is the number of trials for each n. *prob is the probability of success on each trial.\nSuppose we wanted to flip a fair coin one time, and let landing on heads represent success.\n\nrbinom(n = 1 , size = 1, prob = 0.5)\n\n[1] 0\n\n\nDo the same thing 100 times:\n\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.5)) |> \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\n\n\n\n\nIn our graph above, we use the function scale_x_continuous() because our x-axis variable is continuous, meaning it can take on any real values. The breaks argument to scale_x_continuous() converts our x-axis to having two different “tick marks”. There is a fairly even distribution of Tails and Heads. More draws would typically result in an even more equal split.\nRandomness creates (inevitable) tension between distribution as a “thing” and distribution as a vector of draws from that thing. In this case, the vector of draws is not balanced between Tails and Heads. Yet, we “know” that it should be since the coin is, by definition, fair. In a sense, the mathematics require an even split. Yet, randomness means that the vector of draws will rarely match the mathematically “true” result. And that is OK! First, randomness is an intrinsic property of the real world. Second, we can make the effect of randomness be as small as we want by increasing the number of draws.\n\nSuppose instead we wanted to simulate an unfair coin, where the probability of landing on Heads was 0.75 instead of 0.25.\n\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.75)) |> \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\n\n\n\n\nThe distribution — the imaginary urn — from which we draw the results of a coin flip for a fair coin is a different distribution — a different imaginary urn — from the distribution for a biased coin. In fact, there are an infinite number of distributions. Yet as long as we can draw values from a distribution, we can work with it. Mathematics:\n\\[y_i \\sim B(n, p)\\]\nEach value for \\(y\\) is drawn from a binomial distribution with parameters \\(n\\) for the number of trials and \\(p\\) for the probability of success.\nInstead of each n consisting of a single trial, we could have situation in which we are, 10,000 times, flipping a coin 10 times and summing up, for each experiment, the number of heads. In that case:\n\nset.seed(9)\ntibble(heads = rbinom(n = 10000, size = 10, prob = 0.5)) |> \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 10 Times\",\n         subtitle = \"Extreme results are possible with enough experiments\",\n         x = \"Total Number of Heads in Ten Flips\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n2.8.1.4 rnorm()\n\nThe most important distribution is the normal distribution. Mathematics:\n\\[y_i \\sim N(\\mu, \\sigma^2)\\]\nEach value \\(y_i\\) is drawn from a normal distribution with parameters \\(\\mu\\) for the mean and \\(\\sigma\\) for the standard deviation.\nThis bell-shaped distribution is defined by two parameters: (1) the mean \\(\\mu\\) (spoken as “mu”) which locates the center of the distribution and (2) the standard deviation \\(\\sigma\\) (spoken as “sigma”) which determines the variation of values around that center. In the figure below, we plot three normal distributions where:\n\nThe solid normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).\nThe dotted normal curve has mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).\nThe dashed normal curve has mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\n\n\n\n\n\nThree normal distributions.\n\n\n\n\nNotice how the solid and dotted line normal curves have the same center due to their common mean \\(\\mu\\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \\(\\sigma = 5\\). On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \\(\\sigma = 2\\). However, they are centered at different locations.\nWhen the mean \\(\\mu = 0\\) and the standard deviation \\(\\sigma = 1\\), the normal distribution has a special name. It’s called the standard normal distribution or the \\(z\\)-curve.\nFurthermore, if a variable follows a normal curve, there are three rules of thumb we can use:\n\n68% of values will lie within \\(\\pm\\) 1 standard deviation of the mean.\n95% of values will lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations of the mean.\n99.7% of values will lie within \\(\\pm\\) 3 standard deviations of the mean.\n\nLet’s illustrate this on a standard normal curve. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100%. For example:\n\nThe middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34% + 34% = 68% of the area under the curve. In other words, 68% of values.\nThe middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5% + 34% + 34% + 13.5% = 95% of the area under the curve. In other words, 95% of values.\nThe middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% of the area under the curve. In other words, 99.7% of values.\n\n\n\n\n\nRules of thumb about areas under normal curves.\n\n\n\n\n\n2.8.2 Combinations\nWe often need to create a tibble with all the possible combinations of two or more variables. expand_grid() is the easiest approach.\n\nexpand_grid(x = c(\"A\", \"B\"), y = c(1, 2, 3, 4))\n\n# A tibble: 8 × 2\n  x         y\n  <chr> <dbl>\n1 A         1\n2 A         2\n3 A         3\n4 A         4\n5 B         1\n6 B         2\n7 B         3\n8 B         4\n\n\ncrossing() does something similar if the input variables are already in a tibble.\n\n2.8.3 Matrices\nRecall that a “matrix” in R is a rectangular array of data, shaped like a data frame or tibble, but containing only one type of data, e.g., numeric. Large matrices also print out ugly. (There are other differences, none of which we care about here.) Example:\n\nm <- matrix(c(3, 4, 8, 9, 12, 13, 0, 15, -1), ncol = 3)\nm\n\n     [,1] [,2] [,3]\n[1,]    3    9    0\n[2,]    4   12   15\n[3,]    8   13   -1\n\n\nThe easiest way to pull information from a matrix is to use [ ], the subset operator. Here is how we grab the second and third columns of m:\n\nm[, 2:3]\n\n     [,1] [,2]\n[1,]    9    0\n[2,]   12   15\n[3,]   13   -1\n\n\nNote, however, that matrices with just one dimension “collapse” into single vectors:\n\nm[, 2]\n\n[1]  9 12 13\n\n\nTibbles, on the other hand, always maintain their rectangular shapes, even with only one column or row.\n\n2.8.4 Missing Values\nSome observations in a tibble are blank. These are called missing values, and they are often marked as NA. We can create such a tibble as follows:\n\ntbl <- tribble(\n  ~ a, ~ b, ~ c,\n    2,   3,   5,\n    4,  NA,   8,\n   NA,   7,   9,\n)\n\ntbl\n\n# A tibble: 3 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     2     3     5\n2     4    NA     8\n3    NA     7     9\n\n\nThe presence of NA values can be problematic.\n\ntbl |> \n  summarize(avg_a = mean(a))\n\n# A tibble: 1 × 1\n  avg_a\n  <dbl>\n1    NA\n\n\nFortunately, most R functions take an argument, na.rm, which, when set to TRUE, removes NA values from any calculations.\n\ntbl |> \n  summarize(avg_a = mean(a, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_a\n  <dbl>\n1     3\n\n\nAnother approach is to use drop_na().\n\ntbl |> \n  drop_na(a)\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     2     3     5\n2     4    NA     8\n\n\nHowever, be careful! If you use drop_na() without a specific variable provided, you will remove all rows with a missing value for any variable in the tibble.\n\ntbl |> \n  drop_na()\n\n# A tibble: 1 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     2     3     5\n\n\nA final approach is to use is.na() to explicitly determine if a value is missing.\n\ntbl |> \n  mutate(a_missing = is.na(a))\n\n# A tibble: 3 × 4\n      a     b     c a_missing\n  <dbl> <dbl> <dbl> <lgl>    \n1     2     3     5 FALSE    \n2     4    NA     8 FALSE    \n3    NA     7     9 TRUE     \n\n\n\n2.8.5 Working by rows\nTibbles and the main Tidyverse functions are designed to work by columns. You do something to all the values for variable a. But, sometimes, we want to work across the tibble, comparing the value for a in the first row to the value of b in the first row, and so on. To do that, we need two tricks. First, we use rowwise() to inform R that the next set of commands should be executed across the rows.\n\ntbl |> \n  rowwise()\n\n# A tibble: 3 × 3\n# Rowwise: \n      a     b     c\n  <dbl> <dbl> <dbl>\n1     2     3     5\n2     4    NA     8\n3    NA     7     9\n\n\nNote how “# Rowwise:” is printed out. Having set up the pipe to work across rows, we need to pass c_across() to whichever function we are using, generally specifying the variables we want to use. If we don’t provide any arguments to c_across(), it will use all the columns in the tibble.\n\ntbl |> \n  rowwise() |> \n  mutate(sum_a_c = sum(c_across(c(a, c)))) |> \n  mutate(largest = max(c_across())) |> \n  mutate(largest_na = max(c_across(), na.rm = TRUE))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `largest = max(c_across())`.\nℹ In row 1.\nCaused by warning:\n! Using `c_across()` without supplying `cols` was deprecated in dplyr 1.1.0.\nℹ Please supply `cols` instead.\n\n\n# A tibble: 3 × 6\n# Rowwise: \n      a     b     c sum_a_c largest largest_na\n  <dbl> <dbl> <dbl>   <dbl>   <dbl>      <dbl>\n1     2     3     5       7       7          7\n2     4    NA     8      12      NA         12\n3    NA     7     9      NA      NA          9\n\n\n\n2.8.6 Using skim()\n\nThe skimr package offers a very useful function known as the skim() function, allowing you to get valuable information from the data set in one glance. This is similar to the glimpse() function, but it’s a little more detailed and offers some preliminary analysis of the topic.\nLet’s try skimming the nhanes dataset.\n\nskim(nhanes)\n\n\nData summary\n\n\nName\nnhanes\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n3\n\n\nnumeric\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nrace\n0\n1\n5\n8\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\neducation\n2779\n0.72\nFALSE\n4\nSom: 2267, Col: 2098, Hig: 1517, Mid: 1339\n\n\nhh_income\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\ndepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nsurvey\n0\n1.00\n2010.00\n1.00\n2009.00\n2009.00\n2010.00\n2011.00\n2011.00\n▇▁▁▁▇\n\n\nage\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nweight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nheight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nbmi\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\npulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\ndiabetes\n142\n0.99\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ngeneral_health\n2461\n0.75\n3.38\n0.94\n1.00\n3.00\n3.00\n4.00\n5.00\n▁▃▇▇▂\n\n\npregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nsleep\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\n\n\n\nThe skim() function provides information such as the mean, the number of unique counts, and even bar graphs about the distribution of the data. This information is extremely useful because it allows us to easily find things about that data and give us a starting point. For example, there are over 7000 missing values for the pregnancies variable. That means that we’re going to have to run drop_na() so that we can ignore those 7000 missing values.\nBy running skim() you can create a starting point, both for your analysis and for the creation of your graph."
  },
  {
    "objectID": "02-wrangling.html#summary",
    "href": "02-wrangling.html#summary",
    "title": "2  Wrangling",
    "section": "\n2.9 Summary",
    "text": "2.9 Summary\n\nData science is data cleaning.\nReal data is nasty.\nThis chapter covered many, many commands. You should have them all memorized by now.\nNo! That is ridiculous. We don’t have them all memorized. Why should you? The point of this chapter was to give you a tour of what you can do in R and how to do it. With that information, you have a base from which to try to solve the problems you will encounter in the future.\n\nThe key data science concept from this chapter is, again, the idea of a “distribution.” The word distribution is used in two very different ways. First, a distribution is an invisible object that you can never see or touch. It is the imaginary urn from which you can take draws. Only in very special cases will you ever be able to “know” what the distribution is, mainly in cases where there is a physical process, like a roulette wheel, which you can inspect or in the case of an assumed mathematical formula. In almost all real world data science problems, the “distribution” is a mental creation whose reality you can never confirm.\nThe second way that the word distribution is used is to refer to a vector of values, a variable in an R tibble. The 115 ages in trains are a distribution as are 1,000 draws from rnorm().\nWhether “distribution” means the imaginary object or a vector of numbers drawn from that imaginary object depends on context.\n\n\n\n\n\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  },
  {
    "objectID": "03-data.html#introduction",
    "href": "03-data.html#introduction",
    "title": "3  Data",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\n\n\n\n\n\nGetting data into and out of R is a major part of any real world data science project.\nLoad the packages which we will need in this chapter.\n\nlibrary(tidyverse)\nlibrary(dbplyr)\nlibrary(janitor)\n\nYou can find most of the files that we use for this chapter here. We can also access these files by saving the URL to R and then using it so that we can download and access the files located on the internet.\n\ngithub_url <- \"https://raw.githubusercontent.com/PPBDS/all.primer.tutorials/master/inst/tutorials/033-data-files/data/\""
  },
  {
    "objectID": "03-data.html#reading-and-writing-files",
    "href": "03-data.html#reading-and-writing-files",
    "title": "3  Data",
    "section": "\n3.2 Reading and writing files",
    "text": "3.2 Reading and writing files\nThis section reviews common file formats. We will review how to pull data from these files into R and how cto reate new files so that we can share data with other people.\n\n3.2.1 Text files\nThe most common type of data text file is “CSV,” which stands for comma separated value. In other words, CSV files are files whose values are separated by commas. Each comma from the csv file corresponds to a column, and the column names are, by default, taken from the first line of the file.\nCSV files (and their counterparts) are easily transferable between computers and programming languages and are extremely simple because they’re effectively just text files that have a special format.\nHowever, due to their simple nature, CSV files are only able to move basic data and only as text values. They also have poor support for special characters like commas, which can make the dataset harder to organize and understand by adding a new column for a few specific entries. This make CSV files good for sharing data between computers and languages, but not efficient for transporting or saving large amounts of data.\n\n3.2.1.1 Reading and writing from a CSV file\nHere’s an example of a CSV file and what it looks like. Use read_csv() from the readr package — which is one of the main packages within the tidyverse collection of packages — to load the data into R. The file argument is the file path for the CSV file.\n\n# We can access files either by using their URL or their file path. In this\n# case, we're using the GitHub URL to access the database. This is done by\n# pasting the file name into our URL using the paste0() command.\n\nfile_1 <- paste0(github_url, \"test_1.csv\")\n\nread_csv(file = file_1)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n\n\nUse write_csv() to save a tibble to a csv file. write_csv() has two main arguments: x and file. The x argument is the data set that you want to save. The file argument is the file path to which you want to save the file. The end of the file argument is the name that you want to use for the file.\n\ncsv_data <- tribble(\n  ~ `a`, ~ `b`, ~ `c`,\n      1,     2,     3,\n      4,     5,     6)\n\nwrite_csv(x = csv_data, file = \"my_csv.csv\")\n\nWhen we read the csv file again, the data shows up. This is useful for saving information both to share and for your own projects.\n\nread_csv(\"my_csv.csv\")\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n\nWhen you want to remove a file from your system, use file.remove().\n\nfile.remove(\"my_csv.csv\")\n\nSometimes, CSV files will be inconveniently formatted. Maybe they have the wrong column names, have information at the top of the file, or have comments interspersed within the file.\n\n3.2.1.2 skip\n\nConsider the following csv file: test_2.csv. Here’s what it looks like as a text file:\n\n\nTop two rows consist of junk which\nwe don't care about. Data starts on row 3.\na,b,c\n9,8,7\n4,5,6\n\n\nAs you can see, there is text at the top of this file. Often times information about how data was collected, or other relevant information, is included at the top of the data file. However, read_csv() can’t differentiate between this text and the data that we want to read, causing it to fail and output gibberish.\n\n# You can also get a csv file by using the URL of the file. This won't work for\n# all file types though.\n\nfile_2 <- paste0(github_url, \"test_2.csv\")\n\nread_csv(file_2)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Top two rows consist of junk which\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 1\n  `Top two rows consist of junk which`      \n  <chr>                                     \n1 we don't care about. Data starts on row 3.\n2 a,b,c                                     \n3 9,8,7                                     \n4 4,5,6                                     \n\n\nWe can use the skip argument to skip the first 2 text lines and allow read_csv() to work.\n\nread_csv(file = file_2,\n         skip = 2)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     9     8     7\n2     4     5     6\n\n\nNow that we’ve gotten rid of the warnings, let’s look at the other message R sends: the column specification message.\n\n3.2.1.3 col_types\n\nThe column specification message is a message that R sends to tell you what data types it is using for the column.\nData types are the types discussed in Chapter 2 Wrangling, such as characters, factors, integers, and dates. When we use a tibble, each column has to have a specific type of data. In this example, all of the columns have numbers in them. If there are characters in the column, the columns are going to have a character data type.\nTo get rid of the column specification message, use the col_types() argument and specify the data types. You can do this by just copying the column specification message and putting it as the col_types() argument.\n\nread_csv(file = file_2,\n         skip = 2,\n         col_types = cols(a = col_double(),\n                          b = col_double(),\n                          c = col_double()))\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     9     8     7\n2     4     5     6\n\n\nYou can also change the column arguments so that you get the data type that you want. Take test_7.csv.\n\ntest_7 <- paste0(github_url, \"test_7.csv\")\n\nread_csv(test_7)\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): student\ndbl (1): grade\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  grade student\n  <dbl> <chr>  \n1     1 Sam    \n2     5 Becca  \n\n\nLet’s try parsing this so that the student column is a factor and the grade column is an integer.\n\nread_csv(test_7,\n         col_types = cols(grade = col_integer(),\n                          student = col_factor()))\n\n# A tibble: 2 × 2\n  grade student\n  <int> <fct>  \n1     1 Sam    \n2     5 Becca  \n\n\nBy being clever with the columns, we can make our lives easier down the line when we graph the data.\nWe can also manipulate other arguments for CSV files.\n\n3.2.1.4 col_names and clean_names()\n\nLet’s try changing the column names in the test_3.csv file.\nAs you can see below, the file doesn’t have any column names, resulting in the first row being considered as the names for the rest of the file.\n\nfile_3 <- paste0(github_url, \"test_3.csv\")\n\nread_csv(file_3)\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): 11, 21, 33\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n   `11`  `21`  `33`\n  <dbl> <dbl> <dbl>\n1     4     5     6\n\n\nWe can fix this by changing the col_names argument.\n\nread_csv(file_3, col_names = c(\"a\", \"b\", \"c\"))\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1    11    21    33\n2     4     5     6\n\n\nYou can also create names automatically by setting col_names to FALSE.\n\nread_csv(file_3, col_names = FALSE)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): X1, X2, X3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  <dbl> <dbl> <dbl>\n1    11    21    33\n2     4     5     6\n\n\nChanging the names of the columns allows for you to call on the columns later when the data is actually a tibble. Setting the column names to something that you can understand makes it much easier to understand your code later on.\nBut what if we have good column names, they just aren’t formatted correctly? Let’s look at test_4.csv for an example.\n\nfile_4 <- paste0(github_url, \"test_4.csv\")\n\nfile_4_tibble <- read_csv(file_4)\n\nNew names:\nRows: 3 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(4): one powers, Two_Powers...2, 3_Powers, Two_Powers...4\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Two_Powers` -> `Two_Powers...2`\n• `Two_Powers` -> `Two_Powers...4`\n\nfile_4_tibble\n\n# A tibble: 3 × 4\n  `one powers` Two_Powers...2 `3_Powers` Two_Powers...4\n         <dbl>          <dbl>      <dbl>          <dbl>\n1            1              2          3              2\n2            1              4         81              5\n3            1              8         27              8\n\n\nThe function runs but the column names are not easy to work with. It is possible to access the column by using a backticks (`) around the name.\n\nfile_4_tibble$`one powers`\n\n[1] 1 1 1\n\n\nBut that can still cause problems down the line and it’s just annoying to use backticks every time you want a column name. The clean_names() function from the janitor package formats the column names so that they follow the underscore separated naming convention and are all unique.\n\nfile_4_tibble |> \n  clean_names()\n\n# A tibble: 3 × 4\n  one_powers two_powers_2 x3_powers two_powers_4\n       <dbl>        <dbl>     <dbl>        <dbl>\n1          1            2         3            2\n2          1            4        81            5\n3          1            8        27            8\n\n\nThese issues are more common when you’re pulling data off of the internet as that’s normally dirty data and can have a lot of columns formatted in weird ways.\n\n\n3.2.1.5 na\n\nAnother feature of read_csv() is the na argument.\n\nfile_5 <- paste0(github_url, \"test_5.csv\")\n\nread_csv(file_5)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): b\ndbl (2): a, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a b         c\n  <dbl> <chr> <dbl>\n1     1 .         3\n2     4 5         6\n\n\ntest_5.csv is missing a value, and it uses a . as a substitute. This makes the computer think that the period is the actual value of the data point, which is obviously not true (we want numbers instead). By default, read_csv() treats white space like spaces or tabs as a missing value, but you can set this argument directly as well.\n\nread_csv(file_5,\n         na = \".\")\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1    NA     3\n2     4     5     6\n\n\n\n3.2.1.6 comment\n\nYou can also tell the code to ignore comment lines, which may be common in a csv file that was written by a human and as such has comments in it. test_6.csv is a perfect example of this. Here’s what it looks like:\n\n\n[1] \"a,b,c\"                          \"# This is a comment line\"      \n[3] \"98,99,100\"                      \"# Here is another comment line\"\n[5] \"4,5,6\"                         \n\n\nBy setting the comment argument, we’re able to skip lines that have a certain starting point. In this case, the comment is the # sign, so we just need to include that in.\n\nread_csv(file_6, comment = \"#\")\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1    98    99   100\n2     4     5     6\n\n\nIf you ever want to skip certain lines, just use the comment argument in order to designate it as something to be skipped. This is best used with cases where the read_csv() command will not compile without it.\n\n3.2.1.7 read_delim()\n\nSo far, we’ve covered how we can organize CSV data, but at it’s core we’re working with comma separated values. But what happens when we want to read data from something that doesn’t use commas to separate the values?\nWhen our tabular data comes in a different format, we can use the read_delim() function instead. For example, a different version of test_6.csv could exist that has no column names and uses pipes (|) as the delimiter instead of commas.\nHere’s another file named delim_1, which uses the | to separate the lines instead of a comma like a normal CSV file.\n\n\n[1] \"population|town\"   \"150|Cambridge, MA\" \"92|Newton, MA\"    \n\n\nWith read_delim(), we specify the first argument as the path to the file, as done with read_csv(). Then we provide values to the delim argument to have the code use | as the separator instead of the comma.\n\n# Because delim_1 uses pipes to separate values, we can just use that as our\n# delim value. However, for more complex symbols like tab, we use something\n# different like \"\\\\t\". This varies for every symbol, but you can find most\n# delim values on the internet.\n\ndelim_1 <- paste0(github_url, \"delim_1.txt\")\n\nread_delim(delim_1, delim = \"|\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"|\"\nchr (1): town\ndbl (1): population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  population town         \n       <dbl> <chr>        \n1        150 Cambridge, MA\n2         92 Newton, MA   \n\n\nYou can often find CSV files on websites like kaggle as well as by exporting from an Excel spreadsheet. Keep in mind that data imported off of the internet is often very messy, so try to use some of the functions listed here to clean it up. For a full list of arguments and in-depth documentation about the read_csv() function, please visit this website.\n\n\n3.2.2 Excel files\nExcel is a spreadsheet program that use tables to analyze, store, or manipulate data. The tables are composed of cells which include text, numbers, or formulas. Excel files have the filename extensions .xls or .xlsx, and they’re capable of storing additional things that you cannot store in a .csv file such as fonts, text formatting, graphics, etc.\nIn order to write excel files you have to install complex packages, and they are hard to create. Writing excel files is beyond the scope of this Primer.\nThis makes Excel files valuable because they’re commonly accepted and usable (Microsoft Excel is a very common program), but they’re also hard to use because you can’t write new data into them. As such, Excel files are common for data that was originally in Excel, like accounting data or other spreadsheet applications.\nReading Excel files is easy. To do so, we use the read_excel() function from the readxl package.\n\nlibrary(readxl)\n\n# Unfortunately, it is not possible to read Excel files directly from the web.\n# So we download the file by hand and then read it in from the current working\n# directory. Note that the \"proper\" way of handling this would be to create a\n# temp directory with tempdir(), download the file into that directory, read it,\n# and then delete the temp directory. That way, you would not have random\n# downloaded files hanging around.\n\n# The mode = \"wb\" is a necessary addition for Windows users because Windows is\n# weird. It's not necessary on MacOS and may cause an error as well.\n\ndownload.file(url = paste0(github_url, \"excel_1.xlsx\"), \n              destfile = \"example_excel.xlsx\", mode = \"wb\")\n\n\nread_excel(path = \"example_excel.xlsx\")\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n\nIf the .xlsx file has multiple sheets, you have to use the sheet argument to specify the sheet number or name. The read_excel() function also has arguments similar to the read_csv() function such as col_names, col_types, and na.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 RDS files\nOne very important aspect of R is saving objects into RDS files, which store a single R object into a file. These files allow us to save R objects such as plots and tibbles into R and then reload the object that it contains later without re-running the code that made it. This is especially useful for when you’re dealing with bulk data and want to save the plot that comes with it for later so that you don’t have to do all of the data wrangling and plotting again. With a RDS file, you save the entire object, allowing you to do more things with it later without having to go through all of the code. However, RDS files are limited to R projects only, and they’re incomprehensible both to human eyes and to other programming languages. This makes RDS files ideal for saving objects temporarily in your own project or for sharing objects to other R users.\nTake the following R object, a graph of the iris data set.\n\niris_p <- iris |> \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_jitter() +\n  labs(title = \"Sepal Dimensions of Various Species of Iris\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\")\niris_p\n\n\n\n\nWhen we save to a RDS file, we use the function write_rds(). Just like write_csv(), this function has two main arguments: x and file. The x argument is the object that you want to save. The file argument is the file path where you want to save the file. This determines the name that you use for the file.\n\nwrite_rds(x = iris_p, file = \"iris_p.rds\")\n\nread_rds() reads the file back into R. Just like read_csv() read_rds() has one main argument, which is the path to the file that you are wanting to read into R.\n\nread_rds(file = \"iris_p.rds\")\n\n\n\n\nWe can then use that R object in more operations, such as adding a trend line.\n\nrds_p <- read_rds(file = \"iris_p.rds\")\nrds_p + \n  geom_smooth(method = \"loess\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\nBy saving the iris_p plot in a RDS file, we eliminate the time needed to calculate and generate that plot again because we can use the saved information. We can then use the object by reading the file back into r and using it like any normal plot, adding new layers and doing new operations.\nThere are also .Rdata files which can store multiple objects, but RDS files can accomplish a similar task. This makes it much easier to use RDS files for anything that you want to keep in R.\n\n3.2.4 JSON\nAn increasingly common format for sharing data is JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and from which you can obtain data.\nJSON files are a minimal readable format that structures data, and they’re commonly used to transmit data between a server and a web application like a website. For people familiar with the Javascript coding language, you’ll likely see the similarities between the JSON file format and the Javascript syntax. This makes JSON files ideal for internet transport, but they don’t see much use within a project like RDS files do.\nThe functions fromJSON() and toJSON() allow you to convert between R objects and JSON. Both functions come from the jsonlite package.\n\nlibrary(jsonlite)\n\nThe function toJSON() converts a tibble to JSON format. Consider the example_1 tibble:\n\nexample_1 <- tibble(name= c(\"Miguel\", \"Sofia\", \"Aya\", \"Cheng\"), \n                    student_id = 1:4, exam_1 = c(85, 94, 87, 90), \n                    exam_2 = c(86, 93, 88, 91))\n\nexample_1\n\n# A tibble: 4 × 4\n  name   student_id exam_1 exam_2\n  <chr>       <int>  <dbl>  <dbl>\n1 Miguel          1     85     86\n2 Sofia           2     94     93\n3 Aya             3     87     88\n4 Cheng           4     90     91\n\n\n\n# The pretty argument adds indentation and whitespace when TRUE.\n\ntoJSON(example_1, pretty = TRUE) \n\n[\n  {\n    \"name\": \"Miguel\",\n    \"student_id\": 1,\n    \"exam_1\": 85,\n    \"exam_2\": 86\n  },\n  {\n    \"name\": \"Sofia\",\n    \"student_id\": 2,\n    \"exam_1\": 94,\n    \"exam_2\": 93\n  },\n  {\n    \"name\": \"Aya\",\n    \"student_id\": 3,\n    \"exam_1\": 87,\n    \"exam_2\": 88\n  },\n  {\n    \"name\": \"Cheng\",\n    \"student_id\": 4,\n    \"exam_1\": 90,\n    \"exam_2\": 91\n  }\n] \n\n\nThe function fromJSON() converts JSON format to a tibble.\n\njson_format_ex <-\n'[\n  {\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]'\n\nfromJSON(json_format_ex) \n\n    Name Age Occupation\n1  Mario  32    Plumber\n2  Peach  21   Princess\n3   <NA>  NA       <NA>\n4 Bowser  NA      Koopa\n\n\nMake sure to follow the JSON format exactly when you’re writing JSON files, as the format is what makes them special and what allows them to work."
  },
  {
    "objectID": "03-data.html#databases",
    "href": "03-data.html#databases",
    "title": "3  Data",
    "section": "\n3.3 Databases",
    "text": "3.3 Databases\n\n\n\n\nDROP TABLE is a particularly infamous SQL command\n\n\n\n\nDatabases are one of the most common methods of storing data, as they are capable of storing large amounts of data while also allowing for multiple people to access and change them at the same time. Think of these like a giant book with a bunch of tables that hold data. These are useful by themselves, but with the advent of computers we are now able to use relational databases.\nA relational database is a database that has the tables interact with one another based on common data, allowing you to create custom tables from an existing set of records. For example, a relational database may hold multiple tables that use the same ID to keep track of different information, like one table having an ID and a movie name while another has the ID and the rating. You can then combine those two in your code, creating a table with the ID, name, and rating. This allows the person who made the database to easily put in new data as well as allow you to pull out some data at a time without loading the entire database.\nIt’s common to use and interact with these databases in the real world due to most businesses using relational databases to keep track of their data and to update it at their leisure. It’s not uncommon for databases to hold thousands or even millions of rows due to the need to keep track of data.\nIn this section, we’ll be going over how to pull data from these databases and how to interact with the data without using the entire database.\n\n3.3.1 Reading data from a SQLite database\nSQLite is probably the simplest relational database that one can use in combination with R. SQLite databases are self-contained and usually stored and accessed locally on one computer, instead of on the internet or through the cloud. Data is usually stored in a file with a .db extension. Similar to Excel files, these are not plain text files and cannot be read in a plain text editor.\nThe first thing you need to do to read data into R from a database is to connect to the database. We do that using dbConnect() function from the DBI (database interface) package. This does not read in the data, but simply tells R where the database is and opens up a communication channel.\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# This example uses a different github URL, so you can't use the paste0() trick\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/can_lang.db?raw=true\",\n              destfile = \"example_db.db\", mode = \"wb\")\n\ncon_lang_data <- dbConnect(RSQLite::SQLite(), \"example_db.db\")\n\nOften relational databases have many tables, and their power comes from the useful ways they can be joined. Thus anytime you want to access data from a relational database, you need to know the table names. You can get the names of all the tables in the database using dbListTables().\n\ntables <- dbListTables(con_lang_data)\ntables\n\n[1] \"lang\"\n\n\nWe only get one table name returned, which tells us that there is only one table in this database. To reference a table in the database to do things like select columns and filter rows, we use the tbl() function from the dbplyr package. The package dbplyr allows us to work with data stored in databases as if they were local data frames, which is useful because we can do a lot with big datasets without actually having to bring these vast amounts of data into your computer!\n\nlang_db <- tbl(con_lang_data, \"lang\")\nlang_db\n\n# Source:   table<lang> [?? x 6]\n# Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/example_db.db]\n   category                              langu…¹ mothe…² most_…³ most_…⁴ lang_…⁵\n   <chr>                                 <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 Aboriginal languages                  Aborig…     590     235      30     665\n 2 Non-Official & Non-Aboriginal langua… Afrika…   10260    4785      85   23415\n 3 Non-Official & Non-Aboriginal langua… Afro-A…    1150     445      10    2775\n 4 Non-Official & Non-Aboriginal langua… Akan (…   13460    5985      25   22150\n 5 Non-Official & Non-Aboriginal langua… Albani…   26895   13135     345   31930\n 6 Aboriginal languages                  Algonq…      45      10       0     120\n 7 Aboriginal languages                  Algonq…    1260     370      40    2480\n 8 Non-Official & Non-Aboriginal langua… Americ…    2685    3020    1145   21930\n 9 Non-Official & Non-Aboriginal langua… Amharic   22465   12785     200   33670\n10 Non-Official & Non-Aboriginal langua… Arabic   419890  223535    5585  629055\n# … with more rows, and abbreviated variable names ¹​language, ²​mother_tongue,\n#   ³​most_at_home, ⁴​most_at_work, ⁵​lang_known\n\n\nAlthough it looks like we just got a data frame from the database, we didn’t! It’s a reference, showing us data that is still in the SQLite database (note the first two lines of the output). It does this because databases are often more efficient at selecting, filtering and joining large data sets than R. And typically, the database will not even be stored on your computer, but rather a more powerful machine somewhere on the web. So R is lazy and waits to bring this data into memory until you explicitly tell it. To do so, we use the collect() function.\nHere we will filter for only rows in the Aboriginal languages category according to the 2016 Canada Census, and then use collect() to finally bring this data into R as a data frame.\n\naboriginal_lang_db <- filter(lang_db, category == \"Aboriginal languages\")\naboriginal_lang_db\n\n# Source:   SQL [?? x 6]\n# Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/example_db.db]\n   category             language                 mothe…¹ most_…² most_…³ lang_…⁴\n   <chr>                <chr>                      <dbl>   <dbl>   <dbl>   <dbl>\n 1 Aboriginal languages Aboriginal languages, n…     590     235      30     665\n 2 Aboriginal languages Algonquian languages, n…      45      10       0     120\n 3 Aboriginal languages Algonquin                   1260     370      40    2480\n 4 Aboriginal languages Athabaskan languages, n…      50      10       0      85\n 5 Aboriginal languages Atikamekw                   6150    5465    1100    6645\n 6 Aboriginal languages Babine (Wetsuwet'en)         110      20      10     210\n 7 Aboriginal languages Beaver                       190      50       0     340\n 8 Aboriginal languages Blackfoot                   2815    1110      85    5645\n 9 Aboriginal languages Carrier                     1025     250      15    2100\n10 Aboriginal languages Cayuga                        45      10      10     125\n# … with more rows, and abbreviated variable names ¹​mother_tongue,\n#   ²​most_at_home, ³​most_at_work, ⁴​lang_known\n\n\n\naboriginal_lang_data <- collect(aboriginal_lang_db)\naboriginal_lang_data\n\n# A tibble: 67 × 6\n   category             language                 mothe…¹ most_…² most_…³ lang_…⁴\n   <chr>                <chr>                      <dbl>   <dbl>   <dbl>   <dbl>\n 1 Aboriginal languages Aboriginal languages, n…     590     235      30     665\n 2 Aboriginal languages Algonquian languages, n…      45      10       0     120\n 3 Aboriginal languages Algonquin                   1260     370      40    2480\n 4 Aboriginal languages Athabaskan languages, n…      50      10       0      85\n 5 Aboriginal languages Atikamekw                   6150    5465    1100    6645\n 6 Aboriginal languages Babine (Wetsuwet'en)         110      20      10     210\n 7 Aboriginal languages Beaver                       190      50       0     340\n 8 Aboriginal languages Blackfoot                   2815    1110      85    5645\n 9 Aboriginal languages Carrier                     1025     250      15    2100\n10 Aboriginal languages Cayuga                        45      10      10     125\n# … with 57 more rows, and abbreviated variable names ¹​mother_tongue,\n#   ²​most_at_home, ³​most_at_work, ⁴​lang_known\n\n\nWhy bother to use the collect() function? The data looks pretty similar in both outputs shown above. And dbplyr provides lots of functions similar to filter() that you can use to directly feed the database reference (i.e. what tbl() gives you) into downstream analysis functions (e.g., ggplot2 for data visualization and lm for linear regression modeling). However, this does not work in every case; look what happens when we try to use nrow to count rows in a data frame:\n\nnrow(aboriginal_lang_db)\n\n[1] NA\n\n\nor tail to preview the last 6 rows of a data frame:\ntail(aboriginal_lang_db)\n## Error: tail() is not supported by sql sources\nThese functions only work when we use the version that we used collect() on:\n\nnrow(aboriginal_lang_data)\n\n[1] 67\n\n\n\ntail(aboriginal_lang_data)\n\n# A tibble: 6 × 6\n  category             language                  mothe…¹ most_…² most_…³ lang_…⁴\n  <chr>                <chr>                       <dbl>   <dbl>   <dbl>   <dbl>\n1 Aboriginal languages Tahltan                        95       5       0     265\n2 Aboriginal languages Thompson (Ntlakapamux)        335      20       0     450\n3 Aboriginal languages Tlingit                        95       0      10     260\n4 Aboriginal languages Tsimshian                     200      30      10     410\n5 Aboriginal languages Wakashan languages, n.i.…      10       0       0      25\n6 Aboriginal languages Woods Cree                   1840     800      75    2665\n# … with abbreviated variable names ¹​mother_tongue, ²​most_at_home,\n#   ³​most_at_work, ⁴​lang_known\n\n\nIn order to delete and stop using an SQLite server, you need to first disconnect the file from the connection be using dbDisconnect() and passing in the connection object as an argument. You can then safely delete the database file from your computer by using file.remove().\n\ndbDisconnect(con_lang_data)\nfile.remove(\"example_db.db\")\n\nSome operations will not work to extract columns or single values from the reference given by the tbl function. Thus, once you have finished your data wrangling of the tbl() database reference object, it is advisable to bring it into your local machine’s memory using collect() as a data frame.\n\nWarning: Usually, databases are very big! Reading the object into your local machine may give an error or take a lot of time to run so be careful if you plan to do this!\n\n\n3.3.2 Interacting with SQLite databases\nNow that we’ve figured out how to get data from a database, let’s look at how to wrangle data within that database.\nBecause databases normally contain large amounts of data, it’s advisable to do your wrangling before you use collect() to transform the database table into a tibble. This stops you from pulling large amounts of data onto your computer and just ignoring it.\nSo let’s try pulling in a database and see how we can manipulate it.\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Again, use mode = \"wb\" if you're using a Windows operating system.\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/chinook.db?raw=true\",\n             dest = \"chinook.db\", mode = \"wb\")\n\ncon <- dbConnect(RSQLite::SQLite(), \"chinook.db\")\n\nFirst off, look at what data this database holds.\n\ndbListTables(con)\n\n [1] \"albums\"          \"artists\"         \"customers\"       \"employees\"      \n [5] \"genres\"          \"invoice_items\"   \"invoices\"        \"media_types\"    \n [9] \"playlist_track\"  \"playlists\"       \"sqlite_sequence\" \"sqlite_stat1\"   \n[13] \"tracks\"         \n\n\nThere are 13 tables in this database. Let’s access the first couple of tables, just so that we can get a good look at them and see how they’re structured.\n\nalbums <- tbl(con, \"albums\")\nalbums\n\n# Source:   table<albums> [?? x 3]\n# Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/chinook.db]\n   AlbumId Title                                 ArtistId\n     <int> <chr>                                    <int>\n 1       1 For Those About To Rock We Salute You        1\n 2       2 Balls to the Wall                            2\n 3       3 Restless and Wild                            2\n 4       4 Let There Be Rock                            1\n 5       5 Big Ones                                     3\n 6       6 Jagged Little Pill                           4\n 7       7 Facelift                                     5\n 8       8 Warner 25 Anos                               6\n 9       9 Plays Metallica By Four Cellos               7\n10      10 Audioslave                                   8\n# … with more rows\n\nartists <- tbl(con, \"artists\")\nartists\n\n# Source:   table<artists> [?? x 2]\n# Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/chinook.db]\n   ArtistId Name                \n      <int> <chr>               \n 1        1 AC/DC               \n 2        2 Accept              \n 3        3 Aerosmith           \n 4        4 Alanis Morissette   \n 5        5 Alice In Chains     \n 6        6 Antônio Carlos Jobim\n 7        7 Apocalyptica        \n 8        8 Audioslave          \n 9        9 BackBeat            \n10       10 Billy Cobham        \n# … with more rows\n\n\nAs you can see, these tables both have a common column: the ArtistId column. However, they all have different information linked to that ID, such as “albums” having the albums that artist produced and “artists” having the name of that artist.\nHere is the full relationship diagram of the database that we’re using."
  },
  {
    "objectID": "03-data.html#webscraping",
    "href": "03-data.html#webscraping",
    "title": "3  Data",
    "section": "\n3.4 Webscraping",
    "text": "3.4 Webscraping\nPlease read the web scraping chapter from R for Data Science (2e)."
  },
  {
    "objectID": "03-data.html#working-with-apis",
    "href": "03-data.html#working-with-apis",
    "title": "3  Data",
    "section": "\n3.5 Working with APIs",
    "text": "3.5 Working with APIs\n\n\n\n\n\n\n\n\n\n“API” stands for Application Program Interface. They allow us to access open data from government agencies, companies, and other organizations. API provides the rules for software applications to interact with one another. Open data APIs provide the rules you need to know to write R code to request and pull data from the organization’s web server into R. Usually, some of the computational burden of querying and subsetting the data is taken on by the source’s server, to create the subset of requested data to pass to your computer. In practice, this means you can often pull the subset of data you want from a very large available dataset without having to download the full dataset and load it locally into your R session.\nAs an overview, the basic steps for accessing and using data from a web API when working in R are:\n\nFigure out the API rules for HTTP requests\nWrite R code to create a request in the proper format\nSend the request using GET or POST HTTP methods\nOnce you get back data from the request, parse it into an easier-to-use format if necessary\n\nTo get the data from an API, you should first read the organization’s API documentation. An organization will post details on what data is available through their API(s), as well as how to set up HTTP requests to get that data. To request the data through the API, you will typically need to send the organization’s web server an HTTP request using a GET or POST method. The API documentation details will typically show an example GET or POST request for the API, including the base URL to use and the possible query parameters that can be used to customize the dataset request.\nHere is an example:\nThe National Aeronautics and Space Administration (NASA) has an API for pulling the Astronomy Picture of the Day. In their API documentation, they specify that the base URL for the API request should be https://api.nasa.gov/planetary/apod and that you can include parameters to specify the date of the daily picture you want, whether to pull a high-resolution version of the picture, and a NOAA API key you have requested from NOAA.\nMany organizations will require you to get an API key and use this key in each of your API requests. This key allows the organization to control API access, including enforcing rate limits per user. API rate limits restrict how often you can request data (such as an hourly limit of 1,000 requests per user for NASA APIs).\nAPI keys should be kept private, so if you are writing code that includes an API key, be very careful not to include the actual key in any code that is public (even any code in public GitHub repositories). To ensure privacy, save the value of your key in a file named .Renviron in your home directory. This file should be a plain text file and must end in a blank line. Once you’ve saved your API key to a global variable in that file (e.g., with a line added to the .Renviron file like NOAA_API_KEY = “abdafjsiopnab038”), you can assign the key value to an R object in an R session using the Sys.getenv function (e.g., noaa_api_key <- Sys.getenv(“NOAA_API_KEY”)), and then use the object noaa_api_key anywhere you would otherwise have used the character string with your API key.\nTo find more R packages for accessing and exploring open data, check out the Open Data CRAN task view. You can also browse through the ROpenSci packages, all of which have GitHub repositories where you can further explore how each package works! ROpenSci is an organization with the mission to create open software tools for science. If you create your own package to access data relevant to scientific research through an API, consider submitting it for peer-review through ROpenSci.\nThe riem package, developed by Maelle Salmon and an ROpenSci package, is an excellent and straightforward example of how you can use R to pull open data through a web API. This package allows you to pull weather data from airports around the world directly from the Iowa Environmental Mesonet. To show you how to pull data into R through an API, in this section we will walk you through code in the riem package or code based closely on code in the package.\nTo get a certain set of weather data from the Iowa Environmental Mesonet, you can send an HTTP request specifying a base URL, https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/, as well as some parameters describing the subset of dataset you want (e.g., date ranges, weather variables, output format). Once you know the rules for the names and possible values of these parameters (more on that below), you can submit an HTTP GET request using the functionGET() from the httr package.\nWhen you are making an HTTP request using the GET() or POST() functions from the httr package, you can include the key-value pairs for any query parameters as a list object in the query argument of the function. For example, suppose you want to get wind speed in miles per hour (data = “sped”) for Denver, CO, (station = “DEN”) for the month of June 2016 (year1 = “2016”, month1 = “6”, etc.) in Denver’s local time zone (tz = “America/Denver”) and in a comma-separated file (format = “comma”). To get this weather dataset, you can run:\n\nlibrary(httr)\n\n\nlibrary(httr)\nmeso_url <- \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/\"\ndenver <- GET(url = meso_url,\n                    query = list(station = \"DEN\",\n                                 data = \"sped\",\n                                 year1 = \"2016\",\n                                 month1 = \"6\",\n                                 day1 = \"1\",\n                                 year2 = \"2016\",\n                                 month2 = \"6\",\n                                 day2 = \"30\",\n                                 tz = \"America/Denver\",\n                                 format = \"comma\")) |>\n  content() |> \n  read_csv(skip = 5, na = \"M\")\n\nRows: 9108 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): station\ndbl  (1): sped\ndttm (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# There are 9,106 rows of data to look at! Let's just look at subset for our\n# purposes.\n\ndenver |> \n  slice(1:3)\n\n# A tibble: 3 × 3\n  station valid                sped\n  <chr>   <dttm>              <dbl>\n1 DEN     2016-06-01 00:00:00   9.2\n2 DEN     2016-06-01 00:05:00   9.2\n3 DEN     2016-06-01 00:10:00   6.9\n\n\nThe content() call extracts the content from the response to the HTTP request sent by the GET() function. The Iowa Environmental Mesonet API offers the option to return the requested data in a comma-separated file (format = “comma” in the GET request), so here content and read_csv() are used to extract and read in that csv file. Usually, data will be returned in a JSON format instead.\nThe only tricky part of this process is figuring out the available parameter names (e.g., station) and possible values for each (e.g., “DEN” for Denver). Currently, the details you can send in an HTTP request through Iowa Environmental Mesonet’s API include:\n\nA four-character weather station identifier (station)\nThe weather variables (e.g., temperature, wind speed) to include (data)\nStarting and ending dates describing the range for which you’d like to pull data (year1, month1, day1, year2, month2, day2)\nThe time zone to use for date-times for the weather observations (tz)\nDifferent formatting options (e.g., delimiter to use in the resulting data file [format], whether to include longitude and latitude)\n\nTypically, these parameter names and possible values are explained in the API documentation. In some cases, however, the documentation will be limited. In that case, you may be able to figure out possible values, especially if the API specifies a GET rather than POST method, by playing around with the website’s point-and-click interface and then looking at the url for the resulting data pages. For example, if you look at the Iowa Environmental Mesonet’s page for accessing this data, you’ll notice that the point-and-click web interface allows you the options in the list above, and if you click through to access a dataset using this interface, the web address of the data page includes these parameter names and values.\nThe riem package implements all these ideas in three very clean and straightforward functions. You can explore the code behind this package and see how these ideas can be incorporated into a small R package, in the /R directory of the package’s GitHub page.\nR packages already exist for many open data APIs. If an R package already exists for an API, you can use functions from that package directly, rather than writing your own code using the API protocols and httr functions. Other examples of existing R packages to interact with open data APIs include:\n\ntwitteR: Twitter\nrnoaa: National Oceanic and Atmospheric Administration\nQuandl: Quandl (financial data)\nRGoogleAnalytics: Google Analytics\ncensusr, acs: United States Census\nWDI, wbstats: World Bank\nGuardianR, rdian: The Guardian Media Group\nblsAPI: Bureau of Labor Statistics\nrtimes: New York Times\ndataRetrieval, waterData: United States Geological Survey If an R package doesn’t exist for an open API and you’d like to write your own package, find out more about writing API packages with this vignette for the httr package. This document includes advice on error handling within R code that accesses data through an open API.\n\nInformation for this section on API’s was taken from Mastering Software Development in R textbook, authored by Roger D. Peng, Sean Kross, and Brooke Anderson."
  },
  {
    "objectID": "03-data.html#summary",
    "href": "03-data.html#summary",
    "title": "3  Data",
    "section": "\n3.6 Summary",
    "text": "3.6 Summary\n\nPulling data into and out of R is a key factor in the data science process.\nUse files like CSV, Excel, RDS, JSON, and SQL files/databases to organize and share your data.\nUse SelectorGadget to pull data off of websites.\nUse an API when to get data from government agencies or companies.\nIn this chapter, we looked at a few common file formats and used the readr package to read them and pull data from them so that we can use that data in our plots.\nWe also looked at databases and how we can pull in a few select pieces of data from them so that we aren’t overloading our computers with thousands or millions of data rows. We went over how we can write SQL queries so that we can acheive some special effects without causing errors or detonating our computers.\nAnd finally, we looked at how we can pull data from websites both through webscraping and through APIs, letting us pull data from the internet quickly and easily. This lets us find data for our projects and load it into our R session without having to create it ourselves or download a file from the internet.\nIn the end, this chapter is about getting data from other people and using it inside your own projects.\n\n\n\n\nMake sure to use up to date data."
  },
  {
    "objectID": "04-rubin-causal-model.html#preceptor-table",
    "href": "04-rubin-causal-model.html#preceptor-table",
    "title": "4  Rubin Causal Model",
    "section": "4.1 Preceptor Table",
    "text": "4.1 Preceptor Table\nA Preceptor Table is a table with rows and columns such that, if none of the data is missing, the thing we want to know is trivial to calculate. Preceptor Tables vary in the number of their rows and columns. We use question marks to indicate missing data in a Preceptor Table. The rows in the Preceptor Table are the units — people, galaxies, oak trees — which are the subjects of interest. Even the simplest Preceptor Table will have two columns. The first is an ID column which serves to label each unit. The second column is the outcome of interest, the variable we are trying to predict/understand/change.\nAssume that there are five adult brothers and you are given four of their heights. What is the average height of all five brothers? Consider a Preceptor Table for this problem:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcome\n      \n    \n    \n      Height (cm)\n    \n  \n  \n    Robert\n178\n    Andy\n?\n    Beau\n172\n    Ishan\n173\n    Nicholas\n165\n  \n  \n  \n\n\n\n\nIn this case, we have a row for each brother and a column for their heights. An individual unit is a brother. An outcome is that brother’s height. We will always have an ID column in Preceptor Tables so that we can identify different units. It is always furthest to the left. In addition to an ID column, we will always have an Outcome column displaying the main variable of interest. For simplicity, we often leave out the ID column in this chapter. But it is always there in the background. It must be possible for us to tell apart the different units.\nTo calculate the average height, we need to know Andy’s height – our missing data. Keep in mind that there is a truth out there, a state of the world independent of our knowledge of it. Andy is a specific height. If we had a complete Preceptor Table, with no missing values, we could calculate the average height of the brothers exactly. No fancy statistics would be needed, just arithmetic.\nImplicitly in every Preceptor Table is a notion of time. When, exactly, are we making this prediction? Since heights of adults change very, very slowly, we can ignore for this problem whether or not we are making this prediction tomorrow or a year from now. But, if we really want to know the average height of the brothers 40 years from now, we would need to adjust our estimate since people shrink with age.\n\n4.1.1 Harvard Height\nConsider a more complex problem. We have the heights of 100 Harvard students, and from that we want to know the average height of all students in the school.\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcome\n      \n    \n    \n      Height (cm)\n    \n  \n  \n    Student 1\n?\n    Student 2\n?\n    ...\n...\n    Student 473\n172\n    Student 474\n?\n    ...\n...\n    Student 3,258\n?\n    Student 3,259\n162\n    ...\n...\n    Student 6,700\n?\n  \n  \n  \n\n\n\n\nAgain, are these 100 students randomly sampled? Could we estimate the 90th percentile of height in the student population? These questions are more complicated, and we might be less confident in our best guess. Note, also, that we will sometimes not know exactly how many rows there are in the Precptor Table. In that case, we would use “Student N” as the last ID, where N is the total number of students at Harvard.\n\n\n4.1.2 Population Table\nThe Population Table is distinct from the Preceptor Table. The aim of the Population Table is to illustrate the broader population in which we are interested, while also including the data from our Preceptor Table and from our dataset. This table has three sources of data: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\nThe rows in the Preceptor Table contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (year and age) but they do not contain any outcome results (height). We are trying to answer questions about the height of Harvard students in 2022, so our Age column will read somewhere between 15 and 27 and our Year entries for these rows will read “2022.”\n\n\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a survey of Harvard students in 2015, so Age value for those students — none of whom are still at Harvard — will be their ages in 2015, and the Year value for these rows will be “2015.”\n\n\n\nOur Population rows contain no data. These are subjects which fall within our population, but for which we have no data. As such, all values, other than year, are missing.\n\n\n\n\n\n\n  \n    \n      Population Table\n    \n    \n  \n  \n    \n      Source\n      Year\n      Age\n      Height\n    \n  \n  \n    Population\n\n2010\n\n?\n\n?\n\n    ...\n\n...\n\n...\n\n...\n\n    Data\n\n2015\n\n18\n\n180\n\n    Data\n\n2015\n\n23\n\n163\n\n    ...\n\n...\n\n...\n\n...\n\n    Population\n\n2018\n\n?\n\n?\n\n    ...\n\n...\n\n...\n\n...\n\n    Preceptor Table\n\n2022\n\n19\n\n?\n\n    ...\n\n...\n\n...\n\n...\n\n    Population\n\n2025\n\n?\n\n?\n\n    Preceptor Table\n\n?\n\n?\n\n172\n\n    Preceptor Table\n\n?\n\n?\n\n...\n\n  \n  \n  \n\n\n\n\nImplicit in the Preceptor Table is a notion of time. Now that we can see our actual data compared with our greater population and our desired data, we must expand our observations. That is to say that, given our data is sourced from 2015 and our desired data is from 2022, we must include a greater time span in our population.\n\nAs such, we will see that rows from our larger population may include anywhere between 2010 and 2025. This is a ballpark range. Height is relatively stable, so it is reasonable to assume that the population is stable ove a longer time period."
  },
  {
    "objectID": "04-rubin-causal-model.html#causal-effect",
    "href": "04-rubin-causal-model.html#causal-effect",
    "title": "4  Rubin Causal Model",
    "section": "4.2 Causal effect",
    "text": "4.2 Causal effect\n\n\n\n\n\nThis study, which shows the impact of exposure to Spanish-speaking individuals on attitudes towards immigration, was conducted by Ryan Enos.\n\n\n\n\nThe Rubin Causal Model (RCM) is based on the idea of potential outcomes. For example, Enos (2014) measured attitudes toward immigration among Boston commuters. Individuals were exposed to one of two possible conditions, and then their attitudes towards immigrants were recorded. One condition was waiting on a train platform near individuals speaking Spanish. The other was being on a train platform without Spanish-speakers. To calculate the causal effect of having Spanish-speakers nearby, we need to compare the outcome for an individual in one possible state of the world (with Spanish-speakers) to the outcome for that same individual in another state of the world (without Spanish-speakers). However, it is impossible to observe both potential outcomes at once. One of the potential outcomes is always missing, since a unit cannot travel back in time, and experience both treatments. This dilemma is the Fundamental Problem of Causal Inference.\nIn most circumstances, we are interested in comparing two experimental manipulations, one generally termed “treatment” and the other “control.” The difference between the potential outcome under treatment and the potential outcome under control is a “causal effect” or a “treatment effect.” According to the RCM, the causal effect of being on the platform with Spanish-speakers is the difference between what your attitude would have been under “treatment” (with Spanish-speakers) and under “control” (no Spanish-speakers).\nThe commuter survey consisted of three questions, each measuring agreement on a 1 to 5 integer scale, with 1 being liberal and 5 being conservative. For each person, the three answers were summed, generating an overall measure of attitude toward immigration which ranged from 3 (very liberal) to 15 (very conservative). If your attitude towards immigrants would have been a 13 after being exposed to Spanish-speakers and a 9 with no such exposure, then the causal effect of being on a platform with Spanish-speakers is a 4-point increase in your score.\n\nWe will use the symbol \\(Y\\) to represent potential outcomes, the variable we are interested in understanding and modeling. \\(Y\\) is called the response or outcome variable. It is the variable we want to “explain.” In our case this would be the attitude score. If we are trying to understand a causal effect, we need two symbols so that control and treated values can be represented separately: \\(Y_t\\) and \\(Y_c\\).\n\n4.2.1 Potential outcomes\nSuppose that Yao is one of the commuters surveyed in this experiment. If we were omniscient, we would know the outcomes for Yao under both treatment (with Spanish-speakers) and control (no Spanish-speakers), and we’d be able to ignore the Fundamental Problem of Causal Inference. We can show this using a Preceptor Table. Calculating the number we are interested in is trivial because none of the data is missing.\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n    \n    \n      Attitude if Treated\n      Attitude if Control\n    \n  \n  \n    Yao\n13\n9\n  \n  \n  \n\n\n\n\nRegardless of what the causal effect is for other subjects, the causal effect for Yao of being on the train platform with Spanish-speakers is a shift towards a more conservative attitude.\nUsing the response variable — the actual symbol rather than a written description — makes for a more concise Preceptor Table.\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n    \n    \n      $$Y_t$$\n      $$Y_c$$\n    \n  \n  \n    Yao\n\n13\n\n9\n\n  \n  \n  \n\n\n\n\nThe “causal effect” is the difference between Yao’s potential outcome under treatment and his potential outcome under control.\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t$$\n      $$Y_c$$\n      $$Y_t - Y_c$$\n    \n  \n  \n    Yao\n\n13\n\n9\n\n+4\n\n  \n  \n  \n\n\n\n\n\nRemember that, in the real world, we will have a bunch of missing data! We can not use simple arithmetic to calculate the causal effect on Yao’s attitude toward immigration. Instead, we will be required to estimate it. An estimand is some unknown variable in the real world that we are trying to measure. In this case, it is \\(Y_{t}-Y_{c}\\), not \\(+4\\). An estimand is not the value you calculated, but is rather the unknown variable you want to estimate.\n\n\n\n\n\nDon Rubin was a Professor of Statistics at Harvard.\n\n\n\n\n\n\n4.2.2 Causal and predictive models\nCausal inference is often compared with prediction. In prediction, we want to know an outcome, \\(Y\\). In causal inference, we want to know a function of potential outcomes, such as the treatment effect: \\(Y_t - Y_c\\).\nThese are both missing data problems. Prediction involves estimating an outcome variable that we don’t have, and thus is missing, whether because it is in the future or because it is from data that we are unable to collect. Thus, prediction is the term for using statistical inference to fill in missing data for individual outcomes. Causal inference, however, involves filling in missing data for more than one potential outcome. This is unlike prediction, where only one outcome can ever be observed, even in principle.\n\n\nKey point: In a predictive model, there is only one \\(Y\\) value for each unit. This is very different to the RCM where there are (at least) two potential outcomes (treatment and control). There is only one outcome column in a predictive model, whereas there are two or more in a causal model.\n\nWith a predictive model, we cannot infer what would happen to the outcome \\(Y\\) if we changed \\(X\\) for a given unit. We can only compare two units, one with one value of \\(X\\) and another with a different value of \\(X\\).\nIn a sense, all models are predictive. However, only a subset of models are causal, meaning that, for a given individual, you can change the value \\(X\\) and observe a change in outcome, \\(Y(u)\\), and from that calculate a causal effect.\n\n\n4.2.3 No causation without manipulation\nIn order for a potential outcome to make sense, it must be possible, at least a priori. For example, if there is no way for Yao, under any circumstance, to ever be in the train study, then \\(Y_{t}\\) is impossible for him. It can never happen. And if \\(Y_{t}\\) can never be observed, even in theory, then the causal effect of treatment on Yao’s attitude is undefined.\nThe causal effect of exposure to Spanish-speakers is well defined because it is the simple difference of two potential outcomes, both of which might happen. In this case, we (or something else) can manipulate the world, at least conceptually, so that it is possible that one thing or a different thing might happen.\nThis definition of causal effects becomes much more problematic if there is no way for one of the potential outcomes to happen, ever. For example, what is the causal effect of Yao’s height on his weight? It might seem we would just need to compare two potential outcomes: Yao’s weight under the treatment (where treatment is defined as being 3 inches taller) and Yao’s weight under the control (where control is defined as his current height).\nA moment’s reflection highlights the problem: we can’t increase Yao’s height. There is no way to observe, even conceptually, what Yao’s weight would be if he were taller because there is no way to make him taller. We can’t manipulate Yao’s height, so it makes no sense to investigate the causal effect of height on weight. Hence the slogan: No causation without manipulation.\nThis then raises the question of what can and cannot be manipulated. If something cannot be manipulated, we should not consider it causal. So can race ever be considered causal? What about sex? A genetic condition like color-blindness? Can we manipulate these characteristics? In the modern world these questions are not simple.\nTake color-blindness for example. Say we are interested in how color-blindness impacts ability to complete a jig-saw puzzle. Because color-blindness is genetic some might argue it cannot be manipulated. But advances in technology like gene-therapy might allow us to actually change someone’s genes. Could we then claim the ability to manipulate color-blindness? If yes, we could then measure the causal effect of color-blindness on ability to complete jig-saw puzzles.\nThe slogan of “No causation without manipulation” may at first seem straight-forward, but it is clearly not so simple. Questions about race, sex, gender and genetics are very complex and should be considered with care.\n\n\n4.2.4 Multiple units\nGenerally, a study has many individuals (or, more broadly, “units”) who each have their own potential outcomes. More notation is needed to allow us to differentiate between different units.\nIn other words, there needs to be a distinction between \\(Y_t\\) for Yao, and \\(Y_t\\) for Emma. We use the variable \\(u\\) (\\(u\\) for “unit”) to indicate that the outcome under control and the outcome under treatment can differ for each individual unit (person).\nInstead of \\(Y_t\\), we will use \\(Y_t(u)\\) to represent “Attitude if Treated.” If you want to talk about only Emma, you could say “Emma’s Attitude if Treated” or “\\(Y_t(u = Emma)\\)” or “the \\(Y_t(u)\\) for Emma”, but not just \\(Y_t\\). That notation is too ambiguous when there is more than one subject.\nLet’s look at a Preceptor Table with more subjects using our new notation:\n\n\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n9\n\n+4\n\n    Emma\n\n14\n\n11\n\n+3\n\n    Cassidy\n\n11\n\n6\n\n+5\n\n    Tahmid\n\n9\n\n12\n\n-3\n\n    Diego\n\n3\n\n4\n\n-1\n\n  \n  \n  \n\n\n\n\nFrom this Preceptor Table, there are many possible estimands we might be interested in. Consider some examples, along with their true values:\n\n\n\nA potential outcome for one person, e.g., Yao’s potential outcome under treatment: \\(13\\).\nA causal effect for one person, such as for Emma. This is the difference between the potential outcomes: \\(14 - 11 = +3\\).\nThe most positive causal effect: \\(+5\\), for Cassidy.\nThe most negative causal effect: \\(-3\\), for Tahmid.\nThe median causal effect: \\(+3\\).\nThe median percentage change: \\(+27.2\\%\\). To see this, calculate the percentage change for each person. You’ll get 5 percentages: \\(+44.4\\%\\), \\(+27.2\\%\\), \\(+83.3\\%\\), \\(-25.0\\%\\), and \\(-25.0\\%\\).\n\n\nSimilar concepts can also be applied to the Population Table:\n\n\n\n\n\n\n  \n    \n      Population Table\n    \n    \n  \n  \n    \n      Source\n      Year\n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Population\n\n2010\n\n?\n\n?\n\n?\n\n?\n\n    ...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n    Data\n\n2015\n\nYao\n\n5\n\n3\n\n+2\n\n    Data\n\n2015\n\nCassidy\n\n-1\n\n2\n\n-3\n\n    ...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n    Population\n\n2018\n\n?\n\n?\n\n?\n\n?\n\n    ...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n    Preceptor Table\n\n2022\n\nYao\n\n13\n\n9\n\n+4\n\n    ...\n\n...\n\n...\n\n...\n\n...\n\n...\n\n    Population\n\n2025\n\n?\n\n?\n\n?\n\n?\n\n  \n  \n  \n\n\n\n\nFor example, we get a much better picture of all our data, as it all combines into one nice looking Population Table. We can take a look a past data about Yao, or Cassidy, and their previous outcomes and causal effects. We can also see the rest of the units which fall under our desired population, but we don’t have any data about, hence the question makes.\nConsider these examples:\n\nDifference in potential outcome for one person, eg., the difference between Yao’s \\(Y_t(u)\\) values: \\(-8\\)\nDifference in causal effect for one person, for Yao it would be \\(-2\\): \\(+2- +4\\)\n\nAll of the variables calculated in the Preceptor and Population Tables are examples of estimands we might be interested in. One estimand is important enough that it has its own name: the average treatment effect, often abbreviated as ATE. The average treatment effect is the mean of all the individual causal effects. Here, the mean is \\(+1.6\\).\nWhat does our real-world Preceptor Table look like?\n\n\n\n\n\n\n\n\n  \n    \n      Causal Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n?\n\n?\n\n    Emma\n\n14\n\n?\n\n?\n\n    Cassidy\n\n?\n\n6\n\n?\n\n    Tahmid\n\n?\n\n12\n\n?\n\n    Diego\n\n3\n\n?\n\n?\n\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n      Predictive Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      $$Y_t(u)$$\n    \n  \n  \n    Yao\n\n13\n\n    Emma\n\n14\n\n    Cassidy\n\n6\n\n    Tahmid\n\n12\n\n    Diego\n\n3"
  },
  {
    "objectID": "04-rubin-causal-model.html#simple-models",
    "href": "04-rubin-causal-model.html#simple-models",
    "title": "4  Rubin Causal Model",
    "section": "4.3 Simple models",
    "text": "4.3 Simple models\n\n\nHow can we fill in the question marks? Because of the Fundamental Problem of Causal Inference, we can never know the missing values. Because we can never know the missing values, we must make assumptions. “Assumption” just means that we need a “model,” and all models have parameters.\n\n4.3.1 A single value for tau\nOne model might be that the causal effect is the same for everyone. There is a single parameter, \\(\\tau\\), which we then estimate. (\\(\\tau\\) is a Greek letter, written as “tau” and rhyming with “cow.”) Once we have an estimate, we can fill in the Preceptor Table because, knowing it, we can estimate what the unobserved potential outcome is for each person. We use our assumption about \\(\\tau\\) to estimate the counterfactual outcome for each unit.\nRemember what our Preceptor Table looks like with all of the missing data:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n?\n\n?\n\n    Emma\n\n14\n\n?\n\n?\n\n    Cassidy\n\n?\n\n6\n\n?\n\n    Tahmid\n\n?\n\n12\n\n?\n\n    Diego\n\n3\n\n?\n\n?\n\n  \n  \n  \n\n\n\n\nIf we assume \\(\\tau\\) is the treatment effect for everyone, how do we fill in the table? We are using \\(\\tau\\) as an estimate for the causal effect. By definition: \\(Y_t(u) - Y_c(u) = \\tau\\). Using simple algebra, it is then clear that \\(Y_t(u) = Y_c(u) + \\tau\\) and \\(Y_c(u) = Y_t(u) - \\tau\\). In other words, you could add it to the observed value of every observation in the control group (or subtract it from the observed value of every observation in the treatment group), and thus fill in all the missing values.\nAssuming there is a constant treatment effect, \\(\\tau\\), for everyone, filling in the missing values would look like this:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n$$13 - \\tau$$\n\n$$\\tau$$\n\n    Emma\n\n14\n\n$$14 - \\tau$$\n\n$$\\tau$$\n\n    Cassidy\n\n$$6 + \\tau$$\n\n6\n\n$$\\tau$$\n\n    Tahmid\n\n$$12 + \\tau$$\n\n12\n\n$$\\tau$$\n\n    Diego\n\n3\n\n$$3 - \\tau$$\n\n$$\\tau$$\n\n  \n  \n  \n\n\n\n\nNow we need to find an estimate for \\(\\tau\\) in order to fill in the missing values. One approach is to subtract the average of the observed control values from the average of the observed treated values. \\[((13 + 14 + 3) / 3) - ((6 + 12) /  2)\\] \\[10 - 9 = +1\\]\nOr, in other words, we use this formula:\n\\[\\frac{\\Sigma Y_t(u)}{n_t} + \\frac{\\Sigma Y_c(u)}{n_c} = \\widehat{ATE}\\]\n\\(\\Sigma\\) represents the sum of the treated/control values, and \\(n_t\\)/\\(n_c\\) represents the number of values within the treated and control groups. This formula is for something called \\(\\widehat{ATE}\\), which we will discuss in more depth in a later section.\nContinuing with the example, calculating the ATE or the causal effect, gives us an estimate of \\(+1\\) for \\(\\tau\\). Let’s fill in our missing values by adding \\(\\tau\\) to the observed values under control and by subtracting \\(\\tau\\) from the observed value under treatment like so:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n$$13 - (+1)$$\n\n+1\n\n    Emma\n\n14\n\n$$14 - (+1)$$\n\n+1\n\n    Cassidy\n\n$$6 + (+1)$$\n\n6\n\n+1\n\n    Tahmid\n\n$$12 + (+1)$$\n\n12\n\n+1\n\n    Diego\n\n3\n\n$$3 - (+1)$$\n\n+1\n\n  \n  \n  \n\n\n\n\nWhich gives us:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n12\n\n+1\n\n    Emma\n\n14\n\n13\n\n+1\n\n    Cassidy\n\n7\n\n6\n\n+1\n\n    Tahmid\n\n13\n\n12\n\n+1\n\n    Diego\n\n3\n\n2\n\n+1\n\n  \n  \n  \n\n\n\n\nIf we make the assumption that there is a single value for \\(\\tau\\) and that \\(1\\) is a good estimate of that value, then we can determine the missing potential outcomes. The Preceptor Table no longer has any missing values, so we can use it to easily answer (almost) any conceivable question.\n\n\n4.3.2 Two values for tau\nA second model might assume that the causal effect is different between levels of a category but the same within those levels. For example, perhaps there is a \\(\\tau_F\\) for females and \\(\\tau_M\\) for males where \\(\\tau_F != \\tau_M\\). We are making this assumption to give us a different model with which we can fill in the missing values in our Preceptor Table. We can’t make any progress unless we make some assumptions. That is an inescapable result of the Fundamental Problem of Causal Inference.\nConsider a model in which causal effects differ based on sex. When we are looking at a “category” of units — for instance, gender — we call this a covariate. Possible covariates include, but are not limited to, sex, age, political party and almost everything else which might be associated with attitudes toward immigration.\n\n\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Covariate\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      Gender\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n$$13 - \\tau_M$$\n\nMale\n\n$$\\tau_M$$\n\n    Emma\n\n14\n\n$$14 - \\tau_F$$\n\nFemale\n\n$$\\tau_F$$\n\n    Cassidy\n\n$$6 + \\tau_F$$\n\n6\n\nFemale\n\n$$\\tau_F$$\n\n    Tahmid\n\n$$12 + \\tau_M$$\n\n12\n\nMale\n\n$$\\tau_M$$\n\n    Diego\n\n3\n\n$$3 - \\tau_M$$\n\nMale\n\n$$\\tau_M$$\n\n  \n  \n  \n\n\n\n\nWe would have two different estimates for \\(\\tau\\).\n\\(\\tau_M\\) would be \\[(13+3)/2 - 12 = -4\\] \\(\\tau_F\\) would be \\[(14-6 = +8)\\]\nUsing those values, we would fill out our new table like this:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n$$13 - (-4)$$\n\n-4\n\n    Emma\n\n14\n\n$$14 - (+8)$$\n\n+8\n\n    Cassidy\n\n$$6 + (+8)$$\n\n6\n\n+8\n\n    Tahmid\n\n$$12 + (-4)$$\n\n12\n\n-4\n\n    Diego\n\n3\n\n$$3 - (-4)$$\n\n-4\n\n  \n  \n  \n\n\n\n\nWhich gives us:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n17\n\n-4\n\n    Emma\n\n14\n\n6\n\n+8\n\n    Cassidy\n\n14\n\n6\n\n+8\n\n    Tahmid\n\n8\n\n12\n\n-4\n\n    Diego\n\n3\n\n7\n\n-4\n\n  \n  \n  \n\n\n\n\nWe now have two different estimates for Emma (and for everyone else in the table). When we estimate \\(Y_c(Emma)\\) using an assumption of constant treatment effect (a single value for \\(\\tau\\)), we get \\(Y_c(Emma) = 13\\). When we estimate assuming treatment effect is constant for each sex, we calculate that \\(Y_c(Emma) = 8\\). This difference between our estimates for Emma highlights the difficulties of inference. Models drive inference. Different models will produce different inferences.\n\n\n4.3.3 Heterogenous treatment effects\nIs the assumption of a constant treatment effect, \\(\\tau\\), usually true? No! It is never true. People vary. The effect of a pill on you will always be different from the effect of a pill on your friend, at least if we measure outcomes accurately enough. Treatment effects are always heterogeneous, meaning that they vary across individuals.\nReality looks like this:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n$$13 - \\tau_{yao}$$\n\n$$\\tau_{yao}$$\n\n    Emma\n\n14\n\n$$14 - \\tau_{emma}$$\n\n$$\\tau_{emma}$$\n\n    Cassidy\n\n$$6 + \\tau_{cassidy}$$\n\n6\n\n$$\\tau_{cassidy}$$\n\n    Tahmid\n\n$$12 + \\tau_{tahmid}$$\n\n12\n\n$$\\tau_{tahmid}$$\n\n    Diego\n\n3\n\n$$3 - \\tau_{diego}$$\n\n$$\\tau_{diego}$$\n\n  \n  \n  \n\n\n\n\nCan we solve for \\(\\tau_{yao}\\)? No! That is the Fundamental Problem of Causal Inference. So how can we make any progress from here if we are unwilling to assume there is at least some structure to the causal effect across different individuals? Instead of worrying about the causal effect for specific individuals, we, instead, focus on the causal effect for the entire population.\n\n\n4.3.4 Average treatment effect\nThe average treatment effect (ATE) is the average difference in potential outcomes between the treated group and the control groups. Because averaging is a linear operator, the average difference is the same as the difference between the averages. The distinction between this estimand and estimands like \\(\\tau\\), \\(\\tau_M\\) and \\(\\tau_F\\), is that, in this case, we do not care about using the average treatment effect to fill in missing values in each row. The average treatment effect is useful because we don’t have to assume anything about each individuals’ \\(\\tau\\), like \\(\\tau_{yao}\\), but can still understand something about the average causal effect across the whole population.\nAs we did before, the simplest way to estimate the ATE is to take the mean of the treated group (\\(10\\)) and the mean of the control group (\\(9\\)) and then take the difference in those means (\\(1\\)). If we use this method to an estimate of the ATE, we’ll call it \\(\\widehat{ATE}\\), pronounced “ATE-hat.”\nIf we already did this exact same calculation above, why are we talking about it again? Remember that we are unwilling to assume treatment effect is constant in our study population, and we cannot solve for \\(\\tau\\) if \\(\\tau\\) is different for different individuals. This is where \\(\\widehat{ATE}\\) is helpful.\nSome estimands may not require filling in all the question marks in the Preceptor Table. We can get a good estimate of the average treatment effect without filling in every question mark — the average treatment effect is just a single number. Rarely in a study do we care about what happens to individuals. In our case, we don’t care about what specifically would happen to Cassidy’s attitude if treated. Instead, we care generally about how our experiment impacts people’s attitudes towards immigrants. This is why an average estimate, like \\(\\widehat{ATE}\\) can be helpful.\nAs we noted before, this is a popular estimand. Why?\n\nThere’s an obvious estimator for this estimand: the mean difference of the observed outcomes between the treated group and the control group: \\(Y_t(u) - Y_c(u)\\).\nIf treatment is randomly assigned, the estimator is unbiased: you can be fairly confident in the estimate if you have a large enough treatment and control groups.\nAs we did earlier, if you are willing to assume that the causal effect is the same for everyone (a big assumption!), you can use your estimate of the ATE, \\(\\widehat{ATE}\\), to fill in the missing individual values in your Preceptor Table.\n\nJust because the ATE is often a useful estimand doesn’t mean that it always is.\nConsider point #3. For example, let’s say the treatment effect does vary dependent on sex. For males there is a small negative effect (-4), but for females there is a larger positive effect (+8). However, the average treatment effect for the whole sample, even if you estimate it correctly, will be a single positive number (+1) – since the positive effect for females is larger than the negative effect for males.\nEstimating the average treatment effect, by calculating \\(\\widehat{ATE}\\), is easy. But is our \\(\\widehat{ATE}\\) a good estimate of the actual ATE? After all, if we knew all the missing values in the Preceptor Table, we could calculate the ATE perfectly. But those missing values may be wildly different from the observed values. Consider this Preceptor Table:\n\n\n\n\n\n\n  \n    \n      Preceptor Table\n    \n    \n  \n  \n    \n      ID\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n10\n\n+3\n\n    Emma\n\n14\n\n11\n\n+3\n\n    Cassidy\n\n9\n\n6\n\n+3\n\n    Tahmid\n\n15\n\n12\n\n+3\n\n    Diego\n\n3\n\n0\n\n+3\n\n  \n  \n  \n\n\n\n\nIn this example, there is indeed a constant treatment effect for everyone: \\(+3\\). Note that the observed values are all the same, but the unobserved values were such that our estimated ATE, \\(+1\\), is pretty far from the actual ATE, \\(+3\\). If we think we have a reasonable estimate of ATE, using that value as a constant for \\(\\tau\\) might be our best guess."
  },
  {
    "objectID": "04-rubin-causal-model.html#assumptions",
    "href": "04-rubin-causal-model.html#assumptions",
    "title": "4  Rubin Causal Model",
    "section": "4.4 Assumptions",
    "text": "4.4 Assumptions\nIn this section, we will explore four topics: validity, stability, representativeness and unconfoundedness.\nOur earlier Population Table familiarized us with the three sources of data for which we are making inferences: the Preceptor Table, our data, the greater population from which both are drawn. Consider a new Population Table.\n\n\n\n\n\n\n\n  \n    \n      Population Table\n    \n    \n  \n  \n    \n      Source\n      \n        Outcomes\n      \n      Year\n      \n        Covariates\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      Sex\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Population\n?\n?\n2012\n?\n?\n    Population\n?\n?\n2012\n?\n?\n    ...\n...\n...\n...\n...\n...\n    Data\n13\n?\n2014\nMale\n?\n    Data\n?\n9\n2014\nFemale\n?\n    ...\n...\n...\n...\n...\n...\n    Preceptor Table\n?\n?\n2022\nFemale\n?\n    Preceptor Table\n?\n?\n2022\nFemale\n?\n    ...\n...\n...\n...\n...\n...\n    Population\n?\n?\n2023\n?\n?\n    Population\n?\n?\n2023\n?\n?\n  \n  \n  \n\n\n\n\nThe rows from our data have covariates and one potential outcome. (By definition, no real data can include more than one potential outcome.) The rows from the Preceptor Table include covariates, but not outcomes. The rows from our greater population include no data, as we know nothing about these units.\n\n\n\n4.4.1 Validity\n\nTo understand validity in regards to the Population Table, we must first recognize an inherent flaw in any experiment design: no two units receive exactly the same treatment.\nIf this doesn’t ring true, consider our Spanish-speakers train experiment. The units on the Spanish-speaking platform received the same treatment, right? No, actually!\nConsider different volume levels, measured in decibels (dB), at which Spanish is being spoken.\n\n\n\n\n\n\n  \n  \n    \n      ID\n      \n        Outcomes\n      \n    \n    \n      $$Y_{\\text{58 dB}}(u)$$\n      $$Y_{\\text{59 dB}}(u)$$\n      $$Y_{\\text{60 dB}}(u)$$\n      $$Y_{\\text{61 dB}}(u)$$\n      $$Y_c(u)$$\n    \n  \n  \n    Yao\n\n13\n\n?\n\n?\n\n?\n\n?\n\n    Emma\n\n?\n\n11\n\n?\n\n?\n\n?\n\n    Cassidy\n\n?\n\n?\n\n?\n\n?\n\n10\n\n    Tahmid\n\n?\n\n?\n\n?\n\n?\n\n12\n\n    Diego\n\n?\n\n?\n\n6\n\n?\n\n?\n\n  \n  \n  \n\n\n\n\nCertain units heard the Spanish-speakers at higher volumes than other units. And it is entirely possible that the volume of the speech affects the outcome. There is also the issue of the time spent on the platform. Maybe Yao tends to run late, and only hears the Spanish-speakers for thirty seconds. Emma, on the other hand, arrives early. She hears the Spanish-speakers for fifteen minutes before the train arrives! Thus, despite the fact that Emma and Yao are in the same treatment — that is, hearing Spanish on the platform — they had very different versions of that treatment.\nIndeed, there are an infinite number of possible treatments. Indeed, it is a virtual certainty that every treated unit received a different treatment. However, validity, if reasonable assumption in this specific example, allows us to pretend/assume that Yao, Emma and Diego all received the same treatment. We place all their treated outcomes in the same column. Only if this is true (or true’ish) can we estimate an average treatment effect.\nMore commonly, we simply assume that all treated units received the same treatment. Validity allows us to ignore variation in treatment. In fact, concerns about validity apply to all the variables (covariates and outcomes), not just the treatments. If the columns in the data are not the same thing as the columns in the Preceptor Table, you have a problem. Validity is the assumption which allows us to create the Population Table.\n\n\n4.4.2 Stability\nStability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor Table, and the larger population from which both are drawn.\n\nIn our height example, it is much easier to assume stability over a greater period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. Can we say the same for this example, where we are looking at attitudes on immigration?\nWith something like political ideology, it is much harder to assert that the relationships among the data collected in 2010 would be similar to the relationships among the data collected in 2025. Our data, for instance, was collected in 2014. We want to make predictions for 2022. And, frankly, it may be difficult to argue that our results would be stable if we re-conducted the experiment.\nWhen we are confronted with this uncertainty, we can consider making our timeframe smaller. However, we would still need to assume stability from 2014 (time of data collection) to today. Stability allows us to ignore the passage of time.\n\n\n4.4.3 Representativeness\n\n\n\nRepresentativeness, or the lack thereof, is the relationship between the rows in the Population Table with our data and the other rows. Ideally, we would like our data to be a random sample from the population. Sadly, this is almost never the case.\nDoes the train experiment allow us to calculate a causal effect for people who commute by cars? Can we calculate the causal effect for people in New York City? Before we generalize to broader populations we have to consider if our experimental estimates are applicable beyond our experiment. Maybe we think that commuters in Boston and New York are similar enough to generalize our findings. We could also conclude that people who commute by car are fundamentally different than people who commute by train. If that was true, then we could not say our estimate is true for all commuters because our sample does not accurately represent the broader group we want to generalize to.\n\n\n\n\n\n\n\n4.4.4 Unconfoundedness\nA fourth assumption we use when working with causal models — but not with predictive models — is “unconfoundedness.” If whether or not a unit received treatment or control is random, we write that treatment assignment is not “confounded.” If, however, treatment assignment depends on the value of a potential outcome, then treatment assignment is confounded. Our lives are easiest if we can (reasonably!) assume unconfoundedness. In that case, we can estimate the average treatment effect by subtracting the average outcome of control units from the average outcome of treated units, as we do above.\nConsider the “Perfect Doctor” as an example of the problems caused by confounded treatment assignments. Imagine we have this omniscient doctor who knows how any patient will respond to a certain drug. She has perfect knowledge of the entire Preceptor Table. Using this information, she always assign each patient the treatment with the best outcome, whether that is treatment or control. Consider:\n\n\n\n\n\n\n  \n    \n      Holy Grail of Information\n    \n    \n  \n  \n    \n      ID\n      \n        Blood Pressure Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n130\n\n105\n\n+25\n\n    Emma\n\n120\n\n140\n\n-20\n\n    Cassidy\n\n100\n\n170\n\n-70\n\n    Tahmid\n\n115\n\n125\n\n-10\n\n    Diego\n\n135\n\n100\n\n35\n\n    MEAN\n\n120\n\n128\n\n-8\n\n  \n  \n  \n\n\n\n\nThe Perfect Doctor would assign the treatment to Emma, Cassidy and Tahmid. She would assign control to Yao and Diego. And that is good! This is what the doctor should do. This is the best treatment assignment for the patients. But it is not a good assignment mechanism for estimating the average causal effect because treatment assignment is confounded by the values of the potential outcomes.\nWe, the non-Perfect Doctors, do not have access to the entire Precetor Table. We can only see this:\n\n\n\n\n\n\n  \n    \n      Skewed Holy Grail of Information\n    \n    \n  \n  \n    \n      ID\n      \n        Blood Pressure Outcomes\n      \n      \n        Causal Effect\n      \n    \n    \n      $$Y_t(u)$$\n      $$Y_c(u)$$\n      $$Y_t(u) - Y_c(u)$$\n    \n  \n  \n    Yao\n\n?\n\n105\n\n?\n\n    Emma\n\n120\n\n?\n\n?\n\n    Cassidy\n\n100\n\n?\n\n?\n\n    Tahmid\n\n115\n\n?\n\n?\n\n    Diego\n\n?\n\n100\n\n?\n\n    MEAN\n\n111.66\n\n102.5\n\n9.16\n\n  \n  \n  \n\n\n\n\nThe true causal effect of the treatment, as we can see in the first table, is -8. In other words, the treatment lowers blood pressure on average. But, using just the data we have access to if the Perfect Doctor performs the treatment assignment, we would estimate — if we mistakenly assume random assignment — that the causal effect is positive, that treatment increases blood pressure.\nThe best way to ensure unconfoundedness is to randomize the treatment across units. Don’t let the doctor decide who gets the treatment and who gets the control. Randomize assignment. As long as you use randomization as your assignment mechanism, you’re good. There is the possibility that you can’t use pure randomization due to ethical or practical reasons, so we are forced to use non-random assignment mechanisms. Many statistical methods have been developed for causal inference when there is a non-random assignment mechanism. Those methods, however, are beyond the scope of this book."
  },
  {
    "objectID": "04-rubin-causal-model.html#summary",
    "href": "04-rubin-causal-model.html#summary",
    "title": "4  Rubin Causal Model",
    "section": "4.5 Summary",
    "text": "4.5 Summary\n\n\n\n\n\n\nGlossary\n\n\n\n\nA Preceptor Table is a table that includes all rows and columns such that, if no data is missing, it is easy to calculate our quantity of interest. However there is always data missing.\nA Population Table has three sources of data: the Preceptor Table, the dataset, and the greater population from which both are drawn.\nA Potential Outcome is the outcome for an individual depending on if they receive the treatment or not.\nCausal Effect is the difference between potential outcomes.\nThe Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit. We must make assumptions in order to estimate causal effects.\n\n\n\n\nThe fundamental components of every problem in causal inference are units, treatments and outcomes. The units are the rows in the table. The treatments are (some of) the columns. The outcomes are the values under the treatment columns. (There are also covariate columns and the values within them.) Whenever you confront a problem in causal inference, start by identifying the units, treatments and outcomes.\nA causal effect is the difference between one potential outcome and another. How different would your life be if you missed the train?\n\nA Preceptor Table includes all rows and columns such that, if no data is missing, it is easy to calculate our quantity of interest. Unfortunately, data is always missing in causal models because, at most, we can only observe one potential outcome for each unit. The causal effect of a treatment on a single unit at a point in time is the difference between the value of the outcome variable with the treatment and without the treatment. The Fundamental Problem of Causal Inference is that it is impossible to observe the causal effect on a single unit. We must make assumptions — i.e, we must make models — in order to estimate causal effects.\nThe Population Table has three sources of data: the Preceptor Table, the dataset, and the greater population from which both are drawn.\nThe assumption of validity, if met, allows us to create the Population Table because it ensures that the columns of data from the different sources can be put into a single table. If the relationships among the data are the same (or at least same’ish), over time, then we can assume stability. A model estimated on our data will also apply to our Preceptor Table. Representativeness examines the rows we have relative to the rows in the Population Table which we might have had. Unconfoundedness, which only matters in causal settings, means that either the treatment was randomly assigned or that we can act as if it was.\nRandom assignment of treatments to units is the best experimental set up for estimating causal effects. Other assignment mechanisms are subject to confounding. If the treatment assigned is correlated with the potential outcomes, it is very hard to estimate the true treatment effect. (As always, we use the terms “causal effects” and “treatment effects” interchangeably.) With random assignment, we can, mostly safely, estimate the average treatment effect (ATE) by looking at the difference between the average outcomes of the treated and control units.\nBe wary of claims made in situations without random assignment: Here be dragons!\n\n\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on Exclusionary Attitudes.” Proceedings of the National Academy of Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111."
  },
  {
    "objectID": "05-probability.html#sec-distributions",
    "href": "05-probability.html#sec-distributions",
    "title": "5  Probability",
    "section": "\n5.1 Distributions",
    "text": "5.1 Distributions\nA variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a “distribution.” This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a “frequency distribution” or an “empirical distribution” is a list of values, so this usage is not unreasonable.\n\n5.1.1 Scaling distributions\nConsider the vector which is the result of rolling one dice 10 times.\n\nten_rolls <- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)\n\nThere are other ways of storing the data in this vector. Instead of reporting every observation, we could record the number of times each value appears or the percentage of the total which this number accounts for.\n\n\n\n\n\n\n\n\nDistribution of Ten Rolls of a Fair Dice\n    \n\nCounts and percentages reflect the same information\n    \n\n\nOutcome\n      Count\n      Percentage\n    \n\n\n1\n2\n0.2\n\n\n2\n2\n0.2\n\n\n4\n1\n0.1\n\n\n5\n4\n0.4\n\n\n6\n1\n0.1\n\n\n\n\n\n\nIn this case, with only 10 values, it is actually less efficient to store the data like this. But what happens when we have 1,000 rolls?\n\n\n\n\n\n\n\n\nDistribution of One Thousand Rolls of a Fair Dice\n    \n\nCounts and percentages reflect the same information\n    \n\n\nOutcome\n      Count\n      Percentage\n    \n\n\n1\n190\n0.190\n\n\n2\n138\n0.138\n\n\n3\n160\n0.160\n\n\n4\n173\n0.173\n\n\n5\n169\n0.169\n\n\n6\n170\n0.170\n\n\n\n\n\n\nInstead of keeping around a vector of length 1,000, we can just keep 12 values — the 6 possible outcomes and their frequency — without losing any information.\nTwo distributions can be identical even if they are of very different lengths. Let’s compare our original distribution of 10 rolls of the dice with another distribution which just features 100 copies of those 10 rolls.\n\nmore_rolls <- rep(ten_rolls, 100)\n\n\n\n\n\n\nThe two graphs have the exact same shape because, even though the vectors are of different lengths, the relative proportions of the outcomes are identical. In some sense, both vectors are from the same distribution. Relative proportions, not the total counts, are what matter.\n\n5.1.2 Normalizing distributions\nIf two distributions have the same shape, then they only differ by the labels on the y-axis. There are various ways of “normalizing” distributions so as to place them all the same scale. The most common scale is one in which the area under the distribution adds to 1, e.g., 100%. For example, we can transform the above plots:\n\n\n\n\n\nWe sometimes refer to a distribution as “unnormalized” if the area under the curve does not add up to 1.\n\n5.1.3 Simulating distributions\nThere are two distinct concepts: a distribution and a set values drawn from that distribution. But, in typical usage, we employ “distribution” for both. When given a distribution (meaning a vector of numbers), we often use geom_histogram() or geom_density() to display it. But, sometimes, we don’t want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its center and its variation around that center.\n\nWe use summarize() to calculate statistics for a variable, a column, a vector of values, or a distribution. Note the language sloppiness. For the purposes of this book, “variable,” “column,” “vector,” and “distribution” all mean the same thing. Other popular statistical functions include: mean(), median(), min(), max(), n() and sum(). Functions which may be new to you include three measures of the “spread” of a distribution: sd() (the standard deviation), mad() (the scaled median absolute deviation) and quantile(), which is used to calculate an interval which includes a specified proportion of the values.\n\nThink of the distribution of a variable as an urn from which we can pull out, at random, values for that variable. Drawing a thousand or so values from that urn, and then looking at a histogram, can show where the values are centered and how they vary. Because people are sloppy, they will use the word distribution to refer to at least three related entities:\n\nthe (imaginary!) urn from which we are drawing values.\nall the values in the urn\nall the values which we have drawn from the urn, whether that be 10 or 1,000\n\nSloppiness in the usage of the word distribution is universal. However, keep three distinct ideas separate:\n\nThe unknown true distribution which, in reality, generates the data which we see. Outside of stylized examples in which we assume that a distribution follows a simple mathematical formula, we will never have access to the unknown true distribution. We can only estimate it. This unknown true distribution is often referred to as the data generating mechanism, or DGM. It is a function or black box or urn which produces data. We can see the data. We can’t see the urn.\nThe estimated distribution which, we think, generates the data which we see. Again, we can never know the unknown true distribution. But, by making some assumptions and using the data we have, we can estimate a distribution. Our estimate may be very close to the true distribution. Or it may be far away. The main task of data science to to create and use these estimated distributions. Almost always, these distributions are instantiated in computer code. Just as there is a true data generating mechanism associated with the (unknown) true distribution, there is an estimated data generating mechanism associated with the estimated ditribution.\nA vector of numbers drawn from the estimated distribution. Both true and estimated distributions can be complex animals, difficult to describe accurately and in detail. But a vector of numbers drawn from a distribution is easy to understand and use. So, in general, we work with vectors of numbers. When someone — either a colleague or a piece of R code — creates a distribution which we want to use to answer a question, we don’t really want the distribution itself. Rather, we want a vectors of “draws” from that distribution. Vectors are easy to work with! Complex computer code is not.\n\nAgain, people (including us!) will often be sloppy and use the same word, “distribution,” without making it clear whether they are talking about the true distribution, the estimated distribution, or a vector of draws from the estimated distribution. The same sloppiness applies to the use of the term data generating mechanism. Try not to be sloppy.\nMuch of the rest of the Primer involves learning how to work with distributions, which generally means working with the draws from those distributions. Fortunately, the usual rules of arithmetic apply. You can add/subtract/multiply/divide distributions by working with draws from those distributions, just as you can add/subtract/multiply/divide regular numbers."
  },
  {
    "objectID": "05-probability.html#probability-distributions",
    "href": "05-probability.html#probability-distributions",
    "title": "5  Probability",
    "section": "\n5.2 Probability distributions",
    "text": "5.2 Probability distributions\n\n\n\n\nBruno de Finetti, an Italian statistician who wrote a famous treatise on the theory of probability that began with the statement “PROBABILITY DOES NOT EXIST.”\n\n\n\n\nFor the purposes of this Primer, a probability distribution is a mathematical object which maps a set of outcomes to probabilities, where each distinct outcome has a chance of occurring between 0 and 1 inclusive. The probabilities must sum to 1. The set of possible outcomes, i.e., the sample space — heads or tails for the coin, 1 through 6 for a single dice, 2 through 12 for the sum of a pair of dice — can be either discrete or continuous. Remember, discrete data can only take on certain values. Continuous data, like height and weight, can take any value within a range. The set of outcomes is the domain of the probability distribution. The range is the associated probabilities.\nAssume that a probability distribution is created by a probability function, a set function which maps outcomes to probabilities. The concept of a “probability function” is often split into two categories: probability mass functions (for discrete random variables) and probability density functions (for continuous random variables). As usual, we will be a bit sloppy, using the term probability distribution for both the mapping itself and for the function which creates the mapping.\nWe discuss three types of probability distributions: empirical, mathematical, and posterior.\nThe key difference between a distribution, as we have explored them in Section 5.1, and a probability distribution is the requirement that the sum of the probabilities of the individual outcomes must be exactly 1. There is no such requirement for a distribution in general. But any distribution can be turned into a probability distribution by “normalizing” it. In this context, we will often refer to a distribution which is not (yet) a probability distribution as an “unnormalized” distribution.\n\nPay attention to notation. Recall that when we are talking about a specific probability (represented by a single value), we will use \\(\\rho\\) (the Greek letter “rho”) with a subscript which specifies the exact outcome of which it is the probability. For instance, \\(\\rho_h = 0.5\\) denotes the probability of getting heads on a coin toss when the coin is fair. \\(\\rho_t\\) — spoken as “PT” or “P sub T” or “P tails” — denotes the probability of getting tails on a coin toss. However, when we are referring to the entire probability distribution over a set of outcomes, we will use \\(P()\\). For example, the probability distribution of a coin toss is \\(P(\\text{coin})\\). That is, \\(P(\\text{coin})\\) is composed of the two specific probabilities (50% and 50%) mapped from the two values in the domain (Heads and Tails). Similarly, \\(P(\\text{sum of two dice})\\) is the probability distribution over the set of 11 outcomes (2 through 12) which are possible when you take the sum of two dice. \\(P(\\text{sum of two dice})\\) is made up of 11 numbers — \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_{12}\\) — each representing the unknown probability that the sum will equal their value. That is, \\(\\rho_2\\) is the probability of rolling a 2.\n\n5.2.1 Flipping a coin\nData science problems start with a question. Example:\nWhat are the chances of getting three heads in a row when flipping a fair coin?\nQuestions are answered with the help of probability distributions.\nAn empirical distribution is based on data. Think of this as the probability distribution created by collecting data in the real world or by running a simulation on your computer. In theory, if we increase the number of coins we flip (either in reality or via simulation), the empirical distribution will look more and more similar to the mathematical distribution. The mathematical distribution is the Platonic form. The empirical distribution will often look like the mathematical probability distribution, but it will rarely be exactly the same.\nIn this simulation, there are 44 heads and 56 tails. The outcome will vary every time we run the simulation, but the proportion of heads to tails should not be too different if the coin is fair.\n\n\n\n\n\nA mathematical distribution is based on a mathematical formula. Assuming that the coin is perfectly fair, we should, on average, get heads as often as we get tails.\n\n\n\n\n\nThe distribution of a single observation is described by this formula.\n\\[ P(Y = y) = \\begin{cases} 1/2 &\\text{for }y= \\text{Heads}\\\\ 1/2 &\\text{for }y= \\text{Tails} \\end{cases}\\] We sometimes do not know that the probability of heads and the probability of tails both equal 50%. In that case, we might write:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ \\rho_T &\\text{for }y= \\text{Tails} \\end{cases}\\]\nYet, we know that, by definition, \\(\\rho_H + \\rho_T = 1\\), so we can rewrite the above as:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ 1- \\rho_H &\\text{for }y= \\text{Tails} \\end{cases}\\]\nCoin flipping (and related scenarios with only two possible outcomes) are such common problems, that the notation is often simplified further, with \\(\\rho\\) understood, by convention, to be the probability of heads. In that case, we can write the mathematical distribution is two canonical forms:\n\\[P(Y) = Bernoulli(\\rho)\\] and\n\\[y_i \\sim Bernoulli(\\rho)\\] All five of these versions mean the same thing! The first four describe the mathematical probability distribution for a fair coin. The capital \\(Y\\) within the \\(P()\\) indicates a random variable. The fifth highlights one “draw” from that random variable, hence the lower case \\(y\\) and the subscript \\(i\\).\nMost probability distributions do not have special names, which is why we will use the generic symbol \\(P\\) to refer to them. But some common probability distributions do have names, like “Bernoulli” in this case.\nIf the mathematical assumptions are correct, then, as your sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nA posterior distribution is based on beliefs and expectations. It displays your beliefs about things you can’t see right now. You may have posterior distributions for outcomes in the past, present, or future.\nIn the case of the coin toss, the posterior distribution changes depending on your beliefs. For instance, let’s say your friend brought a coin to school and asked to bet you. If the result is heads, you have to pay them $5. In that case, your posterior probability distribution might look like this:\n\n\n\n\n\nThe fact that your friend wants to bet on heads suggests to you that the coin is not fair. Does it prove that the coin is unfair? No! Much depends on the sort of person you think your friend is. Your posterior probability distribution is your opinion, based on your experiences and beliefs. My posterior probability distribution will often be (very) different from yours.\nThe full terminology is mathematical (or empirical or posterior) probability distribution. But we will often shorten this to just mathematical (or empirical or posterior) distribution. The word “probability” is understood, even if it is not present.\n\nRecall the question with which we started this section: What are the chances of getting three heads in a row when flipping a fair coin? To answer this question, we need to use a probability distribution as our data generating mechanism. Fortunately, the rbinom() function allows us to generate the results for coin flips. For example:\n\nrbinom(n = 10, size = 1, prob = 0.5)\n\n [1] 1 1 0 1 1 0 0 0 0 0\n\n\ngenerates the results of 10 coin flips, where a result of heads is presented as 1 and tails as 0. With this tool, we can generate 1,000 draws from our experiment:\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5))\n\n# A tibble: 1,000 × 3\n   toss_1 toss_2 toss_3\n    <int>  <int>  <int>\n 1      0      1      1\n 2      0      1      1\n 3      0      1      0\n 4      0      0      1\n 5      1      1      0\n 6      1      0      1\n 7      1      0      0\n 8      1      0      1\n 9      0      0      1\n10      0      1      1\n# … with 990 more rows\n\n\nBecause the flips are independent, we can consider each row to be a draw from the experiment. Then, we simply count up the proportion of experiments in which resulted in three heads.\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5)) |> \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |> \n  summarize(chance = mean(three_heads))\n\n# A tibble: 1 × 1\n  chance\n   <dbl>\n1  0.104\n\n\nThis is close to the “correct” answer of \\(1/8\\)th. If we increase the number of draws, we will get closer to the “truth.” The reason for the quotation marks around “correct” and “truth” is that we are uncertain. We don’t know the true probability distribution for this coin. If this coin is a trick coin — like the one we expect our friend to have brought to school — then the odds of three heads in a row would be much higher:\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.95),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.95),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.95)) |> \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |> \n  summarize(chance = mean(three_heads))\n\n# A tibble: 1 × 1\n  chance\n   <dbl>\n1   0.87\n\n\nThis is our first example of using a data generating mechanism — meaning rbinom() — to answer a question. We will see many more in the chapters to come.\n\n5.2.2 Rolling two dice\nData science begins with a question:\nWhat is the probability of rolling a 7 or an 11 with a pair of dice?\nWe get an empirical distribution by rolling two dice a hundred times, either by hand or with a computer simulation. The result is not identical to the mathematical distribution because of the inherent randomness of the real world and/or of simulation.\n\n\n\n\n\nWe might consider labeling the y-axis in plots of empirical distributions as “Proportion” rather than “Probability” since it is an actual proportion, calculated from real (or simulated) data. We will keep it as “Probability” since we want to emphasize the parallels between mathematical, empirical and posterior probability distributions.\nOur mathematical distribution tells us that, with a fair dice, the probability of getting 1, 2, 3, 4, 5, and 6 are equal: there is a 1/6 chance of each. When we roll two dice at the same time and sum the numbers, the values closest to the middle are more common than values at the edge because there are more combinations of numbers that add up to the middle values.\n\n\n\n\n\n\\[ P(Y = y) = \\begin{cases} \\dfrac{y-1}{36} &\\text{for }y=1,2,3,4,5,6 \\\\ \\dfrac{13-y}{36} &\\text{for }y=7,8,9,10,11,12 \\\\ 0 &\\text{otherwise} \\end{cases} \\]\nThe posterior distribution for rolling two dice a hundred times depends on your beliefs. If you take the dice from your Monopoly set, you have reason to believe that the assumptions underlying the mathematical distribution are true. However, if you walk into a crooked casino and a host asks you to play craps, you might be suspicious, just as in the coin flip example above. The word “suspicious” means that you suspect that the data generating mechanism for these dice is not like that for honest dice. For example, in craps, a “come-out” roll of 7 and 11 is a “natural,” resulting in a win for the “shooter” and a loss for the casino. You might expect those numbers to occur less often than they would with fair dice. Meanwhile, a come-out roll of 2, 3 or 12 is a loss for the shooter. You might also expect values like 2, 3 and 12 to occur more frequently. Your posterior distribution might look like this:\n\n\n\n\n\nSomeone less suspicious of the casino would have a posterior distribution which looks more like the mathematical distribution.\nWe began this section with a question about the probability (or odds) of rolling a 7 or 11 — i.e., a “natural” — with a pair of dice. The answer to the question depends on whether or not we think the dice are fair. In other words, we need to know which distribution to use to answer the question.\nAssume that the dice are fair. In that case, we can create a data generating mechanism by hand. (Alas, there is not a built-in R function for dice like there is for coin flips with rbinom().)\n\nset.seed(7)\n\n# Creating a variable like rolls makes our code easier to read and modify. Of\n# course, we could just hard code the 4 into the size argument for each of the\n# two calls to sample, but that is much less convenient.\n\nrolls <- 4\n\n# The details of the code matter. If we don't have replace = TRUE, sample will\n# only use each of the 6 possible values once. That might be OK if we are just\n# rolling the dice 4 times, but it won't work for thousands of rolls.\n\ntibble(dice_1 = sample(x = 1:6, size = rolls, replace = TRUE),\n       dice_2 = sample(x = 1:6, size = rolls, replace = TRUE)) |> \n  mutate(result = dice_1 + dice_2) |> \n  mutate(natural = ifelse(result %in% c(7, 11), TRUE, FALSE))\n\n# A tibble: 4 × 4\n  dice_1 dice_2 result natural\n   <int>  <int>  <int> <lgl>  \n1      2      2      4 FALSE  \n2      3      6      9 FALSE  \n3      4      3      7 TRUE   \n4      2      6      8 FALSE  \n\n\nThis code is another data generating mechanism or dgm. It allows us to simulate the distribution of the results from rolling a pair of fair dice. To answer our question, we simply increase the number of rolls and calculate the proportion of rolls which result in a 7 or 11.\n\nrolls <- 100000\n\n# We probably don't need 100,000 rolls, but this code is so fast that it does\n# not matter. Generally 1,000 (or even 100) draws from the data generating\n# mechanism is enough for most practical purposes.\n\ntibble(dice_1 = sample(x = 1:6, size = rolls, replace = TRUE),\n       dice_2 = sample(x = 1:6, size = rolls, replace = TRUE)) |> \n  mutate(result = dice_1 + dice_2) |> \n  summarize(natural_perc = mean(result %in% c(7, 11)))\n\n# A tibble: 1 × 1\n  natural_perc\n         <dbl>\n1        0.221\n\n\nThe probability of rolling either a 7 or an 11 with a pair of fair dice is about 22%.\n\n5.2.3 Presidential elections\nData science begins with a question:\nWhat is the probability that the Democratic candidate will win the Presidential election?\nConsider the probability distribution for a political event, like a presidential election. We want to know the probability that Democratic candidate wins X electoral votes, where X comes from the range of possible outcomes: 0 to 538. (The total number of electoral votes in US elections since 1964 is 538.)\n\nThe empirical distribution in this case would involve counting the number of electoral votes that the Democratic candidate won in each of the Presidential elections in the last 50 years or so. For the empirical distribution, we create a tibble with electoral vote results from past elections. Looking at elections since 1964, we can observe that the number of electoral votes that the Democratic candidate received in each election is different.\n\n\n\n\n\nGiven that we only have 15 observations, it is difficult to draw conclusions or make predictions based off of this empirical distribution. But “difficult” does not mean “impossible.” For example, if someone, more than a year before the election, offered to bet us 50/50 that the Democratic candidate was going to win more than 475 electoral votes, we would take the bet. After all, this outcome has only happened once in the last 15 elections, so a 50/50 bet seems like a great deal.\nWe can build a mathematical distribution for X which assumes that the chances of the Democratic candidate winning any given state’s electoral votes is 0.5 and that the results from each state are independent.\n\n\n\n\n\nIf our assumptions about this mathematical distribution are correct — they are not! — then, as the sample size increase, the empirical distribution should look more and more similar to our mathematical distribution.\nHowever, the data from past elections is more than enough to demonstrate that the assumptions of our mathematical probability distribution do not work for electoral votes. The model assumes that the Democrats have a 50% chance of receiving each of the 538 votes. Just looking at the mathematical probability distribution, we can observe that receiving 13 or 17 or 486 votes out of 538 would be extreme and almost impossible if the mathematical model were accurate. However, our empirical distribution shows that such extreme outcomes are quite common. Presidential elections have resulted in much bigger victories or defeats than this mathematical distribution seems to allow for, thereby demonstrating that our assumptions are false.\nThe posterior distribution of electoral votes is a popular topic, and an area of strong disagreement, among data scientists. Consider this posterior from FiveThirtyEight.\n\n\n\n\n\nBelow is a posterior probability distribution from the FiveThirtyEight website for August 13, 2020. This was created using the same data as the above distribution, but is displayed differently. For each electoral result, the height of the bar represents the probability that a given event will occur. However, there are no labels on the y-axis telling us what the specific probability of each outcome is. And that is OK! The specific values are not that useful. If we removed the labels on our own y-axes, would it matter? Probably not. Anytime there are many possible outcomes — 539, in this case — we stop looking at specific outcomes and, instead, look at where most of the “mass” of the distribution lies.\n\n\n\n\n\nBelow is the posterior probability distribution from The Economist, also from August 13, 2020. This looks confusing at first because they chose to combine the axes for Republican and Democratic electoral votes. The Economist was less optimistic, relative to FiveThirtyEight, about Trump’s chances in the election.\n\n\n\n\n\nThese two models, built by smart people using similar data sources, have reached fairly different conclusions. Data science is difficult! There is not one “right” answer. Real life is not a problem set.\n\n\n\n\nWatch the makers of these two models throw shade at each other on Twitter! Eliot Morris is one of the primary authors of the Economist model. Nate Silver is in charge of 538. They don’t seem to be too impressed with each other’s work! More smack talk here and here.\n\n\n\n\nThere are many questions you could explore with posterior distributions. They can relate to the past, present, or future.\n\nPast: How many electoral votes would Hilary Clinton have won if she had picked a different VP?\nPresent: What are the total campaign donations from Harvard faculty?\nFuture: How many electoral votes will the Democratic candidate for president win in 2024?\n\n\n5.2.4 Height\n\nQuestion: What is the height of the next adult male we will meet?\nThe three examples above are all discrete probability distributions, meaning that the outcome variable can only take on a limited set of values. A coin flip has two outcomes. The sum of a pair of dice has 11 outcomes. The total electoral votes for the Democratic candidate has 539 possible outcomes. In the limit, we can also create continuous probability distributions which have an infinite number of possible outcomes. For example, the average height for an American male could be any real number between 0 inches and 100 inches. (Of course, an average value anywhere near 0 or 100 is absurd. The point is that the average could be 68.564, 68.5643, 68.56432 68.564327, or any real number.)\nAll the characteristics for discrete probability distributions which we reviewed above apply just as much to continuous probability distributions. For example, we can create mathematical, empirical and posterior probability distributions for continuous outcomes just as we did for discrete outcomes.\nThe empirical distribution involves using data from the National Health and Nutrition Examination Survey (NHANES).\n\n\n\n\n\nMathematical distribution is completely based on mathematical formula and assumptions, as in the coin flip example. In the coin-flip example, we assumed that the coin was perfectly fair, meaning that the probability of landing on heads or tails was equal. In this case, we make three assumptions. First, a male height follows a Normal distribution. Second, the average height of men is 175 cm. Third, the standard deviation for male height is 9 cm. We can create a Normal distribution using the rnorm() function with these two parameter values.\n\n\n\n\n\nAgain, the Normal distribution which is a probability distribution that is symmetric about the mean described by this formula.\n\\[y_i \\sim N(\\mu, \\sigma^2)\\]\n\nEach value \\(y_i\\) is drawn from a Normal distribution with parameters \\(\\mu\\) for the mean and \\(\\sigma\\) for the standard deviation. If the assumptions are correct, then, as our sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nThe posterior distribution for heights depends on the context. Are we considering all the adult men in America? In that case, our posterior would probably look a lot like the empirical distribution using NHANES data. If we are being asked about the distribution of heights among players in the NBA, then our posterior might look like:\n\n\n\n\n\nCaveats:\n\nContinuous variables are a myth. Nothing that can be represented on a computer is truly continuous. Even something which appears continuous, like height, actually can only take on a (very large) set of discrete variables.\nThe math of continuous probability distributions can be tricky. Read a book on mathematical probability for all the messy details. Little of that matters in applied work.\nThe most important difference is that, with discrete distributions, it makes sense to estimate the probability of a specific outcome. What is the probability of rolling a 9? With continuous distributions, this makes no sense because there are an infinite number of possible outcomes. With continuous variables, we only estimate intervals.\n\nDon’t worry about the distinctions between discrete and continuous outcomes, or between the discrete and continuous probability distributions which we will use to summarize our beliefs about those outcomes. The basic intuition is the same in both cases.\n\n5.2.5 Joint distributions\n\nRecall that \\(P(\\text{coin})\\) is the probability distribution for the result of a coin toss. It includes two parts, the probability of heads (\\(\\rho_h\\)) and the probability of tails (\\(\\rho_t\\)). This is a univariate distribution because there is only one outcome, which can be heads or tails. If there is more than one outcome, then we have a joint distribution.\n\nJoint distributions are also mathematical objects that cover a set of outcomes, where each distinct outcome has a chance of occurring between 0 and 1 and the sum of all chances must equal 1. The key to a joint distribution is that it measures the chance that both outcome \\(a\\) from the set of events A and outcome \\(b\\) from the set of events B will occur. The notation is \\(P(A, B)\\).\nLet’s say that you are rolling two six-sided dice simultaneously. Dice 1 is weighted so that there is a 50% chance of rolling a 6 and a 10% chance of each of the other values. Dice 2 is weighted so there is a 50% chance of rolling a 5 and a 10% chance of rolling each of the other values. Let’s roll both dice 1,000 times. In previous examples involving two dice, we cared about the sum of results and not the outcomes of the first versus the second dice of each simulation. With a joint distributions, the outcomes for individual dice matter; so instead of 11 possible outcomes on the x-axis of our distribution plot (ranging from 2 to 12), we have 36 outcomes. Furthermore, a 2D probability distribution is not sufficient to represent all of the variables involved, so the joint distribution for this example is displayed using a 3D plot.\n\n\n\n\n\n\n\n5.2.6 Conditional distrubutions\nImagine that 60% of people in a community have a disease. A doctor develops a test to determine if a random person has the disease. However, this test isn’t 100% accurate. There is an 80% probability of correctly returning positive if the person has the disease and 90% probability of correctly returning negative if the person does not have the disease.\nThe probability of a random person having the disease is 0.6. Since each person either has the disease or doesn’t (those are the only two possibilities), the probability that a person does not have the disease is \\(1 - 0.6 = 0.4\\).\n\n\n\n\n\n\nIf a person has the disease, then we go up the top branch. The probability of an infected person testing positive is 0.8 because the test is 80% sure of correctly returning positive when the person has the disease.\nBy the same logic, if a person does not have the disease, we go down the bottom branch. The probability of the person incorrectly testing positive is 0.1.\n\nWe decide to go down the top branch if our random person has the disease. We go down the bottom branch if they do not. This is conditional probability. The probability of testing positive is dependent on whether the person has the disease.\nHow would you express this in statistical notation? \\(P(A|B)\\) is the same thing as the probability of A given B. \\(P(A|B)\\) means the probability of A if we know for sure the value of B. Note that \\(P(A|B)\\) is not the same thing as \\(P(B|A)\\).\nThere are three main categories of probability distributions: univariate, joint and condictional. \\(p(A)\\) is the probability distribution for event A. This is a univariate probability distribution because there is only one random variable. \\(p(A, B)\\) is the joint probability distribution of A and B. \\(p(A | B)\\) is the conditional probability distribution of A given that B has taken on a specific value. This is often written as \\(p(A | B = b)\\)."
  },
  {
    "objectID": "05-probability.html#sec-rlist-columns-and-map-functions",
    "href": "05-probability.html#sec-rlist-columns-and-map-functions",
    "title": "5  Probability",
    "section": "\n5.3 List-columns and map functions",
    "text": "5.3 List-columns and map functions\n\nWe need to expand our collection of R tricks by learning about list-columns and map_* functions. Recall that a list is different from an atomic vector. In atomic vectors, each element of the vector has one value. Lists, however, can contain vectors, and even more complex objects, as elements.\n\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx\n\n[[1]]\n[1]  4 16  9\n\n[[2]]\n[1] \"A\" \"Z\"\n\n\nx is a list with two elements. The first element is a numeric vector of length 3. The second element is a character vector of length 2. We use [[]] to extract specific elements.\n\nx[[1]]\n\n[1]  4 16  9\n\n\nThere are a number of built-in R functions that output lists. For example, ggplot objects store all of the plot information in a list. Any function that returns multiple values can be used to create a list output by wrapping that returned object with list().\n\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\nrange(x)\n\n[1] -1.841155  1.098223\n\n# We can create a tibble which includes the results of range(x)\n\ntibble(col_1 = list(range(x))) \n\n# A tibble: 1 × 1\n  col_1    \n  <list>   \n1 <dbl [2]>\n\n\nNotice this is a 1-by-1 tibble with one observation, which is a list of one element. Voila! You have just created a list-column.\nIf a function returns multiple values as a vector, like range() does, you must use list() as a wrapper if you want to create a list-column.\nA list column is a column of your data which is a list rather than an atomic vector. As with stand-alone list objects, you can pipe to str() to examine the column.\n\n# tibble() is what we use to generate a tibble, it acts sort of like the\n# mutate(), but mutate() needs a data frame to add new column, tibble can\n# survive on itself.\n\ntibble(col_1 = list(range(x))) |>\n  str()\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -1.84 1.1\n\n\nWe can use map_* functions to both create a list-column and then, much more importantly, work with that list-column afterwards.\n\n# .x is col_1 from tibble and ~ sum(.) is the formula\n\ntibble(col_1 = list(range(x))) |>\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |> \n  str()\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -1.84 1.1\n $ col_2: num -0.743\n\n\nmap_* functions, like map_dbl() in this example, take two key arguments, .x (the data which will be acted on) and .f (the function which will act on this data). Here, .x is the data in col_1, which is a list-column. .f is the function sum(). However, we can not simply write map_dbl(col_1, sum). Instead, each use of map_* functions requires the use of a tilde — a ~ — to indicate the start of the function and the use of a dot — a . — to specify where the data goes in the function.\nmap_* functions are a family of functions, with the suffix specifying the type of the object to be returned. map() itself returns a list. map_dbl() returns a double. map_int() returns an integer. map_chr() returns a character, and so on. Example:\n\ntibble(ID = 1) |> \n  mutate(col_1 = map(ID, ~range(rnorm(10)))) |>\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |> \n  mutate(col_3 = map_int(col_1, ~ length(.))) |> \n  mutate(col_4 = map_chr(col_1, ~ sum(.))) |> \n  str()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `col_4 = map_chr(col_1, ~sum(.))`.\nCaused by warning:\n! Automatic coercion from double to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ ID   : num 1\n $ col_1:List of 1\n  ..$ : num [1:2] -0.741 0.916\n $ col_2: num 0.176\n $ col_3: int 2\n $ col_4: chr \"0.175759\"\n\n\nConsider a more detailed example:\n\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) |> \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) |> \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) |> \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))\n\n# A tibble: 3 × 4\n     ID draws        max min_max  \n  <int> <list>     <dbl> <list>   \n1     1 <dbl [10]> 1.68  <dbl [2]>\n2     2 <dbl [10]> 0.364 <dbl [2]>\n3     3 <dbl [10]> 1.44  <dbl [2]>\n\n\nThis flexibility is only possible via the use of list-columns and map_* functions. This workflow is extremely common. We start with an empty tibble, using ID to specify the number of rows. With that skeleton, each step of the pipe adds a new column, working off a column which already exists."
  },
  {
    "objectID": "05-probability.html#two-models",
    "href": "05-probability.html#two-models",
    "title": "5  Probability",
    "section": "\n5.4 Two models",
    "text": "5.4 Two models\n\n\n\nThe simplest possible setting for inference involves two models — meaning two possible states of the world — and two outcomes from an experiment. Imagine that there is a disease — Probophobia, an irrational fear of probability — which you either have or don’t have. We don’t know if you have the diseases, but we do assume that there are only two possibilities.\nWe also have a test which is 99% accurate when given to a person who has Probophobia. Unfortunately, the test is only 50% accurate for people who do not have Probophobia. In this experiment, there only two possible outcomes: a positive and a negative result on the test.\nQuestion: If you test positive, what is the probability that you have Probophobia?\nMore generally, we are estimating a conditional probability. Conditional on the outcome of a postive test, what is the probability that you have Probophobia? Mathematically, we want:\n\\[ P(\\text{Probophobia | Test = Postive} ) \\]\nTo answer this question, we need to use the tools of joint and conditional probability from earlier in the Chapter. We begin by building, by hand, the joint distribution of the possible models (you have the Probophobia or you do not) and of the possible outcomes (you test positive or negative). Building the joint distribution involves assuming that each model is true and then creating the distribution of outcomes which might occur if that assumption is true.\nFor example, assume you have Probophobia. There is then a 50% chance that you test positive and a 50% chance you test negative. Similarly, if we assume that the second model is true — that you don’t have Probophobia — then there is 1% chance you test positive and a 99% you chance negative. Of course, for you (or any individual) we do not know for sure what is happening. We do not know if you have the disease. We do not know what your test will show. But we can use these relationships to construct the joint distribution.\n\n\n\n\n\n# Pipes generally start with tibbles, so we start with a tibble which just\n# includes an ID variable. We don't really use ID. It is just handy for getting\n# organized. We call this object `jd_disease`, where the `jd` stands for\n# joint distribution.\n\nsims <- 10000\n\njd_disease <- tibble(ID = 1:sims, have_disease = rep(c(TRUE, FALSE), 5000)) |>\n  mutate(positive_test =\n           if_else(have_disease,\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.99)),\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.5))))\n\n\n\njd_disease\n\n# A tibble: 10,000 × 3\n      ID have_disease positive_test\n   <int> <lgl>                <int>\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     6 FALSE                    0\n 7     7 TRUE                     1\n 8     8 FALSE                    1\n 9     9 TRUE                     1\n10    10 FALSE                    0\n# … with 9,990 more rows\n\n\nThe first step is to simply create an tibble that consists of the simulated data we need to plot our distribution. Keep in mind that in the setting we have two different probabilities and they are completely separate from each other and we want to keep the two probabilities and the disease results in two and only two columns so that we can graph using the ggplot() function. And that’s why we used the rep and seq functions when creating the table, we used the seq function to set the sequence we wants, in this case is only two numbers, 0.01 (99% accuracy for testing negative if no disease, therefore 1% for testing positive if no disease) and 0.5 (50% accuracy for testing positive/negative if have disease), then we used the rep functions to repeat the process 10,000 times for each probability, in total 20,000 times. Note that this number “20,000” also represent the number of observations in our simulated data, we simulated 20,000 results from testing, where 10,000 results from the have-disease group and 10,000 for the no-disease group, we often use the capital N to represent the population, in this simulated data \\(N=20,000\\).\nPlot the joint distribution:\n\n\n\n\n\nBelow is a joint distribution displayed in 3D. Instead of using the “jitter” feature in R to unstack the dots, we are using a 3D plot to visualize the number of dots in each box. The number of people who correctly test negative is far greater than of the other categories. The 3D plot shows the total number of cases for each section (True positive, True negative, False positive, False negative),the 3D bar coming from those combinations. Now,pay attention to the two rows of the 3D graph, if you trying to add up the length of the 3D bar for the top two sections and the bottom two sections, they should be equal to each other, where each have 10,000 case. This is because we simulate the experience in two independent and separate world one in the have-disease world and one in the no-disease world.\n\n\n\n\n\n\n\n\nThis Section is called “Two Models” because, for each person, there are two possible states of the world: have the disease or not have the disease. By assumption, there are no other outcomes. We call these two possible states of the world “models,” even though they are very simple models.\nIn addition to the two models, we have two possible results of our experiment on a given person: test positive or test negative. Again, this is an assumption. We do not allow for any other outcome. In coming sections, we will look at more complex situations where we consider more than two models and more than two possible results of the experiment. In the meantime, we have built the unnormalized joint distribution for models and results. This is a key point! Look back earlier in this Chapter for discussions about both unnormalized distributions and joint distributions.\nWe want to analyze these plots by looking at different slices. For instance, let’s say that you have tested positive for the disease. Since the test is not always accurate, you cannot be 100% certain that you have it. We isolate the slice where the test result equals 1 (meaning positive).\n\njd_disease |> \n  filter(positive_test == 1)\n\n# A tibble: 7,484 × 3\n      ID have_disease positive_test\n   <int> <lgl>                <int>\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     7 TRUE                     1\n 7     8 FALSE                    1\n 8     9 TRUE                     1\n 9    11 TRUE                     1\n10    12 FALSE                    1\n# … with 7,474 more rows\n\n\nMost people test positive are infected This is a result for common diseases like cold. We can easily create an unnormalized conditional distribution with:\n\n\n\n\n\nfilter() transforms a joint distribution into a conditional distribution.\nTurn this unnormalized distribution into a posterior probability distribution:\n\n\n\n\n\nIf we zoom in on the plot, about 70% of people who tested positive have the disease and 30% who tested positive do not have the disease. In this case, we are focusing on one slice of the probability distribution where the test result was positive. There are two disease outcomes: positive or negative. By isolating a section, we are looking at a conditional distribution. Conditional on a positive test, you can visualize the likelihood of actually having the disease versus not.\nNow recalled the question we asked at the start of the session: If you test positive, what is the probability that you have Probophobia?\nBy looking at the posterior graph we just create, we can answer this question easily: With a positive test, you can be almost 70% sure that you have Probophobia, however there is a good chance about 30% that you receive a false positive, so don’t worry too much there is still about a third of hope that you get the wrong result\nNow let’s consider the manipulation of this posterior, here is another question. Question : 10 people walks up to testing center, 5 of them tested negative, 5 of them tested positive, what is the probability of at least 6 people are actually healthy? \n\ntibble(test = 1:100000) |>\n  mutate(person1 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person2 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person3 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person4 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person5 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person6 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person7 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person8 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person9 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person10 = map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  select(!test) |> \n  \n  # The tricky part of this code is that we want to sum the outcomes across the\n  # rows of the tibble. This is different from our usual approach of summing\n  # down the columns, as with summarize(). The way to do this is to, first, use\n  # rowwise() to tell R that we want to work with rows in the tibble and then,\n  # second, use c_across() to indicate which variables we want to work with.\n  \n  rowwise() |> \n  mutate(total = sum(c_across(person1:person10))) |>\n  \n  ggplot(aes(total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   binwidth = 1,\n                   color = \"white\") +\n    scale_x_continuous(breaks = c(0:10)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "objectID": "05-probability.html#three-models",
    "href": "05-probability.html#three-models",
    "title": "5  Probability",
    "section": "\n5.5 Three models",
    "text": "5.5 Three models\n\n\n\n\n\n\nImagine that your friend gives you a bag with two marbles. There could either be two white marbles, two black marbles, or one of each color. Thus, the bag could contain 0% white marbles, 50% white marbles, or 100% white marbles. The proportion, \\(p\\), of white marbles could be, respectively, 0, 0.5, or 1.\nQuestion: What is the chance of the bag contains exactly two white marbles, given that when we selected the marbles three times, every time we select a white marble?\n\\[ P(\\text{2 White Marbles in bag | White Marbles Sampled = 3} ) \\] Just as during the Probophobia models, in order to answer this question, we need to start up with the simulated data and then graphing out the joint distribution of this scenerio because we need to considered all possible outcomes of this model, and then based on the joint distribution we can slice out the the part we want (the conditional distribution) in the end making a posterior graph as well as normalizing it to see the probability.\nStep 1: Simulate the data into an tibble\nLet’s say you take a marble out of the bag, record whether it’s black or white, then return it to the bag. You repeat this three times, observing the number of white marbles you see out of three trials. You could get three whites, two whites, one white, or zero whites as a result of this trial. We have three models (three different proportions of white marbles in the bag) and four possible experimental results. Let’s create 3,000 draws from this joint distribution:\n\n# Create the joint distribution of the number of white marbles in the bag\n# (in_bag) and the number of white marbles pulled out in the sample (in_sample),\n# one-by-one. in_bag takes three possible values: 0, 1 and 2, corresponding to\n# zero, one and two white marbles potentially in the bag.\n\nset.seed(3)\nsims <- 10000\n\n# We also start off with a tibble. It just makes things easier\n\njd_marbles <- tibble(ID = 1:sims) |> \n  \n  # For each row, we (randomly!) determine the number of white marbles in the\n  # bag. We do not know why the `as.integer()` hack is necessary. Shouldn't\n  # `map_int()` automatically coerce the result of `sample()` into an integer?\n  \n  mutate(in_bag = map_int(ID, ~ as.integer(sample(c(0, 1, 2), \n                                                  size = 1)))) |>\n  \n  # Depending on the number of white marbles in the bag, we randomly draw out 0,\n  # 1, 2, or 3 white marbles in our experiment. We need `p = ./2` to transform\n  # the number of white marbles into the probability of drawing out a white\n  # marble in a single draw. That probability is either 0%, 50% or 100%.\n  \n  mutate(in_sample = map_int(in_bag, ~ rbinom(n = 1, \n                                              size = 3, \n                                              p = ./2))) \n\njd_marbles\n\n# A tibble: 10,000 × 3\n      ID in_bag in_sample\n   <int>  <int>     <int>\n 1     1      0         0\n 2     2      1         3\n 3     3      2         3\n 4     4      1         1\n 5     5      2         3\n 6     6      2         3\n 7     7      1         0\n 8     8      2         3\n 9     9      0         0\n10    10      1         2\n# … with 9,990 more rows\n\n\nStep 2: Plot the joint distribution:\n\n# The distribution is unnormalized. All we see is the number of outcomes in each\n# \"bucket.\" Although it is never stated clearly, we are assuming that there is\n# an equal likelihood of 0, 1 or 2 white marbles in the bag.\n\njd_marbles |>\n  ggplot(aes(x = in_sample, y = in_bag)) +\n    geom_jitter(alpha = 0.5) +\n    labs(title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more white marbles selected\",\n         x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\") +\n    scale_y_continuous(breaks = c(0, 1, 2)) +\n  theme_classic()\n\n\n\n\nHere is the 3D visualization:\n\n\n\n\n\n\nThe y-axes of both the scatterplot and the 3D visualization are labeled “Number of White Marbles in the Bag.” Each value on the y-axis is a model, a belief about the world. For instance, when the model is 0, we have no white marbles in the bag, meaning that none of the marbles we pull out in the sample will be white.\nNow recalls the question, we essentially only care about the fourth column in the joint distribution (x-axis=3) because the question is asking us to create a conditional distribution given that fact that 3 marbles were selected. Therefore, we could isolate the slice where the result of the simulation involves three white marbles and zero black ones. Here is the unnormalized probability distribution.\nStep 3: Plot the unnormalized conditional distribution.\n\n# The key step is the filter. Creating a conditional distribution from a joint\n# distribution is the same thing as filtering that joint distribution for a\n# specific value. A conditional distribution is a \"slice\" of the joint\n# distribution, and we take that slice with filter().\n\njd_marbles |> \n  filter(in_sample == 3) |> \n  ggplot(aes(in_bag)) +\n    geom_histogram(binwidth = 0.5, color = \"white\") +\n    labs(title = \"Unnormalized Conditional Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Count\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    theme_classic()\n\n\n\n\nStep 4: Plot the normalize posterior distribution. Next, let’s normalize the distribution.\n\njd_marbles |> \n  filter(in_sample == 3) |> \n  ggplot(aes(in_bag)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Probability\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nThis plot makes sense because when all three marbles you draw out of the bag are white, there is a pretty good chance that there are no black marbles in the bag. But you can’t be certain! It is possible to draw three white even if the bag contains one white and one black. However, it is impossible that there are zero white marbles in the bag.\nLastly let’s answer the question: What is the chance of the bag contains exactly two white marbles, given that when we selected the white marbles three times, everytime we select a white marble?\nAnswer: As the Posterior Probability Distribution shows (x-axis=2), the chance of the bag contains exactly two white marbles given that we select 3 white marbles out of three tries is about 85%."
  },
  {
    "objectID": "05-probability.html#sec-n-models",
    "href": "05-probability.html#sec-n-models",
    "title": "5  Probability",
    "section": "\n5.6 N models",
    "text": "5.6 N models\n\n\n\n\n\n\n\n\nAssume that there is a coin with \\(\\rho_h\\). We guarantee that there are only 11 possible values of \\(\\rho_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). In other words, there are 11 possible models, 11 things which might be true about the world. This is just like situations we have previously discussed, except that there are more models to consider.\nWe are going to run an experiment in which you flip the coin 20 times and record the number of heads. What does this result tell you about the value of \\(\\rho_h\\)? Ultimately, we will want to calculate a posterior distribution of \\(\\rho_h\\), which is written as p(\\(\\rho_h\\)).\nQuestion: What is the probability of getting exactly 8 heads out of 20 tosses?\nTo start, it is useful to consider all the things which might happen if, for example, \\(\\rho_h = 0.4\\). Fortunately, the R functions for simulating random variables makes this easy.\n\n\n\n\n\nFirst, notice that many different things can happen! Even if we know, for certain, that \\(\\rho_h = 0.4\\), many outcomes are possible. Life is remarkably random. Second, the most likely result of the experiment is 8 heads, as we would expect. Third, we have transformed the raw counts of how many times each total appeared into a probability distribution. Sometimes, however, it is convenient to just keep track of the raw counts. The shape of the figure is the same in both cases.\n\n\n\n\n\nEither way, the figures show what would have happened if that model — that \\(\\rho_h = 0.4\\) — were true.\nWe can do the same thing for all 11 possible models, calculating what would happen if each of them were true. This is somewhat counterfactual since only one of them can be true. Yet this assumption does allow us to create the joint distribution of models which might be true and of data which our experiment might generate. Let’s simplify this is p(models, data), although you should keep the precise meaning in mind.\n\n\n\n\n\nHere is the 3D version of the same plot.\n\n\n\n\n\n\nIn both of these diagrams, we see 11 models and 21 outcomes. We don’t really care about the p(\\(models\\), \\(data\\)), the joint distribution of the models-which-might-be-true and the data-which-our-experiment-might-generate. Instead, we want to estimate \\(p\\), the unknown parameter which determines the probability that this coin will come up heads when tossed. The joint distribution alone can’t tell us that. We created the joint distribution before we had even conducted the experiment. It is our creation, a tool which we use to make inferences. Instead, we want the conditional distribution, p(\\(models\\) | \\(data = 8\\)). We have the results of the experiment. What do those results tell us about the probability distribution of \\(p\\)?\nTo answer this question, we simply take a vertical slice from the joint distribution at the point of the x-axis corresponding to the results of the experiment.\nThis animation shows what we want to do with joint distributions. We take a slice (the red one), isolate it, rotate it to look at the conditional distribution, normalize it (change the values along the current z-axis from counts to probabilities), then observe the resulting posterior.\n\n\nThis is the only part of the joint distribution that we care about. We aren’t interested in what the object looks like where, for example, the number of heads is 11. That portion is irrelevant because we observed 8 heads, not 11. By using the filter function on the simulation tibble we created, we can conclude that there are a total of 465 times in our simulation in which 8 heads were observed.\nAs we would expect, most of the time when 8 coin tosses came up heads, the value of \\(p\\) was 0.4. But, on numerous occasions, it was not. It is quite common for a value of \\(p\\) like 0.3 or 0.5 to generate 8 heads. Consider:\n\n\n\n\n\nYet this is a distribution of raw counts. It is an unnormalized density. To turn it into a proper probability density (i.e., one in which the sum of the probabilities across possible outcomes sums to one) we just divide everything by the total number of observations.\n\n\n\n\n\nSolution:\nThe most likely value of \\(\\rho_h\\) is 0.4, as before. But, it is much more likely that \\(p\\) is either 0.3 or 0.5. And there is about an 8% chance that \\(\\rho_h \\ge 0.6\\).\nYou might be wondering: what is the use of a model? Well, let’s say we toss the coin 20 times and get 8 heads again. Given this result, we can ask: What is the probability that future samples of 20 flips will result in 10 or more heads?\nThere are three main ways you could go about solving this problem with simulations.\nThe first wrong way to do this is assuming that \\(\\rho_h\\) is certain because we observed 8 heads after 20 tosses. We would conclude that 8/20 gives us 0.4. The big problem with this is that you are ignoring your uncertainty when estimating \\(\\rho_h\\). This would lead us to the following code.\n\nsims <- 10000000\n\nodds <- tibble(sim_ID = 1:sims) |>\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) |> \n  mutate(above_ten = if_else(heads >= 10, TRUE, FALSE))\n\nodds\n\n# A tibble: 10,000,000 × 3\n   sim_ID heads above_ten\n    <int> <int> <lgl>    \n 1      1    10 TRUE     \n 2      2     5 FALSE    \n 3      3     2 FALSE    \n 4      4    10 TRUE     \n 5      5     5 FALSE    \n 6      6    10 TRUE     \n 7      7     7 FALSE    \n 8      8    11 TRUE     \n 9      9     9 FALSE    \n10     10     9 FALSE    \n# … with 9,999,990 more rows\n\n\n\nodds |>\n  ggplot(aes(x=heads,fill=above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Wrong Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nUsing this Posterior distribution derived from the (wrong way) simulated data, the probability results in 10 or more head is\n\nodds |>\n  summarize(success = sum(above_ten)/sims)\n\n# A tibble: 1 × 1\n  success\n    <dbl>\n1   0.245\n\n\nabout 24.5%.\nThe second method involves sampling the whole posterior distribution vector we previously created. This would lead to the following correct code.\n\np_draws <- tibble(p = rep(seq(0, 1, 0.1), 1000)) |>\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |>\n  filter(heads == 8)\n  \nodds_2nd <- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) |>\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |> \n  mutate(above_ten = if_else(heads >= 10, TRUE, FALSE)) \n\nodds_2nd\n\n# A tibble: 10,000,000 × 3\n       p heads above_ten\n   <dbl> <int> <lgl>    \n 1   0.4     7 FALSE    \n 2   0.3     8 FALSE    \n 3   0.5    13 TRUE     \n 4   0.4    10 TRUE     \n 5   0.4     6 FALSE    \n 6   0.5     8 FALSE    \n 7   0.5     9 FALSE    \n 8   0.5    12 TRUE     \n 9   0.5    10 TRUE     \n10   0.4     8 FALSE    \n# … with 9,999,990 more rows\n\n\n\nodds_2nd |>\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nUsing this Posterior distribution derived from the (right way 1st) simulated data, the probability results in 10 or more head is\n\nodds_2nd |>\n  summarize(success = sum(above_ten)/sims)\n\n# A tibble: 1 × 1\n  success\n    <dbl>\n1   0.351\n\n\nabout 32.8%\nAs you may have noticed, if you calculated the value using the first method, you would believe that getting 10 or more heads is less likely than it really is. If you were to run a casino based on these assumptions, you will lose all your money. It is very important to be careful about the assumptions you are making. We tossed a coin 20 times and got 8 heads. However, you would be wrong to assume that \\(\\rho_h\\) = 0.4 just based on this result."
  },
  {
    "objectID": "05-probability.html#working-with-probability-distributions",
    "href": "05-probability.html#working-with-probability-distributions",
    "title": "5  Probability",
    "section": "\n5.7 Working with probability distributions",
    "text": "5.7 Working with probability distributions\n\nA probability distribution is not always easy to work with. It is a complex object. And, in many contexts, we don’t really care about all that complexity. So, instead of providing the full probability distribution, we often just use a summary measure, a number or two or three which captures those aspects of the entire distribution which are relevant to the matter at hand. Let’s explore these issues using the 538 posterior probability distribution, as of August 13, 2020, for the number of electoral votes which will be won by Joe Biden. Here is a tibble with 1,000,000 draws from that distribution:\n\n\n\n\ndraws\n\n# A tibble: 1,000,000 × 2\n      ID electoral_votes\n   <int>           <int>\n 1     1             210\n 2     2             277\n 3     3             227\n 4     4             397\n 5     5             378\n 6     6             428\n 7     7             350\n 8     8             385\n 9     9             325\n10    10             463\n# … with 999,990 more rows\n\n\nA distribution and a sample of draws from that distribution are different things. But, if you squint, they are sort of the same thing, at least for our purposes. For example, if you want to know the mean of the distribution, then the mean of the draws will be a fairly good estimate, especially if the number of draws is large enough.\n\nRecall from Chapter 2 how we can draw randomly from specified probability distributions:\n\nrnorm(10)\n\n [1]  2.23367571  2.23194835 -0.62407149 -0.01425707  2.36987572  0.42803447\n [7] -0.55631165  0.68773344 -0.63241510  0.55645222\n\n\n\nrunif(10)\n\n [1] 0.8557192 0.9369414 0.4096510 0.6725821 0.2898478 0.2128326 0.5443893\n [8] 0.3711834 0.4837470 0.8043280\n\n\nThe elements of these vectors are all “draws” from the specified probability distributions. In most applied situations, our tools will produce draws rather than summary objects. Fortunately, a vector of draws is very easy to work with. Start with summary statistics:\n\n# recall mean, media, standard deviation and mad functions.\n\nkey_stats <- draws |> \n  summarize(mn = mean(electoral_votes),\n            md = median(electoral_votes),\n            sd = sd(electoral_votes),\n            mad = mad(electoral_votes))\n\nkey_stats\n\n# A tibble: 1 × 4\n     mn    md    sd   mad\n  <dbl> <dbl> <dbl> <dbl>\n1  325.   326  86.9  101.\n\n\nCalculate a 95% interval directly:\n\nquantile(draws$electoral_votes, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n  172   483 \n\n\nApproximate the 95% interval in two ways:\n\nc(key_stats$mn - 2 * key_stats$sd, \n  key_stats$mn + 2 * key_stats$sd)\n\n[1] 151.5461 499.0198\n\nc(key_stats$md - 2 * key_stats$mad, \n  key_stats$md + 2 * key_stats$mad)\n\n[1] 124.3664 527.6336\n\n\nIn this case, using the mean and standard deviation produces a 95% interval which is closer to the true interval. In other cases, the median and scaled median absolute deviation will do better. Either approximation is generally “good enough” for most work. But, if you need to know the exact 95% interval, you must use quantile()."
  },
  {
    "objectID": "05-probability.html#cardinal-virtues",
    "href": "05-probability.html#cardinal-virtues",
    "title": "5  Probability",
    "section": "\n5.8 Cardinal Virtues",
    "text": "5.8 Cardinal Virtues\nThe four Cardinal Virtues are Wisdom, Justice, Courage, and Temperance. Because data science is, ultimately, a moral act, we use these virtues to guide our work. Every data science project begins with a question.\n\nWisdom starts by creating the Preceptor Table. What data, if we had it, would allow us to answer our question easily? If the Preceptor Table has one outcome, then the model is predictive. If it has more than one (potential) outcome, then the model is causal. We then explore the data we have. You can never look too closely at your data. Key question: Are the data we have close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population? If not, we can’t proceed further. Key in making that decision is the assumption of validity. Do the columns in the Preceptor Table match the columns in the data?\nJustice starts with the Population Table – the data we want to have, the data which we actually have and all the other data from that same population. Each row of the Population Table is defined by a unique Unit/Time combination. We explore three key issues about the Population Table. First, does the relationship among the variables demonstrate stability, meaning is the model stable across different time periods? Second, are the rows associated with the data representative of all the units which we might have had data for from that time period? Third, for causal models only, we consider unconfoundedness. Justice concludes by making an assumption about the data generating mechanism. Which general mathematical formula connects the outcome variable we are interested in with the other data that we have?\nCourage allows us to explore different models. Even though Justice has provided the basic mathematical structure of the model, we still need to decide which variables to include and to estimate the values of unknown parameters. We avoid hypothesis tests. We check our models for consistency with the data we have. We select one model.\nTemperance guides us in the use of the model we have created to answer the questions we began with. We create posteriors of quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\n\n5.8.1 Wisdom\n\n\n\n\nWisdom.\n\n\n\n\nWisdom helps us decide if we can even hope to answer our question with the data we have.\nBegin with the Preceptor Table. What rows and columns of data do you need such that, if you had them all, the calculation of the quantity of interest would be trivial? If you want to know the average height of an adult in India, then the Preceptor Table would include a row for each adult and a column for their height. With no missing data, the average is easy to determine, as are a wide variety of other estimands, other unknown values.\nOne key aspect of this Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. For example, if we want to know the causal effect of exposure to Spanish-speakers on attitude toward immigration then we need a causal model, one which estimates that attitude for each person under both treatment and control. The Preceptor Table would require two columns for the outcome. If, on the other hand, we only want to predict someone’s attitude, or compare one person’s attitude to another’s, then we would only need a Preceptor Table with one column for the outcome.\nAre we are modeling (just) for prediction or are we (also) modeling for causation? Predictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model.\nEvery model is predictive, in the sense that, if we give you new data — and it is drawn from the same population — then you can create a predictive forecast. But only a subset of those models are causal, meaning that, for a given individual, you can change the value of one input and figure out what the new output would be and then, from that, calculate the causal effect by looking at the difference between two potential outcomes.\nWith prediction, all we care about is forecasting Y given X on some as-yet-unseen data. But there is no notion of “manipulation” in such models. We don’t pretend that, for Joe, we could turn variable X from a value of 5 to a value of 6 by just turning some knob and, by doing so, cause Joe’s value of Y to change from 17 to 23. We can compare two people (or two groups of people), one with X equal to 5 and one with X equal to 6, and see how they differ in Y. The basic assumption of predictive models is that there is only one possible Y for Joe. There are not, by assumption, two possible values for Y, one if X equal 5 and another if X equals 6. The Preceptor Table has a single column under Y.\nWith causal inference, however, we can consider the case of Joe with \\(X = 5\\) and Joe with \\(X = 6\\). The same mathematical model can be used. And both models can be used for prediction, for estimating what the value of Y will be for a yet-unseen observation with a specified value for X. But, in this case, instead of only a single column in the Preceptor Table for Y, we have at least two (and possibly many) such columns, one for each of the potential outcomes under consideration.\nThe difference between prediction models and causal models is that the former have one column for the outcome variable and the latter have more than one column.\nSecond, we look at the data we have and perform an exploratory data analysis, an EDA. You can never look at your data too much. The most important variable is the one we most want to understand/explain/predict. In the models we create in later chapters, this variable will go on the left-hand side of our mathematical equations. Some academic fields refer to this as the “dependent variable.” Others use terms like “regressor” or “outcome.” Whatever the terminology, we need to explore the distribution of this variable, its min/max/range, its mean and median, its standard deviation, and so on.\nGelman, Hill, and Vehtari (2020) write:\n\nMost important is that the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient. Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to the cases to which it will be applied.\n\n\nFor example, with regard to the outcome variable, a model of incomes will not necessarily tell you about patterns of total assets. A model of test scores will not necessarily tell you about child intelligence or cognitive development. …\n\nWe care about other variables as well, especially those that are most correlated/connected with the outcome variable. The more time that we spend looking at these variables, the more likely we are to create a useful model.\nThird, the (almost always imaginary) population is key. We need the data we want — the Preceptor Table — and the data we have to be similar enough that we can consider them as all having come from the same statistical population. From Wikipedia:\n\nIn statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all opening hands in all the poker games in Las Vegas tomorrow).\n\nMechanically, assuming that the Precetor Table and the data are drawn from the same population is the same thing as “stacking” the two on top of each other. For that to make sense, the variables must mean the same thing — at least mostly — in both sources. This is the assumption of validity. For example, if the Preceptor Table concerns who people will vote for in the election next week and the data is from a survey taken last week, it is not obvious that we can consider the data as coming from the same population. After all, voting and survey responses are not exactly the same thing. We can only assume that they are — which is precisely what everyone who forecasts elections does — if we assume that both are “valid” measures of the same underlying construct.\nIf we assume that the data we have is drawn from the same population as the data in the Preceptor Table, then we can use information about the former to make inferences about the latter. We can combine the Preceptor Table and the data into a single Population Table. If we can’t do that, if we can’t assume that the two sources come from the same population, then we can’t use our data to answer our questions. We have no choice but to walk away. The heart of Wisdom is knowing when to walk away. As John Tukey noted:\n\nThe combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\n\n\n5.8.2 Justice\n\n\n\n\nJustice.\n\n\n\n\nAfter Wisdom, we have an Population Table. It includes rows for the data we have and the data we want to have. It will almost always have many more rows, rows from the larger population of which the Preceptor Table and our data form a portion. It has missing values, most importantly for potential outcomes which were not observed. The central problem in inference is to fill in the question marks in the Population Table.\nThere are three key issues to explore in any Population Table: stability, representativeness and unconfoundedness.\n\nStability assumes that the relationship between the outcome variable and the covariates is consistent over time. Never forget that temporal nature of almost all real data science problems. Our Preceptor Table will focus on rows for today or for the near future. The data we have will always be from before now. We must almost always assume that the future will be like the past in order to use data from the past to make predictions about the future.\nRepresentativeness is a two-sided concern. We want the data we have to be representative of the population for which we need to calculate parameters. Ideally, we would love for our data to be randomly sampled from the population, but this is almost never the case. But this is a concern, not just with our data, but also for our Preceptor Table. If the data we want is not representative of the entire population then we will need to be careful in the inferences which we draw.\n\nValidity is about the columns in our Population Table. Stability and representativeness are about the rows.\nThe last step of Justice is to make an assumption about the structure of the data generating mechanism (DGM): the mathematical formula, and associated error term, which relates our outcome variable to our covariates.\nJustice requires math. Consider a model of coin-tossing:\n\\[ H_i  \\sim B(\\rho_H, n = 20) \\]\nThe total number \\(H\\) of Heads in experiment \\(i\\) with 20 flips of a single coin, \\(H_i\\), is distributed as a binomial with \\(n = 20\\) and an unknown probability \\(\\rho_h\\) of the coin coming up Heads.\nNote:\n\nThis is a cheat and a simplification! We are Bayesians but we have not specified the full Bayesian machinery. We really need priors on the unknown parameter \\(\\rho_h\\) as well. But that is too complex for an introductory textbook, so we wave our hands, accept the default sensible parameters built into the R packages we use, and point readers to more advanced books, like Gelman, Hill, and Vehtari (2020).\nDefining \\(\\rho_h\\) as the “the probability that the coin comes up Heads” is a bit of a fudge. If you calculate that by hand and then compare it to what our tools produce, they won’t be the same. Instead, the calculated value will be closer to zero. Why? \\(\\rho_h\\) is really the “long-run percentage of the time the coin comes up Heads.” It is not just the percentage from this experiment.\n\n\nIn this simple case, we are fortunate that the parameter \\(\\rho_h\\) has such a (mostly!) simple analog to a real world quantity. Most of the time, parameters are not so easy to interpret. In a more complex model, especially those with interaction terms, we focus less on parameters and more on actual predictions.\n\n5.8.3 Courage\n\n\n\n\nCourage.\n\n\n\n\n\n\nThe three languages of data science are words, math and code, and the most important of these is code. We need to explain the structure of our model using all three languages, but we need Courage to implement the model in code.\nCourage requires us to take the general mathematical formula provide by Justice and then make it specific. Which variables should we include in the model and which do we exclude? Every data science project involves the creation of several models. For each, we specify the precise data generating mechanism. Using that formula, and some R code, we create a fitted model. All models have parameters. We can never know the true values of the parameters, but we can create, and explore, posterior distributions for those unknown true values.\n\nCode allows us to “fit” a model by estimating the values of the unknown parameters, like \\(\\rho_h\\). Sadly, we can never know the true values of these parameters. But, like all good statisticians, we can express our uncertain knowledge in the form of posterior probability distributions. With those distributions, we can compare the actual values of the outcome variable with the “fitted” or “predicted” results of the model. We can examine the “residuals,” the difference between the fitted and actual values.\nEvery outcome is the sum of two parts: the model and what is not in the model:\n\\[outcome = model + what\\ is\\ not\\ in\\ the\\ model\\]\nIt doesn’t matter what the outcome is. It could be the result of a coin flip, the weight of a person, the GDP of a country. Whatever outcome we are considering is always made up of two parts. The first is the model we have created. The second is all the stuff — all the blooming and buzzing complexity of the real world — which is not a part of the model.\nSome of our uncertainty is driven by our ignorance about \\(\\rho_h\\).\nA parameter is something which does not exist in the real world. (If it did, or could, then it would be data.) Instead, a parameter is a mental abstraction, a building block which we will use to to help us accomplish our true goal: To replace at least some of the questions marks in the actual Preceptor Table. Since parameters are mental abstractions, we will always be uncertain as to their value, however much data we might collect.\nBut some, often most, of the uncertainty comes from forces that are, by assumption, not in the model. For example, if the coin is fair, we expect \\(T_i\\) to equal 10. But, often, it will be different, even if we are correct and \\(\\rho_h\\) equals exactly 0.5.\nSome randomness is intrinsic in this fallen world.\n\n5.8.4 Temperance\n\n\n\n\nTemperance.\n\n\n\n\n\n\nThere are few more important concepts in statistics and data science than the “Data Generating Mechanism.” Our data — the data that we collect and see — has been generated by the complexity and confusion of the world. God’s own mechanism has brought His data to us. Our job is to build a model of that process, to create, on the computer, a mechanism which generates fake data consistent with the data which we see. With that DGM, we can answer any question which we might have. In particular, with the DGM, we provide predictions of data we have not seen and estimates of the uncertainty associated with those predictions.\nJustice gave us the structure of the DGM. Courage created the DGM, the fitted model. Temperance will guide us in its use.\nHaving created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.\nSadly, our models are never as good as we would like them to be. First, the world is intrinsically uncertain.\n\n\n\n\nDonald Rumsfeld.\n\n\n\n\n\nThere are known knowns. There are things we know we know. We also know there are known unknowns. That is to say, we know there are some things we do not know. But there are also unknown unknowns, the ones we do not know we do not know. – Donald Rumsfeld\n\nWhat we really care about is data we haven’t seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn’t change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that our forecasts are more uncertain that a naive use of our model might suggest.\nWe need temperance in order to study and understand the unknown unknowns in our data. Temperance is also important when we analyze the “realism” of our models. When we created the mathematical probability distribution for presidential elections, for instance, we assumed that the Democratic candidate would have a 50% chance of winning each vote in the electoral college. By comparing the mathematical model to our empirical cases, however, we recognize that the mathematical model is unlikely to be true. The mathematical model suggested that getting fewer than 100 votes is next to impossible, but many past Democratic candidates in the empirical distribution received less than 100 electoral votes.\nIn Temperance, the key distinction is between the true posterior distribution — what we will call “Preceptor’s Posterior” — and the estimated posterior distribution. Recall our discussion from Section 5.1. Imagine that every assumption we made in Wisdom and Justice were correct, that we correctly understand every aspect of how the world works. We still would not know the unknown value we are trying to estimate — recall the Fundamental Problem of Causal Inference — but the posterior we created would be perfect. That is Preceptor’s Posterior. Sadly, even if our estimated posterior is, very close to Preceptor’s Posterior, we can never be sure of that fact, because we can never know the truth, never be certain that all the assumptions we made are correct.\nEven worse, we must always worry that our estimated posterior, despite all the work we put into creating it, is far from the truth. We, therefore, must be cautious in our use of that posterior, humble in our claims about its accuracy. Using our posterior, despite its fails, is better than not using it. Yet it is, as best, a distorted map of reality, a glass through which we must look darkly. Use your posterior with humility."
  },
  {
    "objectID": "05-probability.html#summary",
    "href": "05-probability.html#summary",
    "title": "5  Probability",
    "section": "\n5.9 Summary",
    "text": "5.9 Summary\n\nThroughout this Chapter, we spent time going through examples of conditional distributions. However, it’s worth noting that all probability distributions are conditional on something. Even in the most simple examples, when we were flipping a coin multiple times, we were assuming that the probability of getting heads versus tails did not change between tosses.\nWe also discussed the difference between empirical, mathematical, and posterior probability distributions. Even though we developed these heuristics to better understand distributions, every time we make a claim about the world, it is based on our beliefs - what we think about the world. We could be wrong. Our beliefs can differ. Two reasonable people can have conflicting beliefs about the fairness of a dice.\nAt the start of this chapter we briefly discuss the definition of an random variable, yet we sort of let it go for the rest of the chapter, but it’s hiding almost everywhere whenever we create an distribution. For example, in two models we have two random variables, the have disease and the don’t have disease, in three models we have three random variables, either to have 0 or 1 or 2 white marbles in bad. Essentially is just like the missing values in the Preceptor table what random variables do you need to know the values of to answer the question? \nIt is useful to understand the three types of distributions and the concept of conditional distributions, but almost every probability distribution is conditional and posterior. We can leave out both words in future discussions, as we generally will in this book. They are implicit.\nIf you are keen to learn more about probability, here is a video featuring Professor Gary King. This is a great way to review some of the concepts we covered in this chapter, albeit at a higher level of mathematics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press. https://doi.org/10.1017/9781139161879."
  },
  {
    "objectID": "06-one-parameter.html#sec-sampling-activity",
    "href": "06-one-parameter.html#sec-sampling-activity",
    "title": "6  One Parameter",
    "section": "\n6.1 Real sampling activity",
    "text": "6.1 Real sampling activity\n\n\n\n\nAn urn with red and white beads.\n\n\n\n\n\n6.1.1 Using the shovel method once\nInstead of performing an exhaustive count, let’s insert a shovel into the urn and remove \\(5 \\cdot 10 = 50\\) beads. We are taking a sample of the total population of beads.\n\n\n\n\nInserting a shovel into the urn.\n\n\n\n\n\n\n\n\nRemoving 50 beads from the urn.\n\n\n\n\nObserve that 17 of the 50 sampled beads are red and thus \\(17/50 = 0.34 = 34\\%\\) of the shovel’s beads are red. We can view the proportion of beads that are red in this shovel as a guess of the proportion of beads that are red in the entire urn. While not as exact as doing an exhaustive count of all the beads in the urn, our guess of 34% took much less time and energy to make.\nRecall that \\(\\rho\\) is the true value of the proportion of red beads. There is only one \\(\\rho\\). Our guesses at the proportion of red beads are known as \\(\\hat{\\rho}\\) (pronounced p hat), where \\(\\hat{\\rho}\\) is the estimated value of \\(\\rho\\) which comes from taking a sample. There are many possible \\(\\hat{\\rho}\\)’s. You and I will often differ in our estimates. We each have a different \\(\\hat{\\rho}\\) even though we agree that there is only one \\(\\rho\\).\nStart this activity over from the beginning, placing the 50 beads back into the urn. Would a second sample include exactly 17 red beads? Maybe, but probably not.\nWhat if we repeated this activity many times? Would our guess at the proportion of the urn’s beads that are red, \\(\\hat{\\rho}\\), be exactly 34% every time? Surely not.\nLet’s repeat this exercise with the help of 33 groups of friends to understand how the value of \\(\\hat{\\rho}\\) varies across 33 independent trials.\n\n6.1.2 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 beads each.\nCount the number of red beads and compute the proportion of the 50 beads that are red.\nReturn the beads into the urn.\nMix the contents of the urn to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red beads from their sample collected. Each group then marks their proportion of their 50 beads that were red in the appropriate bin in a hand-drawn histogram as seen below.\n\n\n\n\nConstructing a histogram of proportions.\n\n\n\n\nHistograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in the figure below.\n\n\n\n\nHand-drawn histogram of first 10 out of 33 proportions.\n\n\n\n\nObserve the following details in the histogram:\n\nAt the low end, one group removed 50 beads from the urn with proportion red between 0.20 and 0.25.\nAt the high end, another group removed 50 beads from the urn with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution.\nThe distribution is somewhat bell-shaped.\n\ntactile_sample_urn saves the results from our 33 groups of friends.\n\n\n# A tibble: 33 × 4\n   group           red_beads prop_red group_ID\n   <chr>               <dbl>    <dbl>    <int>\n 1 Yuki, Harry            21     0.42        1\n 2 Aayush, Karen          15     0.3         2\n 3 Mak, Sophie            17     0.34        3\n 4 Ace, Chris             18     0.36        4\n 5 Ian, Iman              18     0.36        5\n 6 Caroline, Edna         15     0.3         6\n 7 Mark, Jane             21     0.42        7\n 8 Mia, James             15     0.3         8\n 9 Ellie, Terrance        17     0.34        9\n10 Ishan, Cass            15     0.3        10\n# … with 23 more rows\n\n\nFor each group, we are given their names, the number of red_beads they obtained, and the corresponding proportion out of 50 beads that were red, called prop_red. We also have an group_ID variable which gives each of the 33 groups a unique identifier. Each row can be viewed as one instance of a replicated activity: using the shovel to remove 50 beads and computing the proportion of those beads that are red.\nLet’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05. This is a computerized and complete version of the partially completed hand-drawn histogram you saw earlier. Setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. color = \"white\" modifies the color of the boundary for visual clarity.\n\ntactile_sample_urn |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # Add scale_y_continuous with breaks by 1, as the default shows the y-axis\n  # from 1 to 10 with breaks at .5. Breaks by 1 is better for this plot, as all\n  # resulting values are integers.\n  \n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  \n  # The call expression() is used to insert a mathematical expression, like\n  # p-hat. The paste after expression allows us to paste text prior to said\n  # expression.\n  \n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")),\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") \n\n\n\n\n\n6.1.3 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We want to know the proportion of red beads in the urn, with the urn being our population. Performing an exhaustive count of the red and white beads would be too time-consuming. Therefore, it is much more practical to extract a sample of 50 beads using the shovel. Using this sample of 50 beads, we estimated the proportion of the urn’s beads that are red to be about 34%.\nMoreover, because we mixed the beads before each use of the shovel, the samples were random and independent. Because each sample was drawn at random, the samples were different from each other. This is an example of sampling variation. For example, what if instead of selecting 17 beads in our first sample we had selected just 11? Does that mean that the population proportion of the beads is 11/50 or 22%? No! Because we performed 33 trials we can look to our histogram, and see that the peak of the distribution occurs when \\(.35 < \\hat{\\rho} < .4\\) , so it is very likely that the proportion of red beads in the entire population will also fall in or near this range."
  },
  {
    "objectID": "06-one-parameter.html#sec-virtual-sampling",
    "href": "06-one-parameter.html#sec-virtual-sampling",
    "title": "6  One Parameter",
    "section": "\n6.2 Virtual sampling",
    "text": "6.2 Virtual sampling\nWe just performed a tactile sampling activity. We used a physical urn of beads and a physical shovel. We did this by hand so that we could develop our intuition about the ideas behind sampling. In this section, we mimic this physical sampling with virtual sampling, using a computer.\n\n6.2.1 Using the virtual shovel once\nVirtual sampling requires a virtual urn and a virtual shovel. Create a tibble named urn. The rows of urn correspond exactly to the contents of the actual urn.\n\n# set.seed() ensures that the beads in our virtual urn are always in the same\n# order. This ensures that the figures in the book match their written\n# descriptions. We want 40% of the beads to be red.\n\nset.seed(10)\n\nurn <- tibble(color = c(rep(\"red\", 400), \n                        rep(\"white\", 600))) |>\n  \n  # sample_frac() keeps all the rows in the tibble but rearranges their order.\n  # We don't need to do this. A virtual urn does not care about the order of the\n  # beads. But we find it aesthetically pleasing to mix them up.\n  \n  sample_frac() |> \n  mutate(bead_ID = 1:1000) \n\nurn  \n\n# A tibble: 1,000 × 2\n   color bead_ID\n   <chr>   <int>\n 1 white       1\n 2 white       2\n 3 red         3\n 4 red         4\n 5 white       5\n 6 white       6\n 7 white       7\n 8 white       8\n 9 white       9\n10 white      10\n# … with 990 more rows\n\n\nObserve that urn has 1,000 rows, meaning that the urn contains 1,000 beads. The first variable bead_ID is used as an identification variable. None of the beads in the actual urn are marked with numbers. The second variable color indicates whether a particular virtual bead is red or white.\nNote that in this section, we used the variable bead_ID to keep track of each bead in our urn, while in the last section we used group_ID to keep track of the samples drawn by the 33 individual teams. This is a better strategy than naming both variables ID, as it would be much more likely for us to get them confused later on.\nOur virtual urn needs a virtual shovel. We use slice_sample() and some list-column mapping wizardry learned in Section 5.3 to take a sample of 50 beads from our virtual urn.\n\n# Define trial_ID as one instance of us sampling 50 beads from the urn. When\n# trial_ID is called within map(), we are performing slice_sample() upon our urn\n# once, and taking a sample of 50 beads. \n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50)))\n\n# A tibble: 1 × 2\n  trial_ID shovel           \n     <dbl> <list>           \n1        1 <tibble [50 × 2]>\n\n\nAs usual, map functions and list-columns are powerful but confusing. The str() function is a good way to explore a tibble with a list-column.\n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  str()\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ trial_ID: num 1\n $ shovel  :List of 1\n  ..$ : tibble [50 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ color  : chr [1:50] \"white\" \"white\" \"white\" \"red\" ...\n  .. ..$ bead_ID: int [1:50] 812 903 227 283 229 160 523 893 66 277 ...\n\n\nThere are two levels. There is one row in the tibble for each sample. So far, we have only drawn one sample. Within each row, there is a second level, the tibble which is the sample. That tibble has two variables: trial_ID and color. This is the advantage to using slice_sample(), because it selects all columns of our urn, whereas sample() can only sample from a single column. While identifying each individual bead may be irrelevant in our urn scenario, with other problems it could be very useful to have additional data about each individual.\nNow let’s add a column which indicates the number of red beads in the sample taken from the shovel.\n\n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  \n  # To count the number of red beads in each shovel, we can use a lesser \n  # known property of the sum() function: By passing in a comparison \n  # expression, sum() will count the number of occurrences within a vector. \n  # In this case, we count the total number occurrences of the word red\n  # in the color column of shovel.\n\n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))\n\n# A tibble: 1 × 3\n  trial_ID shovel            numb_red\n     <dbl> <list>               <int>\n1        1 <tibble [50 × 2]>       20\n\n\nHow does this work? R evaluates if color == red, and treats TRUE values like the number 1 and FALSE values like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of beads where color equals “red”.\nFinally, calculate the proportion red by dividing numb_red (The number of red beads observed in the shovel), by the shovel size (We are using a shovel of size 50).\n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 50)\n\n# A tibble: 1 × 4\n  trial_ID shovel            numb_red prop_red\n     <dbl> <list>               <int>    <dbl>\n1        1 <tibble [50 × 2]>       23     0.46\n\n\nCareful readers will note that the numb_red is changing in each example above. The reason, of course, is that each block re-runs the shovel exercise, and slice_sample will return a random number of red beads. If we wanted the same number in each block, we would need to use set.seed() each time, always providing the same seed each time.\nLet’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n6.2.2 Using the virtual shovel 33 times\nIn our tactile sampling exercise in Section 6.1, we had 33 groups of students use the shovel, yielding 33 samples of size 50 beads. We then used these 33 samples to compute 33 proportions.\nLet’s use our virtual sampling to replicate the tactile sampling activity in a virtual format. We’ll save these results in a data frame called virtual_samples.\n\nset.seed(9)\n\n virtual_samples <- tibble(trial_ID = 1:33) |>\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |>\n    mutate(prop_red = numb_red / 50) \n\nvirtual_samples\n\n# A tibble: 33 × 4\n   trial_ID shovel            numb_red prop_red\n      <int> <list>               <int>    <dbl>\n 1        1 <tibble [50 × 2]>       21     0.42\n 2        2 <tibble [50 × 2]>       19     0.38\n 3        3 <tibble [50 × 2]>       17     0.34\n 4        4 <tibble [50 × 2]>       15     0.3 \n 5        5 <tibble [50 × 2]>       17     0.34\n 6        6 <tibble [50 × 2]>       21     0.42\n 7        7 <tibble [50 × 2]>        9     0.18\n 8        8 <tibble [50 × 2]>       21     0.42\n 9        9 <tibble [50 × 2]>       16     0.32\n10       10 <tibble [50 × 2]>       20     0.4 \n# … with 23 more rows\n\n\nLet’s visualize this variation in a histogram:\n\nvirtual_samples |> \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # To use mathematical symbols in titles and labels, use the expression()\n  # function, as here.\n  \n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")),\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") +\n  \n  # Label the y-axis in an attractive fashion. Without this code, the axis\n  # labels would include 2.5, which makes no sense because all the values are\n  # integers.\n  \n  scale_y_continuous(breaks = seq(2, 10, 2))\n\n\n\n\nSince binwidth = 0.05, this will create bins with boundaries at 0.30, 0.35, 0.45, and so on. Recall that \\(\\hat{\\rho}\\) is equal to the proportion of beads which are red in each sample.\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 45%. Why do we have these differences in proportions red? Because of sampling variation.\nNow we will compare our virtual results with our tactile results from the previous section. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\n\n\n\nComparing 33 virtual and 33 tactile proportions red. Note that, though the figures differ slightly, both are centered around .35 to .45. This shows that, in both sampling distributions, the most frequently occuring proportion red is between 35% and 45%.\n\n\n\n\nThis visualization allows us to see how our results differed between our tactile and virtual urn results. As we can see, there is some variation between our results. This is not a cause for concern, as there is always expected sampling variation between results.\n\n6.2.3 Using the virtual shovel 1,000 times\n\n\n\n\nSo much sampling, so little time.\n\n\n\n\nAlthough we took 33 samples from the urn in the previous section, we should never do that again! The advantage of modern technology is that we can use virtual simulation as many times as we choose, so we have no restrictions on resources. No longer are the days where we have to recruit our friends to tirelessly sample from the physical urn. We are now data scientists! 33 samples are useless to us. Instead, we use our simulations hundreds or thousands of times to create mathematical models that we can combine with our knowledge to answer our questions. In this section we’ll examine the effects of sampling from the urn 1,000 times.\nWe can reuse our code from above, making sure to replace 33 trials with 1,000.\n\nset.seed(9)\n\n virtual_samples <- tibble(trial_ID = 1:1000) |>\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |>\n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) |> \n    mutate(prop_red = numb_red / numb_beads) \n\nNow we have 1,000 values for prop_red, each representing the proportion of 50 beads that are red in a sample. Using the same code as earlier, let’s visualize the distribution of these 1,000 replicates of prop_red in a histogram:\n\nvirtual_samples |> \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 1,000 proportions red\") \n\n\n\n\nWhy the empty spaces among the bars? Recall that, with only 50 beads, there are only 51 possible values for \\(\\hat{\\rho}\\): 0, 0.02, 0.04, …, 0.98, 1. A value of 0.31 or 0.47 is impossible, hence the gaps.\nThe most frequently occurring proportions of red beads occur, again, between 35% and 45%. Every now and then we observe proportions much higher or lower. This occurs because as we increase the number of trials, tails develop on our distribution as we are more likely to witness extreme \\(\\hat{\\rho}\\) values. The symmetric, bell-shaped distribution shown in the histogram is well approximated by the normal distribution.\nNow that we have a good understanding of virtual sampling, we can apply our knowledge to examine the effects of changing our virtual shovel size.\n\n6.2.4 The effect of different shovel sizes\n\n\n\n\nWhat happens if we use different sized shovels to sample?\n\n\n\n\nInstead of just one shovel, imagine we have three choices of shovels to extract a sample of beads with: shovels of size 25, 50, and 100. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes. Start by creating a tibble with 1,000 rows, each row representing an instance of us sampling from the urn with our chosen shovel size. Then, compute the resulting 1,000 replicates of proportion red. Finally, plot the distribution using a histogram.\n\n# Within slice_sample(), n = 25 represents our shovel of size 25. We also divide\n# by 25 to compute the proportion red.\n\nvirtual_samples_25 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 25))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 25)\n\nvirtual_samples_25 |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 25 beads that were red\")), \n       title = \"25\") \n\n\n\n\nWe will repeat this process with a shovel size of 50.\n\nvirtual_samples_50 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 50)\n\n\nvirtual_samples_50  |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 50 beads that were red\")), \n       title = \"50\")  \n\n\n\n\nWe choose a bin width of .04 for all histograms to more easily compare different shovel sizes. Using a smaller bin size would result in gaps between the bars, as a shovel of size 50 has more possible \\(\\hat{\\rho}\\) values than a shovel of size 25.\nFinally, we will perform the same process with 1000 replicates to map the histogram using a shovel size of 100.\n\nvirtual_samples_100 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 100))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 100)\n\n\nvirtual_samples_100 |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of 100 beads that were red\")), \n       title = \"100\") \n\n\n\n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes:\n\n# Use bind_rows to combine the data from our three saved virtual sampling\n# objects. Use mutate() in each to clarify the n as the necessary number\n# of samples taken. This makes our data easier to interpret and prevents\n# duplicate elements.\n\nvirtual_prop <- bind_rows(virtual_samples_25 |> \n                            mutate(n = 25), \n                          virtual_samples_50 |> \n                            mutate(n = 50), \n                          virtual_samples_100 |> \n                            mutate(n = 100))\n\n# Plot our new object with the x-axis showing prop_red. Add elements binwidth,\n# boundary, and color for stylistic clarity. Use labs() to add an x-axis label\n# and title. Facet_wrap() splits the graph into multiple plots by the variable\n# (~n).\n\ncomparing_sampling_distributions <- ggplot(virtual_prop, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, boundary = 0.4, color = \"white\") +\n  labs(x = expression(paste(\"Proportion, \", hat(rho), \", of the beads that were red\")), \n       title = \"Comparing distributions of proportions red for three different shovel sizes.\") +\n  facet_wrap(~ n) \n\n# Inspect our new faceted graph. \n\ncomparing_sampling_distributions\n\n\n\nComparing the distributions of proportion red for different sample sizes (25, 50, 100). The important takeaway is that our center becomes more concentrated as our sample size increases, indicating a smaller standard deviation between our guesses.\n\n\n\n\nObserve that as the sample size increases, the histogram becomes taller and narrower. This is because the variation of the proportion red for each sample decreases. Remember: A large variation means there are a wide range of values that can be achieved, while smaller variations are concentrated around a specific value.\nThe Central Limit Theorem states, more or less, that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both narrower and more bell-shaped. In other words, the sampling distribution increasingly follows a normal distribution and the variation of this sampling distribution gets smaller, meaning smaller standard errors.\nWhy does variation decrease as sample size increases? If we use a large sample size like 100 or 500, our sample is much more representative of the population simply because more of the population is included. As a result, the proportion red in our sample (\\(\\hat{\\rho}\\)) will be closer to the population proportion (\\(\\rho\\)). On the other hand, smaller samples have much more variation because of our old friend chance. We are much more likely to have extreme samples that are not representative of our population.\nLet’s attempt to visualize the concept of variation a different way. For each sample size, let’s plot the proportion red for all 1,000 samples. With 3 different shovel sizes, we will have 3,000 total points, with each point representing an instance of sampling from the urn with a specific shovel size.\n\nvirtual_prop |>\n  ggplot(aes(x = n, y = prop_red, color = as.factor(n))) +\n  geom_jitter(alpha = .15) + \n  labs(title = \"Results of 1,000 samples for 3 different shovel sizes.\",\n       subtitle = \"As shovel size increases, variation decreases.\",\n       y = \"Proportion red in sample\",\n       color = \"Shovel size\") +\n  \n  # We do not need an x axis, because the color of the points denotes the shovel size. \n   \n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\nThis graph illustrates the exact same concept as the histogram. With the smallest shovel size there is significant variance from sample to sample, as samples take on a wide variety of sample proportions! However, as we increase the sample size, the points become more concentrated, or less variance.\nThere is also a third way to understand variation! We can be numerically explicit about the amount of variation in our three sets of 1,000 values of prop_red by using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable. For all three sample sizes, let’s compute the standard deviation of the 1,000 proportions red.\n\n\n\n\n\n\n\nComparing standard deviations of proportions red for three different shovels\n    \n\nNumber of slots in shovel\n      Standard deviation of proportions red\n    \n\n\n25\n0.099\n\n\n50\n0.067\n\n\n100\n0.045\n\n\n\n\nComparing the number of slots in the shovel with the standard deviation of proportions red. Here, we see that standard deviation decreases with higher sample sizes. Larger sample sizes yield more precise estimates.\n\n\n \nAs the sample size increases, the sample to sample variation decreases, and our guesses at the true proportion of the urn’s beads that are red get more precise. The larger the shovel, the more precise the result.\n\n\n\n\nVariance appears everywhere in data science.\n\n\n\n\nLet’s take a step back from all the variance. The reality is that our code needs to be better optimized, as it is bad practice to make a separate tibble for each sample size. To make comparisons easier, let’s attempt to put all 3 shovel sizes in the same tibble using mapping.\n\ntibble(trial_ID = 1:1000) |>\n  mutate(shovel_ID = map(trial_ID, ~c(25, 50, 100))) |>\n  unnest(shovel_ID) |>\n  mutate(samples = map(shovel_ID, ~slice_sample(urn, n = .))) |>\n  mutate(num_red = map_int(samples, ~sum(.$color == \"red\"))) |>\n  mutate(prop_red = num_red/shovel_ID)\n\n# A tibble: 3,000 × 5\n   trial_ID shovel_ID samples            num_red prop_red\n      <int>     <dbl> <list>               <int>    <dbl>\n 1        1        25 <tibble [25 × 2]>        7     0.28\n 2        1        50 <tibble [50 × 2]>       26     0.52\n 3        1       100 <tibble [100 × 2]>      37     0.37\n 4        2        25 <tibble [25 × 2]>        8     0.32\n 5        2        50 <tibble [50 × 2]>       15     0.3 \n 6        2       100 <tibble [100 × 2]>      46     0.46\n 7        3        25 <tibble [25 × 2]>        6     0.24\n 8        3        50 <tibble [50 × 2]>       21     0.42\n 9        3       100 <tibble [100 × 2]>      40     0.4 \n10        4        25 <tibble [25 × 2]>        9     0.36\n# … with 2,990 more rows\n\n\nTo those of us who do not completely understand mapping, do not fret! The tidyr package provides the expand_grid() function as a neat alternative. We can use expand_grid() and a new variable, shovel_size, to create a tibble which will organize our results. Instead of using 1,000 trials, let’s use 3 to get a feel for the function.\n\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100))\n\n# A tibble: 9 × 2\n  trial_ID shovel_size\n     <int>       <dbl>\n1        1          25\n2        1          50\n3        1         100\n4        2          25\n5        2          50\n6        2         100\n7        3          25\n8        3          50\n9        3         100\n\n\nThe above sets the stage for simulating three samples for each of three different shovel sizes. Similar code as above can be used.\n\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red/shovel_size) \n\n# A tibble: 9 × 5\n  trial_ID shovel_size shovel             numb_red prop_red\n     <int>       <dbl> <list>                <int>    <dbl>\n1        1          25 <tibble [25 × 2]>         7     0.28\n2        1          50 <tibble [50 × 2]>        19     0.38\n3        1         100 <tibble [100 × 2]>       39     0.39\n4        2          25 <tibble [25 × 2]>         8     0.32\n5        2          50 <tibble [50 × 2]>        19     0.38\n6        2         100 <tibble [100 × 2]>       34     0.34\n7        3          25 <tibble [25 × 2]>         7     0.28\n8        3          50 <tibble [50 × 2]>        19     0.38\n9        3         100 <tibble [100 × 2]>       36     0.36\n\n\nAgain, we changed the second line to use shovel_size rather than trial_ID as the mapping variable since we can no longer hard code the shovel size into the call to slice_sample(). Expand to 1,000 simulations for each value of shovel_size and finish with a calculation of standard deviation.\n\nexpand_grid(trial_ID = c(1:1000), shovel_size = c(25, 50, 100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red/shovel_size) |>\n  summarize(st_dev_p_hat = sd(prop_red),\n            .by = shovel_size)\n\n# A tibble: 3 × 2\n  shovel_size st_dev_p_hat\n        <dbl>        <dbl>\n1          25       0.0967\n2          50       0.0682\n3         100       0.0453\n\n\nThis is, approximately, the same result as we saw above, but with 1 re-factored tibble instead of 3 separate ones. We can also use functions like expand_grid() in the future to make our code more concise.\nNow that we have this framework, there’s no need to limit ourselves to the sizes 25, 50, and 100. Why not try all integers from 1 to 100? We can use the same code, except we’ll now set shovel_size = 1:100.\n\nshovels_100 <- expand_grid(trial_ID = c(1:1000), shovel_size = c(1:100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / shovel_size) |> \n  summarize(st_dev_p_hat = sd(prop_red),\n            .by = shovel_size)\n\nglimpse(shovels_100)\n\nRows: 100\nColumns: 2\n$ shovel_size  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ st_dev_p_hat <dbl> 0.48436516, 0.34093399, 0.27629253, 0.24069833, 0.2140195…\n\n\nNow, we have the standard deviation of prop_red for all shovel sizes from 1 to 100. Let’s plot that value to see how it changes as the shovel gets larger:\n\n\n\n\nComparing standard deviations of proportions red for 100 different shovels. The standard deviation decreases at the same rate as the square root of shovel size. The red line shows the standard error.\n\n\n\n\nThe red line here represents an important statistical concept: standard error (SE). As the shovel size increases, and thus our sample size increases, we find that the standard error decreases. If this is confusing right now, fear not! We will delve into the explanation of standard error in our next section.\n\n\n\n\nTo any poets and philosophers confused about this: don’t worry! It won’t be on a problem set.\n\n\n\n\nThis is the power of running many analyses at once using map functions and list columns: before, we could tell that the standard deviation was decreasing as the shovel size increased, but when only looking at shovel sizes of 25, 50, and 100, it wasn’t clear how quickly it was decreasing."
  },
  {
    "objectID": "06-one-parameter.html#sec-standard-errors",
    "href": "06-one-parameter.html#sec-standard-errors",
    "title": "6  One Parameter",
    "section": "\n6.3 Standard error",
    "text": "6.3 Standard error\n\n\n\n\nStandard errors are just the way old people talk about confidence intervals.\n\n\n\n\nStandard errors (SE) quantify the effect of sampling variation on our estimates. In other words, they quantify how much we can expect the calculated proportions of a shovel’s beads that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nThe standard error is the standard deviation of a sample statistic (aka point estimate), such as the proportion. For example, the standard error of the mean refers to the standard deviation of the distribution of sample means taken from a population.\nThe relationship between the standard error and the standard deviation is that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the square root of the sample size. The larger the sample size, the smaller the standard error.\nIf this sounds confusing, don’t worry! It is. Before we can explain this in more depth, it is important to understand some terminology.\n\n6.3.1 Terminology and notation\n\n\n\n\nLet Yoda’s wisdom dull the pain of this terminology section.\n\n\n\n\nAll of the concepts underlying this terminology, notation, and definitions tie directly to the concepts underlying our tactile and virtual sampling activities. It will simply take time and practice to master them.\nFirst, a population is the set of relevant units. The population’s size is upper-case \\(N\\). In our sampling activities, the population is the collection of \\(N\\) = 1,000 identically sized red and white beads in the urn. This is about the simplest possible population. Other examples are all the adult men in the US, all the classrooms in a school, all the wheelbarrows in Massachusetts, all the values of your blood pressure, read at five minute intervals, for your entire life. Often, the population is extends over time, as with your blood pressure readings and is, therefore, more amorphous. Consider all the people who have run for governor of a US state since 1900, or all the people who will run for governor through 2050. Those are also populations.\nSecond, a population parameter is a numerical summary quantity about the population that is unknown, but you wish you knew. For example, when this quantity is the mean, the population parameter of interest is the population mean. This is mathematically denoted with the Greek letter \\(\\mu\\) pronounced “mu.” In our earlier sampling from the urn activity, however, since we were interested in the proportion of the urn’s beads that were red, the population parameter is the population proportion, denoted with \\(\\rho\\).\nThird, a census is an exhaustive enumeration or counting of all \\(N\\) units in the population in order to compute the population parameter’s value exactly. In our sampling activity, this would correspond to counting the number of beads out of \\(N = 1000\\) that are red and computing the population proportion \\(\\rho\\) that are red exactly. When the number \\(N\\) of individuals or observations in our population is large as was the case with our urn, a census can be quite expensive in terms of time, energy, and money. A census is impossible for any populations which includes the future, like our blood pressure next year or candidates for governor in 2040. There is a truth but we could not, even in theory, calculate it.\nFourth, sampling is the act of collecting a sample from the population when we can not, or do not want to, perform a census. The sample size is lower case \\(n\\), as opposed to upper case \\(N\\) for the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). In our sampling activities, we used shovels with varying slots to extract samples of size \\(n\\) = 1 through \\(n\\) = 100.\nFifth, a point estimate, also known as a sample statistic, is a measure computed from a sample that estimates an unknown population parameter. In our sampling activities, recall that the unknown population parameter was the proportion of red beads and that this is mathematically denoted with \\(\\rho\\). Our point estimate is the sample proportion: the proportion of the shovel’s beads that are red. In other words, it is our guess at the proportion of the urn’s beads that are red. The point estimate of the parameter \\(\\rho\\) is \\(\\hat{\\rho}\\). The “hat” on top of the \\(\\rho\\) indicates that it is an estimate of the unknown population proportion \\(\\rho\\).\n\nSixth, a sample is said to be representative if it roughly looks like the population. In other words, are the sample’s characteristics a good representation of the population’s characteristics? In our sampling activity, are the samples of \\(n\\) beads extracted using our shovels representative of the urn’s \\(N\\) = 1000 beads?\nSeventh, a sample is generalizable if any results based on the sample can generalize to the population. In our sampling activity, can we generalize the sample proportion from our shovels to the entire urn? Using our mathematical notation, this is akin to asking if \\(\\hat{\\rho}\\) is a “good guess” of \\(\\rho\\)?\nEighth, biased sampling occurs if certain individuals or observations in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every observation in a population had an equal chance of being sampled. Had the red beads been much smaller than the white beads, and therefore more prone to falling out of the shovel, our sample would have been biased. In our sampling activities, since we mixed all \\(N = 1000\\) beads prior to each group’s sampling and since each of the equally sized beads had an equal chance of being sampled, our samples were unbiased.\nNinth, a sampling procedure is random if we sample randomly from the population in an unbiased fashion. In our sampling activities, this would correspond to sufficiently mixing the urn before each use of the shovel.\n\n\n\n\nFear not if you look like Spongebob after reading this section. We will re-cap right now!\n\n\n\n\nIn general:\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can draw inferences about the population using sampling.\n\nSpecific to our sampling activity:\n\nIf we extract a sample of \\(n=50\\) beads at random, in other words, we mix all of the equally sized beads before using the shovel, then\nthe contents of the shovel are an unbiased representation of the contents of the urn’s 1000 beads, thus\nany result based on the shovel’s beads can generalize to the urn, thus\nthe sample proportion \\(\\hat{\\rho}\\) of the \\(n=50\\) beads in the shovel that are red is a “good guess” of the population proportion \\(\\rho\\) of the \\(N=1000\\) beads that are red, thus\ninstead of manually going over all 1,000 beads in the urn, we can make inferences about the urn by using the results from the shovel.\n\n6.3.2 Statistical definitions\nNow, for some important statistical definitions related to sampling. As a refresher of our 1,000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section 6.2, let’s display our figure showing the difference in proportions red according to different shovel sizes.\n\n\n\n\nPreviously seen three distributions of the sample proportion \\(\\hat{ ho}\\).\n\n\n\n\nThese types of distributions have a special name: sampling distributions; their visualization displays the effect of sampling variation on the distribution of a point estimate; in this case, the sample proportion \\(\\hat{\\rho}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we typically expect.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe that while we are somewhat likely to observe sample proportions of red beads of \\(0.2 = 20\\%\\) when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions, seeing that the standard deviation decreases with the square root of the sample size:\n\n\n\n\nPreviously seen comparing standard deviations of proportions red for 100 different shovels\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red beads decreases. This type of standard deviation has another special name: standard error\n\n6.3.3 What is a “standard error”?\nThe “standard error” (SE) is a term that measures the accuracy with which a sample distribution represents a population through the use of standard deviation. Specifically, SE is used to refer to the standard deviation of a sample statistic (aka point estimate), such as the mean or median. For example, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\n\nIn statistics, a sample mean deviates from the actual mean of a population; this deviation is the standard error of the mean.\n\nMany students struggle to differentiate the standard error from the standard deviation. The relationship between the standard error and the standard deviation is such that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the sample size; the larger the sample size, the smaller the standard error because the statistic will approach the actual value.\n\nThe more data points involved in the calculations of the mean, the smaller the standard error tends to be. When the standard error is small, the data is said to be more representative of the true mean. In cases where the standard error is large, the data may have some notable irregularities. Thus, larger sample size = smaller standard error = more representative of the truth.\nTo help reinforce these concepts, let’s re-display our previous figure but using our new sampling terminology, notation, and definitions:\n\n\n\n\nThree sampling distributions of the sample proportion \\(\\hat{ ho}\\). Note the increased concentration on the bins around .4 as our sample size increases.\n\n\n\n\nFurthermore, let’s display the graph of standard errors for \\(n = 1\\) to \\(n = 100\\) using our new terminology, notation, and definitions relating to sampling.\n\n\n\n\nStandard errors of the sample proportion based on sample sizes of 1 to 100\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n6.3.4 The moral of the story\nIf we could only know two pieces of information from our data, what would they be? First, you need a measure of the center of the distribution. This would include the mean or median, which shows the center of our data points. Second, we need a measure of the variability of the distribution. To understand our center, we must understand how different (or how spread) our data points are from one another. Thus, we need a measure like sd() or MAD. These are summary statistics which are necessary to understanding a distribution. Do those two figures encompass all you need to know about a distribution? No! But, if you are only allowed two numbers to keep, those are the most valuable.\n\nThe mean or median is a good estimate for the center of the posterior and the standard error or mad is a good estimate for the variability of the posterior, with +/- 2 standard errors covering 95% of the outcomes.\n\nThe standard error measures the accuracy of a sample distribution as compared to the population by using the standard deviation. Specifically, the standard deviation of our data points divided by the square root of the sample size. As such, we find that larger sample sizes = lower standard errors = more accurate and representative guesses.\nTo really drive home our point: standard error is just a fancy term for your uncertainty about something you don’t know. Standard error == our (uncertain) beliefs.\n\n\n\n\nIf you are wondering how much you need to know, follow this helpful guide of the information we have learned this chapter!\n\n\n\n\nThis hierarchy represents the knowledge we need to understand standard error (SE). At the bottom, we have math. It’s the foundation for our understanding, but it doesn’t need to be what we take away from this lesson. As we go up, we simplify the topic. The top of the pyramid are the basic levels of understanding that will help you to remember in the future.\nIf I know your estimate plus or minus two standard errors, I know your 95% confidence interval. This is valuable information. Standard error is really just a measure for how uncertain we are about something we do not know, the thing we are estimating. When we recall SE, we should remember that, all in all, it’s a complicated concept that can be distilled into: the way old people talk about confidence intervals.\n\nRecall that \\(\\hat{\\rho}\\) is the estimated value of p which comes from taking a sample. There can be billions and billions of \\(\\hat{\\rho}\\)’s. We look at a large group of \\(\\hat{\\rho}\\)’s, create a distribution of results to represent the possible values of p based on our findings, and then we compute a standard error to account for our own uncertainty about our predictions. Our 95% confidence interval for our prediction == our estimate plus or minus two standard errors.\nIn regards to the fifth layer of the hierarchy, we may wonder:\n\n“I thought that MADs were the same thing as standard deviations. Now you say they are the same things as standard errors. Which is it?”\n\nMADs and standard deviations are, more or less, the same thing. They are both measures of the variability of a distribution. In most cases, they have very similar values. A standard error is also a standard deviation. Specifically, it is the standard deviation of the distribution of the estimates, and that distribution of estimates is, more or less, your posterior. Therefore, we can use MAD, like standard error, to describe that distribution and the variability of that distribution."
  },
  {
    "objectID": "06-one-parameter.html#cardinal-virtues",
    "href": "06-one-parameter.html#cardinal-virtues",
    "title": "6  One Parameter",
    "section": "\n6.4 Cardinal Virtues",
    "text": "6.4 Cardinal Virtues\nRecall the questions we asked at the beginning of this Chapter:\n\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\n\n\nWhat is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\n\nUse the Cardinal Virtues to guide your thinking.\n\n6.4.1 Wisdom\n\n\n\n\n\n\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of “validity,” as to whether or not we can (reasonably!) assume that the two come from the same population.\n\n\n\n\nWhen you see another Precetor Table section.\n\n\n\n\nA Preceptor Table is a table with rows and columns, such that, if no data is missing, we can easily answer our questions.\n\n\n\n\n\n\n\nPreceptor Table\n    \n\nID\n      Color\n    \n\n\n1\nwhite\n\n\n2\nwhite\n\n\n...\n...\n\n\n200\nred\n\n\n201\nwhite\n\n\n...\n...\n\n\n2078\nred\n\n\n2079\nwhite\n\n\n...\n...\n\n\n\n\n\n\n \nNote that the beads do not have ID numbers printed on them. The numbering is arbitrary. Having an ID just reminds us that there are actual units under consideration, even if we can not tell them apart, other than by color. We also include the ID to help visualize the fact that we don’t know the total number of beads in the urn, because our question never tells us! There could be 1,000 beads like our physical urn from earlier, or there could be a million beads. The ellipse at the bottom of the Preceptor Table denotes our uncertainty regarding urn size.\nThere is only one outcome column, “Color,” because this is not a causal model, for which we need to have (at least) two potential outcomes. Predictive models require only one outcome.\nIf we know the color of every bead, then calculating the proportion of beads which are red, \\(\\rho\\), is simple algebra. Once we know \\(\\rho\\) we can simulate the answers to other questions.\n\nThe data we have, unfortunately, only provides the color for 50 beads.\n\n\n\n\n\n\n\nData from Shovel\n    \n\nID\n      Color\n    \n\n\n2\nwhite\n\n\n...\n...\n\n\n200\nred\n\n\n...\n...\n\n\n2079\nwhite\n\n\n...\n...\n\n\n\n\n\n\n \nAgain, there are, in truth, no ID numbers. But keeping track of which beads were in the sample and which beads were not is helpful.\nThe last step of Wisdom is to decide whether or not we can consider the units from the Preceptor Table and the units from the data to have been drawn from the same population. In this case, as with many sampling scenarios, it is trivial that we may make this assumption. If all the rows from the data are also rows in the Preceptor Table, we may assume that they are drawn from the same distribution.\n\nWe also consider why some beads do get sampled, while others do not. This is a consequence of the sampling mechanism. We hope that all members of the population have the same chance of being sampled, or else our data might be biased. Almost all samples have some bias, but we must make a judgement call to see if the data we have is close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population. Our sample of 50 beads is taken from a mixed urn, so hopefully there is a near equal chance of selecting each bead, and our samples are representative of the population.\nValidity involves the columns of our data set. Is the meaning of our columns consistent across the different data sources? In our urn scenario, does bead color in our sampled data and bead color in our Preceptor Table mean the same thing? The answer is yes, and validity can be assumed very easily. If some beads in the urn are old and the color has worn off, it may be more difficult to conclude whether the color column always means the same thing.\n\n\n\n\n6.4.2 Justice\n\n\n\n\n\nJustice starts with an examination of the stability, representativeness, and unconfoundedness of the Population Table and ends with an assumption about the mathematical form of the data generating mechanism.\n\n6.4.2.1 Population Table\nWe use The Population Table to acknowledge the wider source from which we could have collected our data.\nIt includes rows from three sources: the data for units we want to have (the Preceptor Table), the data for units which we have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\n\n\n\n\n\n\n\n\n\nSource\n      Location\n      Time\n      ID\n      Color\n    \n\n\n\nOther\n\n\nKnown, specific urn\n\n\nTime of sample - 2 years\n\n\n1\n\n\n?\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nTime of sample - 3 weeks\n\n\n200\n\n\n?\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nTime of sample - 10 days\n\n\n976\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nData\n\n\nKnown, specific urn\n\n\nTime of sample\n\n\n2\n\n\nwhite\n\n\n\n\nData\n\n\nKnown, specific urn\n\n\nTime of sample\n\n\n200\n\n\nred\n\n\n\n\nData\n\n\nKnown, specific urn\n\n\nTime of sample\n\n\n1080\n\n\nwhite\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nTime of sample + 10 days\n\n\n1\n\n\n?\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nTime of sample + 3 weeks\n\n\n200\n\n\n?\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nTime of sample + 2 years\n\n\n2025\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPreceptor Table\n\n\nKnown, specific urn\n\n\nNow\n\n\n1\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nKnown, specific urn\n\n\nNow\n\n\n200\n\n\nred\n\n\n\n\nPreceptor Table\n\n\nKnown, specific urn\n\n\nNow\n\n\n2078\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nNow + 10 days\n\n\n1\n\n\n?\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nNow + 3 weeks\n\n\n200\n\n\n?\n\n\n\n\nOther\n\n\nKnown, specific urn\n\n\nNow + 2 years\n\n\n2300\n\n\n?\n\n\n\n\n\n\n\n \nEach specific row represents one subject, which are individual beads in our urn scenario. Because there could be thousands or even millions of beads, we provide 3 examples for each category, and use ellipses to denote that there are many more subjects that we have yet to record.\nEach Population Table will have a minimum of 3 types of columns: covariates, time, and outcome(s):\nWhen we construct a Preceptor Table to answer our question, we must select some covariates that we want all of our subjects to have. Our urn scenario only has no covariates, however, so we will explore this issue in later chapters.\n\nBecause we draw our sample from the exact same urn our question asks us about, the data we collect comes directly from the Preceptor Table, all subjects in our population have the same location (“Known, specific urn”). The Preceptor Table and Population categories are essentially identical. This is the perfect scenario for us, but this rarely occurs in real life.\n\nPopulation Tables always have a column for Time. When answering a question we must specify the time that it’s being asked, because stuff happens and things change.\n\nWe must acknowledge that the sample from the urn could have been taken at any time, so the contents of the urn in the past (our data) could be different from the contents of the urn when we want to answer our question now (the Preceptor Table). As such, there is a wider population we could have collected our data from: any time before collecting the sample, or anytime after collecting it.\n\nFinally, Population Tables have an outcome. Sometimes there will be multiple outcome columns, like in the case of casual models where we want the values for two or more potential outcomes.\n\n6.4.2.2 Stability and representativeness\nNow that we have created our Population Table, we can analyze the key assumptions of stability, representativeness and unconfoundedness.\nStability involves time. Is the model — meaning both the mathematical formula and the value of the parameters — stable over time? Realistically, an urn will be the same today, tomorrow or next year. However, what if someone dumps some red beads into the urn after we take our sample? Then we cannot assume stability, because the proportion of red beads in the urn, \\(\\rho\\), the instant before the dump is different than the proportion red in the urn after. We will assume no one is tampering with our urn, and assert stability across time periods.\nRepresentativeness involves the data rows, specifically the rows for which we have data versus the rows for which we might have had data. Are the rows that we do have data for representative of the rows for which we do not have data? For the sample proportion to be similar to the actual population proportion, we ideally want the data we have to be a random, unbiased selection from our population, using a considerable sample size. In the context of our problem, the sampling mechanism of using a shovel of size 50 to sample beads from an urn in which the beads are thoroughly mixed should be enough to consider our sample representative of the population, and we can move on.\nUnconfoundedness involves the potential correlation between treatment assignment and other variables. It is only a concern for causal models. Since this is a predictive model, we do not have to worry about unconfoundedness. There is no “treatment” which might be confounded with anything.\n\n6.4.2.3 The DGM\nThe final aspect of Justice is assuming/creating a mathematical formula — a data generating mechanism, or DGM — which mimics the process by which the data comes to us. The DGM for sampling scenarios with only two possible values is often denoted as follows:\n\\[ T_i  \\sim B(\\rho, n = 50) \\]\nThe total number of red beads selected in a sample of 50 beads, \\(T_i\\), is equal to one draw from a binomial distribution with \\(n = 50\\) observations and an unknown probability \\(\\rho\\) of the proportion of red beads in the urn.\n\n6.4.3 Courage\n\n\n\n\n\nCourage takes the mathematical formula from Justice and, using code, creates a fitted model, included posterior probability distributions for all the unknown parameters.\n\n6.4.3.1 Bayesian framework\nWe are Bayesian statisticians who make Bayesian models. This means that we make specific assumptions and consider data to be fixed and parameters to be variable. One of the most important distinctions is that in Bayesian data science, we don’t know the values of our parameters. Consider the DGM created in Justice:\n\\[ T_i  \\sim B(\\rho, n = 50) \\]\n\nSome non-Bayesian frameworks are concerned with the probability distribution of our observed data, but do not care much about the probability distribution for \\(\\rho\\) and assume it to be fixed. If \\(\\rho\\) is fixed, the equation above becomes one simple binomial distribution. Think of this as a standard 2 dimensional plot.\nWe Bayesians consider our observed data to be fixed. We don’t consider alternate realities where our observed data is different due to sampling variation. Instead, we are concerned with the probability distribution of our parameter. In our urn scenario, \\(\\rho\\) is variable, so we have to create a separate binomial distribution for each possible value of \\(\\rho\\). Think of this as a 3 dimensional joint distribution, as we created in Section 5.6.\nIt is essential to understand the joint distribution and the posterior, two concepts Bayesians use to solve problems. We will provide quick a quick review here, including statistical notation that may be helpful to some.\n\nThe joint distribution, \\(p(y|\\theta)\\), models the outcome \\(y\\) given one or more unknown parameter(s), \\(\\theta\\). The equation illustrates exact same concept we addressed while discussing the distinctions of Bayesian science: because our parameters are variable, we have to create separate distributions for each potential value. Combining all these distributions together creates a joint distribution that is 3 dimensional when plotted.\nThe posterior, \\(p(\\theta|y)\\), is the probability distribution of our parameter(s) \\(\\theta\\), created using data \\(y\\) that updates our beliefs. We have referenced the posterior many times before, and this definition does not change its meaning.\nIn our urn scenario, obtaining the posterior involves first creating many binomial distributions for each possible population proportion. This is the joint distribution, and it is a 3 dimensional model. We then select the distribution that corresponds with our data: 17 red beads are sampled. We can represent the posterior with the following:\n\\[\\text{Prob}(\\text{models} | \\text{data} = 17)\\]\nThis is equivalent to taking a 2 dimensional slice of the 3 dimensional model. We are left with a probability distribution for our parameter, \\(\\rho\\).\n\n6.4.3.2 stan_glm()\nGiven a data set to use and a mathematical formula to work with, the next step is to write some code. We will use the rstanarm package, which provides a user friendly interface to work with the statistical language Stan.\nrstanarm and Stan are appealing because they are powerful. Functions such as stan_glm() can do everything we did by hand in Chapter 5 in a few lines of code. Because we will use a professional statistical library, the objects we make will become more complex. In this Chapter, we provide the steps for answering our questions. Chapter 7 will provide a more detailed explanation of the objects we will make. To be clear, you do not need to fully understand this section or how this code works. This is an introduction, not a formal lesson.\n\nlibrary(rstanarm)\n\nfit_1 <- stan_glm(formula = red ~ 1, \n                  data = tibble(red = c(rep(1, 17), \n                                        rep(0, 33))),\n                  family = binomial,\n                  refresh = 0,\n                  seed = 10) \n\nRecall that we assumed a binomial model for the data generating mechanism. In stan_glm() we denote this with family = binomial. In addition to the type of the distribution, we also need to analyze the outcome and predictor variables involved. The outcome is the quantity we are measuring, in this case the total number of red beads in our sample. Because we have no predictors, we use the argument formula = red ~ 1, which means that we only model the outcome based on the unknown proportion of red beads in the urn, \\(\\rho\\).\nWe pass in data in a binomial format: the 1’s represent the number of successes (red beads drawn), and the 0’s represent the number of failures (white beads drawn). As such, we pass a tibble with 17 red beads and 33 white beads into data.\nWe use refresh = 0 to suppress the behavior of printing to the console, and seed = 10 so that we get the same output every time we run the code. The resulting model is:\n\nfit_1\n\nstan_glm\n family:       binomial [logit]\n formula:      red ~ 1\n observations: 50\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) -0.7    0.3  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWe will learn the meaning of this output in Chapter 7. Once we have the fit_1 object, it is easy to answer two sorts of questions: the posterior probability distribution for \\(\\rho\\) and predictions for new draws from the urn. The key functions are posterior_epred() for the former and posterior_predict() for the latter.\nLet’s create our posterior for \\(\\rho\\) by using posterior_epred():\n\nppd_for_p <- posterior_epred(fit_1, \n                newdata = tibble(.rows = 1)) |> \n  as_tibble() |>\n  rename(p = `1`)\n  \n# posterior_epred() will unhelpfully name the column of our tibble to \"1\". We\n# have two options: either refer to the column name as `1`, or rename the column\n# to make it less confusing. We will rename the column to \"p\" in this chapter, but you\n# will oftentimes see `1` in later chapters.\n\nppd_for_p\n\n# A tibble: 4,000 × 1\n       p\n   <dbl>\n 1 0.176\n 2 0.165\n 3 0.173\n 4 0.246\n 5 0.258\n 6 0.258\n 7 0.243\n 8 0.242\n 9 0.307\n10 0.300\n# … with 3,990 more rows\n\n\nPlot the result:\n\nppd_for_p |> \n  ggplot(aes(x = p)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Distribution is centered at .34\",\n         x = \"Proportion p of Red Beads in Urn\",\n         y = \"Probability\") + \n  \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nWe have successfully created the posterior distribution and can finally answer the question we started the chapter with:\n\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\n\nLook to the posterior probability distribution we have created. We can see that the bulk of the area under the posterior occurs approximately when \\(\\rho\\) is between .28 and .42, so the answer to our question is that it is likely that 28% to 42% of the beads in the urn are red. Is that specific range the truth? No! We are just eye-balling the distribution, giving a rough sense of the likely value of \\(\\rho\\). Any range that is large enough to acknowledge the uncertainty we have regarding the exact value of \\(\\rho\\) is acceptable.\nAlthough the most likely probability (the highest bar on the histogram) occurs when \\(\\rho\\) is around .34, The answer is not a single number. Our posterior distribution is just that: a distribution. Using our data, we have many different results for the proportion of red beads in the entire urn. Certain proportions, like the extremes close to 0% or 100%, are essentially impossible due to our sample value being 34%. On the other hand, we could have just as easily sampled 16 or 18 beads from the urn, and sample proportions such as 32% and 36% are very plausible.\nThis means that, while we can provide a range of possibilities (and we can estimate which of those possibilities occur most frequently), we can never say that we know the total number of red beads with certainty. We know that there is the most chance that \\(\\rho\\) is between .28 and about .42, some chance that \\(\\rho\\) is between .15 and .24 or between .42 and .56, and almost no chance that \\(\\rho\\) is below .15 or above .56. With the posterior we can visualize all of these probabilities at once.\nAnother important question remains:\n\nWhy are there 4,000 rows in the stan_glm() tibble?\n\nBy default, stan_glm() will sample from the posterior in 2 sets of 2,000 iterations. If needed we can change the default number of iterations using the iter argument, but there are few reasons to do so. Some of us may still want to know why we sample from the posterior in the first place. Why not use the entire posterior? The answer is that the posterior is a theoretical beast, which makes it difficult to work with.\nFor example, what if we wanted to know the probability that \\(\\rho\\) is between .3 and .4? To answer this using the pure posterior, we would need to calculate the area under the distribution from when \\(.3 < \\rho < .4\\). This is more difficult then it seems, as the posterior is a distribution, so it has no individual observations to work with as it’s continuous!\nInstead, we can work with draws from the posterior. With enough draws we create a close approximation of the posterior which models the counts of our observations. This is an approximation; it is not exactly the posterior, but close enough for our purposes. We can easily convert our posterior distribution into a posterior probability distribution, by making the area under the graph sum to 1. The posterior probability distribution is often used as a visual aid, as percentages are more easy to conceptualize than raw numbers. One way to convert a posterior distribution into a probability distribution is to group by each value of \\(\\rho\\) and turn the counts into probabilities:\n\nppd_for_p |>\n  round(digits = 2) |>\n  summarize(prob = n()/nrow(ppd_for_p),\n            .by = p) |>\n  arrange(desc(prob))\n\n# A tibble: 46 × 2\n       p   prob\n   <dbl>  <dbl>\n 1  0.35 0.0615\n 2  0.32 0.0605\n 3  0.36 0.0588\n 4  0.34 0.0585\n 5  0.33 0.058 \n 6  0.31 0.054 \n 7  0.37 0.0535\n 8  0.39 0.047 \n 9  0.3  0.0462\n10  0.38 0.0442\n# … with 36 more rows\n\n\nWe can also accomplish a similar effect by passing aes(y = after_stat(count/sum(count)) into geom_histogram() when plotting. Oftentimes, like in answering the probability that \\(\\rho\\) is between .3 and .4, we can work with the posterior distribution to the very end. Just divide the number of draws that meet our condition (are between .3 and .4), by the total number of draws.\n\nsum(ppd_for_p$p > .3 & ppd_for_p$p < .4)/nrow(ppd_for_p)\n\n[1] 0.5445\n\n\nThere is approximately a 54% chance that \\(\\rho\\) is between .3 and .4. Give me enough draws from the posterior and I can show you the world!\n\n6.4.4 Temperance\n\n\n\n\n\nWith the fitted model object fit_1, we can now make predictions into the future and answer our questions.\nRecall the second question we started with:\n\nWhat is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\n\n\n6.4.4.1 Using the posterior\nWhenever someone asks you a question, you need to decide what posterior probability distribution would make it easy for you to answer that question. In this case, if we know the posterior probability distribution for the number of red beads in a shovel of size 20, then a question about the likelihood of drawing more than 8 (or any other value) is easy to answer.\nThe posterior probability distribution for a probability is a tricky thing. It is much easier just to estimate the posterior probability distribution for the outcome — number of red beads out of 20 — and then work with that distribution in order to answer probability-type questions.\nTo predict these future unknown samples, we use posterior_predict(). We pass the posterior created using stan_glm() as the first argument, and because we want to estimate the number of red draws with a shovel size of 20, we use pass a tibble with 20 rows into newdata.\n\nposterior_predict(fit_1, \n                  newdata = tibble(.rows = 20)) |> \n  as_tibble()\n\n# A tibble: 4,000 × 20\n     `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`  `13`\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     0     0     0     0     1     0     0     0     1     0     0     0     1\n 2     0     0     0     0     1     0     1     0     0     0     1     0     0\n 3     0     1     0     0     0     0     0     0     0     0     1     0     0\n 4     0     1     0     1     1     0     0     1     0     0     0     0     1\n 5     0     0     0     1     0     0     1     0     0     0     0     1     0\n 6     0     0     0     0     0     1     1     0     0     0     0     0     1\n 7     0     1     1     1     0     0     1     1     0     0     0     0     1\n 8     1     1     1     1     0     0     1     0     1     1     0     0     0\n 9     1     0     1     1     0     0     0     0     0     1     0     1     1\n10     0     0     1     1     0     0     0     0     0     0     0     0     0\n# … with 3,990 more rows, and 7 more variables: `14` <int>, `15` <int>,\n#   `16` <int>, `17` <int>, `18` <int>, `19` <int>, `20` <int>\n\n\nEach of our 4,000 rows represent one instance of us predicting a future sample from the urn, and each column represents the color bead in a shovel slot. We will create a new column called total, using rowwise() with c_across() to calculate the total number of red beads drawn in the sample. Finally, we will graph the resulting distribution.\n\nppd_reds_in_20 <- posterior_predict(fit_1, \n                  newdata = tibble(.rows = 20)) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(total = sum(c_across(`1`:`20`))) |> \n  select(total)\n\n\nppd_reds_in_20   |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-slot shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\") +  \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nWe have successfully created the posterior probability distribution for the number of red beads drawn in a shovel of size 20. But before we answer our question, some of us may be wondering why we made our predictions using posterior_predict() instead of posterior_epred(). Let’s examine what happens if we use posterior_epred() instead.\n\npost_epred <- posterior_epred(fit_1, \n                  newdata = tibble(.rows = 20)) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(total = sum(c_across(`1`:`20`))) |> \n  select(total)\n\npost_epred  |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior probability distribution using posterior_epred()\",\n         subtitle = \"In our scenario, using posterior_epred() is incorrect\",\n         x = \"Number of red beads\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\nUsing posterior_epred()\n\n\n\n\nWhat happened? posterior_epred() shows the distribution of the entire population, which is continuous. The expected predictions can be fractional, because posterior_epred() returns draws from the posterior (which can be fractional) contingent on some covariate. In our scenario we have no covariates from which to create expected predictions, so posterior_epred() just returns the posterior, but re-scaled to between 0 and 20 beads instead of between 0 and 1 as before. The shape of the distributions are identical:\n\npost_epred |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n  labs(x = \"Number of red beads sampled out of 20\",\n       y = \"Probability\") + \n  ppd_for_p  |>\n  ggplot(aes(x = p)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  labs(x = \"Proportion red in urn\",\n       y = \"Probability\") + \n  plot_annotation(title = 'Expected prediction for sample of size 20 on left, posterior on right.',\n                  subtitle = \"The two distributions have an identical shape.\")\n\n\n\n\nOn the other hand, posterior_predict() models the posterior distribution for future individuals. In our scenario, we model the binomial distribution of a discrete random variable. The bars only appear at real numbers between 1 and 16, because we are predicting the probability of individual samples. We cannot draw fractions of beads in our sample. Using posterior_predict() essentially replicates the DGM, taking many virtual draws from our urn and summarizing all the results.\nIn summary, use posterior_predict() when to predict the outcome of individual(s) in the future, and use posterior_epred() to model the probability across the entire population using the posterior. To answer our question, we want to know the probability of outcomes using a single shovel of size 20. We should use posterior_predict() to model taking individual samples many times, and we can then analyze the probabilities. If this is confusing do not fret! We will have plenty of practice with these 2 functions for the remainder of this Primer.\nNow let’s attempt to actually answer our question:\n\nWhat is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\n\nBecause posterior_predict() takes predictive draws for us, we can simply count the number of draws that have more than 8 red beads, and divide by the total number of draws.\n\n# Same code as earlier, included as a refresher. \n\nppd_reds_in_20 <- posterior_predict(fit_1, \n                  newdata = tibble(.rows = 20)) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(total = sum(c_across(`1`:`20`))) |> \n  select(total)\n\n# Calculating the probability\n\nsum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total)\n\n[1] 0.24925\n\n\nThere is approximately a 25% chance that we will draw more than 8 red beads out of a sample size of 20.\nTo visualize this probability graphically, we will reuse our posterior, and add a new column called above_eight that is TRUE if total > 8.\n\nppd_reds_in_20 <- posterior_predict(fit_1, \n                    newdata = tibble(.rows = 20)) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(total = sum(c_across(`1`:`20`))) |>\n  select(total) |>\n  mutate(above_eight = ifelse(total > 8, TRUE, FALSE))\n\nppd_reds_in_20\n\n# A tibble: 4,000 × 2\n# Rowwise: \n   total above_eight\n   <int> <lgl>      \n 1     4 FALSE      \n 2     3 FALSE      \n 3     5 FALSE      \n 4     6 FALSE      \n 5     3 FALSE      \n 6     4 FALSE      \n 7     6 FALSE      \n 8     3 FALSE      \n 9     7 FALSE      \n10     6 FALSE      \n# … with 3,990 more rows\n\n\nWe can then can set the fill of our histogram to when above_eight == TRUE to visualize the probability of drawing more than 8 red beads.\n\nppd_reds_in_20   |> \n  \n  # Set fill as above_eight. \n  \n  ggplot(aes(x = total, fill = above_eight)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  \n  # Scale_fill_manual()  is calling grey for the first color and red for the\n  # second color. This is going to highlight the portion of the curve that we\n  # want to highlight in red.\n\n  scale_fill_manual(values = c('grey50', 'red')) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-slot shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\",\n         fill = \"More than 8 Red Beads Drawn?\") +  \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nThe red bars illustrate the area under a specific section of the curve, as compared to the entire area under the curve. Each question requires looking at a new area under the curve. When someones asks you a question, they are doing two things. First, they are providing instructions as to the posterior your should create. Here, the results with a shovel of 20 slots. Second, they are asking a question about the area under the curve in a specific region. Here, the region where the number of red beads is greater than 8 is highlighted in red. Therefore, the area below the curve that is red is how we get our estimate.\nSee Chapter 7 for a thorough discussion of the use of rstanarm. This package will be our main tool for the rest of the Primer.\n\n6.4.4.2 Assesing uncertainty\nThe process Cardinal Virtues process may seem tedious, but we use them for good reason. In Wisdom we determined what data we need to answer our question, and in Justice we analyzed whether or not the data we have can answer our question. If we had skipped these steps, we would be at risk of creating models with “bad data” in Courage and Temperance, and draw misleading conclusions.\nWe have successfully answered our questions using the Cardinal Virtues, but we must also realize that there are limitations to our models. Earlier we claimed there is a 26% chance that we will draw more than 8 red beads out of a sample size of 20. However, we must be modest about our claims. Is this question about this particular urn, or all urns? Clearly, as our predictions are based upon the posterior from our specific urn, our conclusions can only be applied to this particular urn; our conclusions are not generalizable to all urns.\nFurthermore, how certain are we in our prediction from Temperance that 26% of samples out of a shovel size of 20 using our urn would produce more than 8 red beads?\nFirst let’s model our situation:\n\\[ T_i  \\sim B(\\rho, n = 20) \\]\nThe total number of red beads selected in a sample of 20 beads, \\(T_i\\), is modeled by a binomial distribution with \\(n = 20\\) observations and an unknown probability \\(\\rho\\) of the proportion of red beads in the urn.\nWe must realize that our 26% estimate is based on an entire posterior. We are uncertain about the exact value of \\(\\rho\\), so posterior_predict() takes our uncertainty into account when creating the model. However, there is in fact some actual value for \\(\\rho\\), even if we don’t know it.\nIf the actual value of \\(\\rho\\) was 30%, we can calculate the correct answer to our question using the binomial probability distribution function:\n\npbinom(q = 8, size = 20, prob = .3, lower.tail = FALSE)\n\n[1] 0.1133315\n\n\nOr maybe the actual value of \\(\\rho\\) is 40%:\n\npbinom(q = 8, size = 20, prob = .4, lower.tail = FALSE)\n\n[1] 0.4044013\n\n\nSadly, the answer to our question could be 11%, 40%, a value in between, or even a more extreme value. We will never know the correct answer, and must settle with our best estimate of 26% that factors in our uncertainty regarding the true value of \\(\\rho\\). Long story short, our models are never the truth.\n\n6.4.5 Traditional practices\nAlthough the process was long, we have successfully gone through the cardinal virtues and answered our questions like real data scientists. In this section we will explain more traditional approaches.\n\n6.4.5.1 Confidence intervals\nLet’s go back to our first question:\n\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\n\nIf we had decided to, we could have answered this question using confidence intervals and statistics. First calculate the standard error:\n\\[  SE = \\frac{\\sigma\\text{ of data}}{\\sqrt{\\text{sample size}}} = \\frac{.4785}{\\sqrt{50}} \\approx .067\\]\nWe calculate the numerator by taking the standard deviation of a dataset with 50 observation, where 1’s represent red beads and 0’s represent white beads:\n\nsd(c(rep(1, 17), rep(0, 33)))\n\n[1] 0.4785181\n\n\nYou can then use the standard error to create a 95% confidence interval:\n\\[  CI = \\bar{x} \\hspace{.1cm} \\pm 2SE = .34 \\hspace{.1cm} \\pm .134\\]\nWith 95% confidence, the proportion of red beads in the urn is between 21% and 47%.\nThis is correct, but quite difficult to conceptualize. If our boss needs a quick answer, by all means we can use a confidence interval to save us from doing all the work we did earlier! However, we must be careful with confidence intervals. If we had chosen to answer the first question using them, we would be unable to answer any questions in Temperance. The confidence interval gives us a general range of our uncertainty, but to answer a question that requires knowledge about the value of a parameter, the confidence interval does us very little good. As data scientists we create the posterior distribution to quantify our uncertainty and answer all our questions.\n\n6.4.5.2 Hypothesis tests\nStatisticians also use hypothesis tests to quickly try to answer questions. Recall our view on hypothesis tests: Amateurs test. Professionals summarize. Traditionally, most scientific papers are not so much interested in estimating \\(\\rho\\). They are interested in testing specific hypotheses. What do we mean by that?\nLet’s look at a possible hypothesis in our urn paradigm: there are equal number of red and white beads in the urn. The null hypothesis, denoted by \\(H_0\\), is the theory we are testing, while the alternative hypothesis, denoted by \\(H_a\\), represents the opposite of our theory. Therefore, our hypothesis is designed as such:\n\\(H_0\\): There are an equal number of red and white beads in the urn.\n\\(H_a\\): There are not an equal number of red and white beads in the urn.\nCan we reject that hypothesis? Convention: if the 95% confidence interval excludes the null hypothesis, then we reject it. Here, that would mean if our posterior estimate (plus or minus 2 standard errors) excluded the possibility of the red and white beads being equal (\\(\\rho = .5\\)) we can reject the null hypothesis. In the previous section we determined that the 95% confidence interval is between 21% and 47%. Because 50% is outside of this interval, we could reject the null hypothesis, and conclude that it is unlikely that the proportion of beads in the urn is 50%.\nIf we were testing the theory that \\(\\rho = .45\\) instead, our null hypothesis would fall within the confidence interval. This does not mean that we accept the null hypothesis. Instead, we simply don’t reject it. In our scenario we only know that there is some possibility that \\(\\rho = .45\\). We’re just back where we started! This is why we never test — unless your boss demands a test. Use your judgment, make your models, summarize your knowledge of the world, and use that summary to make decisions."
  },
  {
    "objectID": "06-one-parameter.html#sec-sampling-case-study",
    "href": "06-one-parameter.html#sec-sampling-case-study",
    "title": "6  One Parameter",
    "section": "\n6.5 Case study: Polls",
    "text": "6.5 Case study: Polls\nLet’s now switch gears to a more realistic sampling scenario: a poll. In practice, pollsters do not take 1,000 repeated samples as we did in our previous sampling activities, but rather take only a single sample that’s as large as possible.\nOn December 4, 2013, National Public Radio in the US reported on a poll of President Obama’s approval rating among young Americans aged 18-29 in an article, “Poll: Support For Obama Among Young Americans Eroding.” The poll was conducted by the Kennedy School’s Institute of Politics at Harvard University. A quote from the article:\n\nAfter voting for him in large numbers in 2008 and 2012, young Americans are souring on President Obama.\nAccording to a new Harvard University Institute of Politics poll, just 41 percent of millennials — adults ages 18-29 — approve of Obama’s job performance, his lowest-ever standing among the group and an 11-point drop from April.\n\nLet’s tie elements of the real life poll in this new article with our “tactile” and “virtual” urn activity from Section 6.1 and Section 6.2 using the terminology, notations, and definitions we learned in Section 6.3. You’ll see that our sampling activity with the urn is an idealized version of what pollsters are trying to do in real life.\nFirst, who is the (Study) Population of \\(N\\) individuals or observations of interest?\n\nUrn: \\(N\\) = 1000 identically sized red and white beads\nObama poll: \\(N\\) = ? young Americans aged 18-29\n\nSecond, what is the population parameter?\n\nUrn: The population proportion \\(\\rho\\) of all the beads in the urn that are red.\nObama poll: The population proportion \\(\\rho\\) of all young Americans who approve of Obama’s job performance.\n\nThird, what would a census look like?\n\nUrn: Manually going over all \\(N\\) = 1000 beads and exactly computing the population proportion \\(\\rho\\) of the beads that are red.\nObama poll: Locating all \\(N\\) young Americans and asking them all if they approve of Obama’s job performance. In this case, we don’t even know what the population size \\(N\\) is!\n\nFourth, how do you perform sampling to obtain a sample of size \\(n\\)?\n\nUrn: Using a shovel with \\(n\\) slots.\nObama poll: One method is to get a list of phone numbers of all young Americans and pick out \\(n\\) phone numbers. In this poll’s case, the sample size of this poll was \\(n = 2089\\) young Americans.\n\nFifth, what is your point estimate (AKA sample statistic) of the unknown population parameter?\n\nUrn: The sample proportion \\(\\hat{\\rho}\\) of the beads in the shovel that were red.\nObama poll: The sample proportion \\(\\hat{\\rho}\\) of young Americans in the sample that approve of Obama’s job performance. In this poll’s case, \\(\\hat{\\rho} = 0.41 = 41\\%\\), the quoted percentage in the second paragraph of the article.\n\nSixth, is the sampling procedure representative?\n\nUrn: Are the contents of the shovel representative of the contents of the urn? Because we mixed the urn before sampling, we can feel confident that they are.\nObama poll: Is the sample of \\(n = 2089\\) young Americans representative of all young Americans aged 18-29? This depends on whether the sampling was random (which samples rarely are)\n\nSeventh, are the samples generalizable to the greater population?\n\nUrn: Is the sample proportion \\(\\hat{\\rho}\\) of the shovel’s beads that are red a “good guess” of the population proportion \\(\\rho\\) of the urn’s beads that are red? Given that the sample was representative, the answer is yes.\nObama poll: Is the sample proportion \\(\\hat{\\rho} = 0.41\\) of the sample of young Americans who supported Obama a “good guess” of the population proportion \\(\\rho\\) of all young Americans who supported Obama at this time in 2013? In other words, can we confidently say that roughly 41% of all young Americans approved of Obama at the time of the poll? Again, this depends on whether the sampling was random.\n\nEighth, is the sampling procedure unbiased? In other words, do all observations have an equal chance of being included in the sample?\n\nUrn: Since each bead was equally sized and we mixed the urn before using the shovel, each bead had an equal chance of being included in a sample and hence the sampling was unbiased.\nObama poll: Did all young Americans have an equal chance at being represented in this poll? Again, this depends on whether the sampling was random.\n\nNinth and lastly, was the sampling done at random?\n\nUrn: As long as you mixed the urn sufficiently before sampling, your samples would be random.\nObama poll: Was the sample conducted at random? We can’t answer this question without knowing about the sampling methodology used by Kennedy School’s Institute of Politics at Harvard University. We’ll discuss this more at the end of this section.\n\nIn other words, the poll by Kennedy School’s Institute of Politics at Harvard University can be thought of as an instance of using the shovel to sample beads from the urn. Furthermore, if another polling company conducted a similar poll of young Americans at roughly the same time, they would likely get a different estimate than 41%. This is due to sampling variation.\nLet’s now revisit the sampling paradigm from Section 6.3.1:\nIn general:\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can infer about the population using sampling.\n\nSpecific to the urn:\n\nIf we extract a sample of \\(n = 50\\) beads at random, in other words, we mix all of the equally sized beads before using the shovel, then\nthe contents of the shovel are an unbiased representation of the contents of the urn’s 1000 beads, thus\nany result based on the shovel’s beads can generalize to the urn, thus\nthe sample proportion \\(\\hat{\\rho}\\) of the \\(n = 50\\) beads in the shovel that are red is a “good guess” of the population proportion \\(\\rho\\) of the \\(N = 1000\\) beads that are red, thus\ninstead of manually going over all 1000 beads in the urn, we can infer about the urn using the shovel.\n\nSpecific to the Obama poll:\n\nIf we had a way of contacting a randomly chosen sample of 2089 young Americans and polling their approval of President Obama in 2013, then\nthese 2089 young Americans would be an unbiased and representative sample of all young Americans in 2013, thus\nany results based on this sample of 2089 young Americans can generalize to the entire population of all young Americans in 2013, thus\nthe reported sample approval rating of 41% of these 2089 young Americans is a good guess of the true approval rating among all young Americans in 2013, thus\ninstead of performing an expensive census of all young Americans in 2013, we can infer about all young Americans in 2013 using polling.\n\nSo as you can see, it was critical for the sample obtained by Kennedy School’s Institute of Politics at Harvard University to be truly random in order to infer about all young Americans’ opinions about Obama. Was their sample truly random? It’s hard to answer such questions without knowing about the sampling methodology they used.\nFor example, what if Kennedy School’s Institute of Politics at Harvard University conducted this poll using only mobile phone numbers? People without mobile phones would be left out and therefore not represented in the sample. This flaw is an example of censoring, the exclusion of certain datapoints due to an issue with data collection. This results in an incomplete observation and increases the prediction uncertainty of the estimand, Obama’s approval rating among young Americans. Ensuring that our samples were random was easy to do in our sampling urn exercises; however, in a real life situation like the Obama poll, this is much harder to do."
  },
  {
    "objectID": "06-one-parameter.html#summary",
    "href": "06-one-parameter.html#summary",
    "title": "6  One Parameter",
    "section": "\n6.6 Summary",
    "text": "6.6 Summary\nIn this chapter, we performed both tactile and virtual sampling exercises to make inferences about an unknown parameter: the proportion of red beads. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \\(\\hat{\\rho}\\) to estimate the true proportion \\(\\rho\\). However, we are not just limited to scenarios related to proportions. We can use sampling to estimate other unknown quantities.\nThere is a truth! There is a true value for \\(\\rho\\) which we do not know. We want to create a posterior probability distribution which summarizes our knowledge. We care about the posterior probability distribution of \\(\\rho\\). The center of that distribution is around the mean or median of the proportion in your sample. The sd (or mad) of that posterior is the standard deviation divided by the square root of our sample size. Note that this is the same thing as the standard deviation of the repeated samples.\nWe journey from reality, to our predictions, to the standard error of our predictions, to the posterior probability distribution for \\(\\rho\\). This is our sequence:\n\\(\\rho\\) (the truth) \\(\\Rightarrow\\) \\(\\hat{\\rho}\\) (my estimate) \\(\\Rightarrow\\) the standard error of \\(\\hat{\\rho}\\) (black box of math mumbo jumbo and computer simulation magic) \\(\\Rightarrow\\) our posterior probability distribution for \\(\\rho\\) (our beliefs about the truth).\nThis journey shows how our beliefs about the truth develop through our work. We begin with \\(\\rho\\); \\(\\rho\\) is the truth, the true but unknown value we are estimating. \\(\\hat{\\rho}\\) is our estimate for \\(\\rho\\). There can be millions and millions of \\(\\hat{\\rho}\\)’s. Next, we must estimate the standard error of our estimates (our \\(\\hat{\\rho}\\)’s) to account for the uncertainty of our predictions. Finally, we create a posterior probability distribution for \\(\\rho\\). This distribution is used to answer any questions about \\(\\rho\\).\n\n6.6.1 Other highlights\n\nStandard error is just a fancy term for your uncertainty about something you don’t know. Standard error \\(\\approx\\) our (uncertain) beliefs.\nLarger sample sizes \\(\\implies\\) lower standard errors \\(\\implies\\) more accurate estimates.\nIf we could only know two pieces of information from our data, we would need a measure of the center of the distribution (like mean or median) and a measure of the variability of the distribution (like sd or MAD).\nThe standard error refers to the standard deviation of a sample statistic (also known as a “point estimate”), such as the mean or median. Therefore, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\nstan_glm() can create a joint distribution and then estimate the posterior probability distribution, conditional on the data which was passed in to the data argument. This is a much easier way to create the posterior distribution, and will be explored in more detail in Chapter 7.\nWe use the posterior distribution to answer our questions.\n\nAs we continue our journey, recall the case of Primrose Everdeen and what she represents: no matter how realistic our model is, our predictions are never certain."
  },
  {
    "objectID": "07-two-parameters.html#wisdom",
    "href": "07-two-parameters.html#wisdom",
    "title": "7  Two Parameters",
    "section": "\n7.1 Wisdom",
    "text": "7.1 Wisdom\n\n7.1.1 Preceptor Table\n\n\n\n\n\nWhat rows and columns of data do you need such that, if you had them all, the calculation of the number of interest would be trivial? If you want to know the average height of an adult in India, then the Preceptor Table would include a row for each adult in India and a column for their height. In this scenario, we want to know the average height for men, where “men” includes all males on Earth that are at least 18 years old.\nOne key aspect of this Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. Mainly: are we are modeling (just) for prediction or are we (also) modeling for causation? Do we need a causal model, one which estimates that attitude under both treatment and control? In a causal model, the Preceptor Table would require two columns for the outcome. In this case, we are not modeling for causation; thus, we do not need two outcome columns.\nPredictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model. Here, we are looking at prediction.\nSo, what does our Preceptor Table look like? Assuming we are predicting height for every adult male on planet Earth at this moment in time, we would have height data for every male person at least 18 years of age. This means that we would have about 4 billion rows, one for each male person, along with a column for each individual’s height.\nHere are some rows from our Preceptor Table:\n\n\n\n\n\n\n\nID\n      Sex\n      Height (cm)\n    \n\n\nPerson 1\nMale\n150\n\n\nPerson 2\nMale\n172\n\n\n...\n...\n...\n\n\nPerson 45000\nMale\n160\n\n\nPerson 45001\nMale\n142\n\n\n...\n...\n...\n\n\n\n\n\n\nThis table would extend all the way until person 4 billion-and-something. If we had this table, all of our questions could be answered with simple math and/or simulations. No inference necessary. Now that we’ve seen our ideal dataset, what does our actual data look like?\n\n7.1.2 EDA for nhanes\n\nIn our quest to find suitable data, consider the nhanes dataset from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention.\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(skimr)\nglimpse(nhanes)\n\nRows: 10,000\nColumns: 15\n$ survey         <int> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2…\n$ gender         <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male…\n$ age            <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58…\n$ race           <chr> \"White\", \"White\", \"White\", \"Other\", \"White\", \"White\", \"…\n$ education      <fct> High School, High School, High School, NA, Some College…\n$ hh_income      <fct> 25000-34999, 25000-34999, 25000-34999, 20000-24999, 350…\n$ weight         <dbl> 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7, 7…\n$ height         <dbl> 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.7,…\n$ bmi            <dbl> 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24,…\n$ pulse          <int> 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80,…\n$ diabetes       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ general_health <int> 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, NA…\n$ depressed      <fct> Several, Several, Several, NA, Several, NA, NA, None, N…\n$ pregnancies    <int> NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA,…\n$ sleep          <int> 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA,…\n\n\nnhanes includes 15 variables, including physical attributes like weight and height. Let’s restrict our attention to three varaibles: age, gender and height.\n\nnhanes |> \n  select(age, gender, height)\n\n# A tibble: 10,000 × 3\n     age gender height\n   <int> <chr>   <dbl>\n 1    34 Male     165.\n 2    34 Male     165.\n 3    34 Male     165.\n 4     4 Male     105.\n 5    49 Female   168.\n 6     9 Male     133.\n 7     8 Male     131.\n 8    45 Female   167.\n 9    45 Female   167.\n10    45 Female   167.\n# … with 9,990 more rows\n\n\nExamine a random sample:\n\nnhanes |> \n  select(age, gender, height) |> \n  slice_sample(n = 5)\n\n# A tibble: 5 × 3\n    age gender height\n  <int> <chr>   <dbl>\n1    50 Female   158.\n2    28 Female   167.\n3    22 Female   155.\n4    80 Male     165 \n5    50 Male     195 \n\n\nWe think of both age and height as numbers. And they are numbers! But R distinguishes between “integers” and “doubles,” only the second of which allow for decimal values. In the nhanes data, age is an integer and height is a double.\n\nnhanes |> \n  select(age, gender, height) |> \n  glimpse()\n\nRows: 10,000\nColumns: 3\n$ age    <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9,…\n$ gender <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fema…\n$ height <dbl> 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 166.7, …\n\n\nBe on the lookout for anything suspicious. Are there any NA’s in your data? What types of data are the columns, i.e. why is age characterized as integer instead of double? Are there more females than males?\nYou can never look at your data too closely.\nIn addition to glimpse(), we can run skim(), from the skimr package, to calculate summary statistics.\n\nnhanes |> \n  select(age, gender, height) |> \n  skim()\n\n\nData summary\n\n\nName\nselect(nhanes, age, gende…\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1.00\n36.74\n22.40\n0.0\n17.0\n36\n54.0\n80.0\n▇▇▇▆▅\n\n\nheight\n353\n0.96\n161.88\n20.19\n83.6\n156.8\n166\n174.5\n200.4\n▁▁▁▇▂\n\n\n\n\n\nInteresting! There are 353 missing values of height in our subset of data. Just using glimpse() does not show us that. Let’s filter out the NA’s using drop_na(). This will delete the rows in which the value of any variable is missing. Because we want to examine height in men (not boys, nor females), let’s limit our data to only include adult males.\n\nch7 <- nhanes |> \n  filter(gender == \"Male\", age >= 18) |> \n  select(height) |> \n  drop_na()\n\nLet’s plot this data using geom_histogram().\n\n\nch7 |>\n  ggplot(aes(x = height)) + \n    geom_histogram(bins = 50) +\n    labs(x = \"Height (cm)\",\n         y = \"Count\",\n         title = \"Male Adult Height in the US\") +\n    theme_classic()\n\n\n\n\nFor pedagogical purposes, we won’t use all the NHANES data. Instead, we will randonly sample 50 observations and then pretend that this is all the data we have access to when trying to answer our questions.\n\n# We start by keeping all the adult males, just as we did above.\n\nch7_all <- nhanes |>\n  filter(gender == \"Male\", age >= 18) |>\n  select(height) |>\n  drop_na() \n\n# The `set.seed()` function ensures that the same 50 observations are selected\n# every time this code is run.\n\nset.seed(9)\n\n# Up till now, we have just been using pipes and then \"spitting\" out the result.\n# Now, we want to create a new object, `ch7`, which we will use for the rest of\n# the chapter.\n\nch7 <- ch7_all |> \n  slice_sample(n = 50)\n\nHow will the data we have — which is only for a sample of adult American men more than a decade ago — allow us to answer our questions, however roughly? This is where the notion of population comes into play.\n\n7.1.3 Population\n\nOne of the most important components of Wisdom is the concept of the “population.”\nThe population is not the set of people for which we have data — the participants in the CDC’s Health and Nutrition Examination Survey conducted from 2009 to 2011. This is the dataset. Nor is it the set of all the individuals about whom we would like to have data. Those are the rows in the Preceptor Table. The population is the larger — potentially much larger — set of individuals which include both the data we have and the data we want. Generally, the population will be much larger than either the data we have or the data we want. In fact, there is almost always a time dimension to consider. We also always want to make inferences about right now or the future. By definition, the data we have is always from the past.\nIn this case, we want to predict average height for males today, not for people in 2009 – 2011. We also want to predict height for males outside the United States, a group that is excluded from our dataset. Is it reasonable to generate conclusions for the world from this group? Maybe? We have limited data to work with and we have to determine how far we are willing to generalize to other groups.\nIt is a judgment call, a matter of Wisdom, as to whether or not we may assume that the data we have and the data we want to have (i.e., the Preceptor Table) are drawn from the same population.\nNote that this judgement call differs from the discussion of representativeness in Justice. In Wisdom, we ask questions about how the data was collected, and what this says about our data. In Justice, we ask questions (already knowing how the data was collected) about whether or not inferences made using this data can apply to a broader scope of the population. Suppose, for instance, that the men sampled in NHANES were all NBA players. Wisdom would tell us that these heights are likely to be much greater than those of the average person. Justice asks whether or not this data is representative of the population which we are trying to understand, and what modifications could imposed to fix any potential bias.\nIn the social sciences, there is never a perfect relationship between the data you have and the question you are trying to answer. Data for American males in the past is not the same thing as data for American males today. Nor is it the same as the data for men in France or Mexico. Yet, this data is relevant. Right? It is certainly better than nothing.\nUsing not-perfect data is generally better than using no data at all.\nIs not-perfect data always better? No! If your problem is estimating the median height of 5th grade girls in Tokyo, we doubt that our data is at all relevant. Wisdom recognizes the danger of using non-relevant data to build a model and then mistakenly using that model in a way which will only make the situation worse. If the data won’t help, don’t use the data, don’t build a model. Better to just use your common sense and experience. Or find better data.\nFor now, we will accept that our data works."
  },
  {
    "objectID": "07-two-parameters.html#justice",
    "href": "07-two-parameters.html#justice",
    "title": "7  Two Parameters",
    "section": "\n7.2 Justice",
    "text": "7.2 Justice\n\n\n\n\n\nHaving looked at our data and decided that it is “close enough” to our questions that creating a model will help us come up with better answers, we move on to Justice.\nJustice emphasizes a few key concepts:\n\nThe Population Table, a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\nIs our data representative of the population?\nIs the meaning of the columns consistent, i.e., can we assume validity? We then make an assumption about the data generating mechanism.\n\nWe inspect both representativeness and validity through our Population Table. Representativeness focuses on the rows of the table, while validity focuses on the columns. Let’s explore what that means in more depth.\n\n7.2.1 The Population Table\nThe Population Table shows rows from three sources: the Preceptor Table, the actual data, and the population (outside of the data).\nOur Preceptor Table rows contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (sex and year) but they do not contain any outcome results (height). We are trying to answer questions about the male population in 2021, so our sex entries for these rows will read “Male” and our year entries of these rows will read “2021”.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a study conducted on males in 2009-2011, so our sex entries for these rows will read “Male” and our year entries of these rows will either read “2009”, “2010”, or “2011”.\nOur other rows contain no data. These are subjects which fall under our desired population, but for which we have no data. As such, all outcomes and covariates are missing. (A subtle point is that, even for other data, we “know” the ID and the Year for each subject. Of course, we don’t really know these things, but, conceptually, we are defining the meaning of those rows on the basis of those variables.)\n\n\n\n\n\n\n\n\nSource\n      Sex\n      Year\n      Height\n    \n\n\n\nPreceptor Table\n\n\nMale\n\n\n2021\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nMale\n\n\n2021\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nActual Data\n\n\nMale\n\n\n2009\n\n\n180\n\n\n\n\nActual Data\n\n\nMale\n\n\n2011\n\n\n160\n\n\n\n\nActual Data\n\n\nMale\n\n\n2010\n\n\n168\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nOther\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nOther\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nOther\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n\n\n\n\n7.2.2 Representativeness\nAs we’ve stated above, representativeness involves our rows. More specifically, are the rows that we do have data for representative of the rows for which we do not have data? Ideally, the data we have is a random, unbiased selection from our population, and so the answer to our question is yes.\nFor our nhanes data, is this the case? It is time to investigate.\nAccording to the CDC, individuals are invited to participate in NHANES based on a randomized process. First, the United States is divided into a number of geographical groups (to ensure counties from all areas). From each of these groups, counties are randomly selected to participate. After a county has been randomly selected, members of the households in that county are notified of the upcoming survey, and must volunteer their time to participate. It is clear that this process goes through several layers of randomization (promising!). That being said, many counties are excluded by the end of the process. It is also possible for certain groups or communities to be less representative of the greater population, though we cannot know that for certain.\nThere is also the fact that participation is voluntary. Perhaps certain individuals (immobile, elderly, anxious) are less likely to participate. Perhaps individuals that are hospitalized do not get the opportunity to participate. This impacts our data!\nRegardless, we can assume that this process ensures that all people in the United States have a somewhat equal chance of being surveyed. Thus, our data is representative of our population.\n\n7.2.3 Validity\nValidity involves our columns. More specifically, whether our columns mean the same thing. Consider the following example: we are predicting height with two different datasets. Because both datasets measure height in centimeters, we may assume validity — that our columns are identical in meaning. However, validity is much more complex than it appears at first glance.\nConsider the method of measurements. In our first dataset, were participants asked to remove their shoes? Were they allowed to keep on thick socks? If this is not the case in our second dataset, our columns do not technically represent the same truth.\nThere are even smaller differences that could impact validity. For instance, it is known that — over the course of a day — the average human’s spine compresses about an inch from the time we wake up until the time we go to sleep! If the first set of data was collected in the morning, while the second set of data was collected in the evening, our predictions may be off.\nAs best we can, we need to investigate any possible reasons that we may not be able to say that our columns mean the same thing. If we find an issue, we may need to adjust one set of data to match the other. For instance, if we knew that height was taken in the evening for one sample (with the other measurements taken in the morning), we may add an inch of height to those findings. This would ensure validity.\n\n\n7.2.4 Functional form\nHowever, a little mathematical notation will make our modeling assumptions clear, will bring some precision to our approach. In this case:\n\\[ y_i =  \\mu + \\epsilon_i \\] with \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) is the height of male \\(i\\). \\(\\mu\\) is the average height of all males in the population. \\(\\epsilon_i\\) is the “error term,” the difference between the height of male \\(i\\) and the average height of all males.\n\\(\\epsilon_i\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). The mean being 0 relates to our concept of accuracy; we are assuming that our data is representative enough to be accurate, and so we can expect our average error to be 0. The standard deviation, on the other hand, relates to our concept of precision; the smaller \\(\\sigma\\) is, the more precise our data is, and the larger \\(\\sigma\\) is, the less precise our data is.\nThis is the simplest model we can construct. Note:\n\nThe model has two unknown parameters: \\(\\mu\\) and \\(\\sigma\\). Before we can do anything else we need to estimate the values of these parameters. Can we ever know their exact value? No! Perfection lies only in God’s own R code. But, by using a Bayesian approach similar to what we used in Chapters Chapter 5 and Chapter 6, we will be able to create a posterior probability distribution for each parameter.\n\n\nThe model is wrong, as are all models.\nThe parameter we most care about is \\(\\mu\\). That is the parameter with a substantively meaningful interpretation. Not only is the meaning of \\(\\sigma\\) difficult to describe, we also don’t particular care about its value. Parameters like \\(\\sigma\\) in this context are nuisance or auxiliary parameters. We still estimate their posterior distributions, but we don’t really care what those posteriors look like.\n\\(\\mu\\) is not the average height of the men in the sample. We can calculate that directly. It is 175.606. No estimation required! Instead, \\(\\mu\\) is the average height of men in the population. Recall from the discussions in Chapter 6 that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. For example, if we are walking around Copenhagen, then the population we really care about, in order to answer our three questions, is the set of adult men which we might meet today. This is not the same as the population of adult men in the US in 2010. But is it close enough? Is it better than nothing? We want to assume that both men from nhanes (the data we have) and men we meet in Copenhagen today (the data we want to have) are drawn from the same population. Each case is a different and the details matter.\n\\(\\sigma\\) is an estimate for the standard deviation of the errors, i.e., variability in height after accounting for the mean.\n\nConsider:\n\\[\\text{outcome} = \\text{model} + \\text{what is not in the model}\\] In this case, the outcome is the height of an individual male. This variable, also called the “response,” is what we are trying to understand and/or explain and/or predict. The model is our creation, a mixture of data and parameters, an attempt to capture the underlying structure in the world which generates the outcome.\n\nWhat is the difference between the outcome and the model? By definition, it is what is not in the model, all the blooming and buzzing complexity of the real world. The model will always be incomplete in that it won’t capture everything. Whatever the model misses is thrown into the error term."
  },
  {
    "objectID": "07-two-parameters.html#courage",
    "href": "07-two-parameters.html#courage",
    "title": "7  Two Parameters",
    "section": "\n7.3 Courage",
    "text": "7.3 Courage\n\n\n\n\n\n\nIn data science, we deal with words, math, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.\n\n7.3.0.1 stan_glm\nBayesian models are not hard to create in R. The rstanarm package provides the tools we need, most importantly the function stan_glm().\n\nlibrary(rstanarm)\n\nThe first argument in stan_glm() is data, which in our case is the filtered ch7 tibble which contains 50 observations. The only other mandatory argument is the formula that we want to use to build the model. In this case, since we have no predictor variables, our formula is height ~ 1.\n\nfit_obj <- stan_glm(data = ch7, \n                    formula = height ~ 1, \n                    family = gaussian, \n                    refresh = 0,\n                    seed = 9)\n\nDetails:\n\nThis may take time. Bayesian models, especially ones with large amounts of data, can take longer than we might like. Indeed, computational limits were the main reason why Bayesian approaches were — and, to some extent, still are — little used. When creating your own models, you will often want to use the cache = TRUE code chunk option. This saves the result of the model so that you don’t recalculate it every time you knit your document.\nThe data argument, like all such usage in R, is for the input data.\nIf you don’t set refresh = 0, the model will puke out many lines of confusing output. You can learn more about that output by reading the help page for stan_glm(). The output provides details on the fitting process as it runs as well as diagnostics about the final result. All of those details are beyond the scope of this book.\nYou should always assign the result of the call of stan_glm() to an object, as we do above. By convention, the name of that object will often include the word “fit” to indicate that it is a fitted model object.\nThere is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. height ~ 1 is the code equivalent of \\(y_i = \\mu\\).\nThe default value for family is gaussian, so we did not need to include it in the call above. From the Justice section, the assumption that \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) is equivalent to using gaussian. If \\(\\epsilon_i\\) had a different distribution, we would need to use a different family. We saw an example of such a situation at the end of Chapter 6 when we performed the urn analysis using stan_glm() and setting family = binomial.\nAlthough you can use the standalone function set.seed() in order to make your code reproducible, it is more convenient to use the seed argument within stan_glm(). Even though the fitting process, unavoidably, contains a random component, you will get the exact same answer if we set seed = 9 and rerun this code. It does not matter what number you use for the seed.\n\n\n7.3.0.1.1 Printed model\nThere are several ways to examine the fitted model. The simplest is to print it. Recall that just typing x at the prompt is the same as writing print(x).\n\nfit_obj\n\nstan_glm\n family:       gaussian [identity]\n formula:      height ~ 1\n observations: 50\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 175.6    1.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.5    0.9   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe first line is telling us which model we used, in our case a stan_glm().\nThe second line tells us this model is using a Gaussian, or normal, distribution. We discussed this distribution in Section 2.8.1.4. We typically use this default unless we are working with a lefthand variable that is extremely non-normal, e.g., something which only takes two values like 0/1 or TRUE/FALSE. Since height is (very roughly) normally distributed, the Gaussian distribution is a good choice.\nThe third line gives us back the formula we provided. We are creating a model predicting height with a constant — which is just about the simplest model you can create. Formulas in R are constructed in two parts. First, on the left side of the tilde (the “~” symbol) is the “response” or “dependent” variable, the thing which we are trying to explain. Since this is a model about height, height goes on the lefthand side. Second, we have the “explanatory” or “independent” or “predictor” variables on the righthand side of the tilde. There will often be many such variables but in this, the simplest possible model, there is only one, a single constant. (The number 1 indicates that constant. It does not mean that we think that everyone is height 1.)\nThe fourth and fifth lines of the output tell us that we have 50 observations and that we only have one predictor (the constant). Again, the terminology is a bit confusing. What does it mean to suggest that \\(\\mu\\) is “constant?” It means that, although \\(\\mu\\)’s value is unknown, it is fixed. It does not change from person to person. The 1 in the formula corresponds to the parameter \\(\\mu\\) in our mathematical definition of the model.\nWe knew all this information before we fit the model. R records it in the fit_obj because we don’t want to forget what we did. The second half of the display gives a summary of the parameter values. We can look at just the second half with the detail argument:\n\nprint(fit_obj, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 175.6    1.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.5    0.9   \n\n\nWe see the output for the two parameters of the model: “(Intercept)” and “sigma”. This can be confusing! Recall that the thing we care most about is \\(\\mu\\), the average height in the population. If we had the Preceptor Table — with a row for every adult male in the population we care about and no missing data — \\(\\mu\\) would be trivial to calculate, and with no uncertainty. But only we know that we named that parameter \\(\\mu\\). All that R sees is the 1 in the formula. In most fields of statistics, this constant term is called the “intercept.” So, now we have three things — \\(\\mu\\) (from the math), 1 (from the code), and “(Intercept)” (from the output) — all of which refer to the exact same concept. This will not be the last time that terminology will be confusing.\nAt this point, stan_glm() — or rather the print() method for objects created with stan_glm() — has a problem. We have full posteriors for both \\(\\mu\\) and \\(\\sigma\\). But this is a simple printed summary. We can’t show the entire distribution. So, what are the best few numbers to provide? There is no right answer to this question! Here, the choice is to provide the “Median” of the posterior and the “MAD_SD”.\n\nAnytime you have a distribution, whether posterior probability or otherwise, the most important single number associated with it is some measure of its location. Where is the data? The two most common choices for this measure are the mean and median. We use the median here because posterior distributions can often be quite skewed, making the mean a less stable measure.\nThe second most important number for summarizing a distribution concerns its spread. How far is the data spread around its center? The most common measure used for this is the standard deviation. MAD SD, the scaled median absolute deviation, is another. If the variable has a normal distribution, then the standard deviation and the MAD SD will be very similar. But the MAD SD is much more robust to outliers, which is why it is used here. (Note that MAD SD is the same measure as what we have referred to as mad up till now. The measure is calculated with the mad() command in R. Terminology is confusing, as usual.)\n\nWe can also change the number of digits shown:\n\nprint(fit_obj, detail = FALSE, digits = 1)\n\n            Median MAD_SD\n(Intercept) 175.6    1.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.5    0.9   \n\n\n\nNow that we understand the meaning of Median and MAD_SD in the above display, we can interpret the actual numbers. The median of the intercept, 175.6, is the median of our posterior distribution for \\(\\mu\\), the average height of all men in the population. The median of sigma, 8.5, is the median of our posterior distribution for the true \\(\\sigma\\), which can be roughly understood as the variability in the height of men, once we account for our estimate of \\(\\mu\\).\n\n\n\nThe MAD SD for each parameter is a measure of the variability of our posterior distributions for that parameter. How spread out are they? Speaking roughly, 95% of the mass of a posterior probability distribution is located within +/- 2 MAD SDs from the median. For example, we would be about 95% confident that the true value of \\(\\mu\\) is somewhere between 173.2 and 178.\n\n7.3.0.1.2 Plotting the posterior distributions\nInstead of doing this math in our heads, we can display both posterior probability distributions. Pictures speak where math mumbles. Fortunately, getting draws from those posteriors is easy:\n\n\nfit_obj |> \n  as_tibble()\n\n# A tibble: 4,000 × 2\n   `(Intercept)` sigma\n           <dbl> <dbl>\n 1          175.  8.34\n 2          175.  8.16\n 3          174.  8.26\n 4          174.  8.25\n 5          173.  9.43\n 6          175.  9.24\n 7          174.  8.47\n 8          174.  8.26\n 9          177.  8.71\n10          173.  9.06\n# … with 3,990 more rows\n\n\nThese 4,000 rows are draws from the estimated posteriors, each in its own column. These are like the vectors which result from calling functions like rnorm() or rbinom(). We can create the plot in a similar way:\n\n\nfit_obj |> \n  as_tibble() |> \n  ggplot(aes(x = `(Intercept)`)) +\n  \n# Recall that after_stat() allows us to work with \"stat variables\" that have\n# been calculated by ggplot, such as \"count\" and \"density\".\n  \n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Average Male Height\",\n         subtitle = \"American men average around 176 cm in height\",\n         x = expression(mu), \n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nAlthough it is possible to have variable names like “(Intercept)”, it is not recommended. Avoid weird names! When you are stuck with them, place them in backticks. Even better, rename them, as we do above.\nNote that the title includes the word “Posterior” and not the complete term “Posterior Probability Distribution.” This will be our practice going forward. “Posterior” means “Posterior Distribution” and, any posterior distribution in which the sum of the range is 1 is a “posterior probability distribution.” In most or our plots, “posterior” implies “posterior probability distribution.”\nAlways go back to first principles. There is some truth, an unknown number, a fact about the world. If we knew everything, if we had the Preceptor Table, inference would not be necessary. Algebra would suffice. Alas, in this imperfect world, we have no choice but to be data scientist. We are always uncertain. We summarize our knowledge of unknown numbers with posterior probability distributions, or “posteriors” for short.\n\n\n# The same plot as above, but for sigma instead of mu.\n\nfit_obj |> \n  as_tibble() |> \n  ggplot(aes(x = sigma)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Standard Deviation of Male Height\",\n         subtitle = \"The standard deviation of height is around 7 to 11 cm\",\n         x = expression(sigma),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nAgain, \\(\\sigma\\) is usually a nuisance parameter. We don’t really care what its value is, so we rarely plot it.\n\n\n\n\n\n7.3.1 Testing"
  },
  {
    "objectID": "07-two-parameters.html#temperance",
    "href": "07-two-parameters.html#temperance",
    "title": "7  Two Parameters",
    "section": "\n7.4 Temperance",
    "text": "7.4 Temperance\n\n\n\n\n\nWe have a model. What can we do with it? Let’s answer the four questions with which we started.\n\n7.4.1 Question 1\n\n\nWhat is the average height of men?\n\nIf we had the Preceptor Table, one with a row for every man alive, and with their actual height, we could calculate this number easily. Just take the average of those 3 billion or so rows! Alas, in our actual Preceptor Table, the vast majority of heights are missing. Question marks make simple algebra impossible. So, as with any unknown number, we need to estimate a posterior probability distribution. Objects created by stan_glm() make this easy to do.\n\nnewobs <- tibble(.rows = 1)\n\npe <- posterior_epred(object = fit_obj,\n                      newdata = newobs) |> \n        as_tibble()\n\npe\n\n# A tibble: 4,000 × 1\n     `1`\n   <dbl>\n 1  175.\n 2  175.\n 3  174.\n 4  174.\n 5  173.\n 6  175.\n 7  174.\n 8  174.\n 9  177.\n10  173.\n# … with 3,990 more rows\n\n\nWe will use posterior_epred() many times. The two key arguments are object, the fitted model object returned by stan_glm(), and newdata, which is the tibble which contains the covariate values associated with the unit (or units) for which we want to make a forecast. (In this case, newdata can be any tibble because an intercept-only model does not make use of covariates. We don’t really need a variable named constant, but including it does no harm.) The epred in posterior_epred() stands for expected prediction. In other words, if we pick a random adult male what do we “expect” his height to be. We also call this the expected value.\n\nWe use as_tibble() to convert the matrix which is returned by posterior_epred(). We have a tibble with 1 column and 4,000 rows. The column, unhelpfully named 1, is 4,000 draws from the posterior probability distribution for the expected height of a random male. Recall from earlier chapters how a posterior probability distribution and the draws from a posterior probability distribution are, more or less, the same thing. Or, rather, a posterior probability distribution, its ownself, is hard to work with. Draws from that distribution, on the other hand, are easy to manipulate. We use draws to answer our questions.\nConverting these 4,000 draws into a posterior probability distribution is straightforward.\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Adult Male Height\",\n         subtitle = \"Note that the plot is very similar to the one created with the parameters\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\nThe rest of the Primer will be filled with graphics like this one. You will make dozens of them yourself. The fundamental structure for doing algebra is the real number. The fundamental structure for data science is the posterior probability distribution. You need to be able to create and interpret them.\n\n7.4.2 Question 2\n\nWhat is the probability that the next adult male we meet will be taller than 180 centimeters?\n\nThere are two fundamentally different kinds of unknowns which we care about: expected values (as in the previous question) and predicted values. With the former, we are not interested in any specific individual. The individual value is irrelevant. With predicted values, we care, not about the average, but about this specific person. With the former, we use posterior_epred(). With the latter, the relevant function is posterior_predict(). Both functions return draws from a posterior probability distribution, but the unknown number which underlies the posterior is very different.\nRecall the mathematics:\n\\[ y_i =  \\mu + \\epsilon_i \\]\nWith expected values or averages, we can ignore the \\(\\epsilon_i\\) term in this formula. The expected value of \\(\\epsilon_i\\) is zero since, by assumption, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). However, we can’t ignore \\(\\epsilon_i\\) when predicting the height for a single individual.\n\nnewobs <- tibble(.rows = 1)\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble()\n\npp\n\n# A tibble: 4,000 × 1\n     `1`\n   <dbl>\n 1  179.\n 2  164.\n 3  172.\n 4  172.\n 5  167.\n 6  184.\n 7  184.\n 8  159.\n 9  182.\n10  168.\n# … with 3,990 more rows\n\n\nAs before, it is straightforward to turn draws from the posterior probability distribution into a graphic:\n\npp |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Height of Random Male\",\n         subtitle = \"Uncertainty for a single individual is much greater than for the expected value\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nNote:\n\nThe posterior for an individual is much wider than the posterior for the expected value.\nEyeballing, seems like there is a 1 out of 3 chance that the next man we meet, or any randomly chosen man, is taller than 180 cm.\nWe can calculate the exact probability by manipulating the tibble of draws directly.\n\n\nsum(pp$`1` > 180)/length(pp$`1`)\n\n[1] 0.309\n\n\nIf 30% or so of the draws from the posterior probability distribution are greater than 180 cm, then there is about a 30% chance that the next individual will be taller than 180 cm.\nAgain, the key conceptual difficulty is the population. The problem we actually have involves walking around London, or wherever, today. The data we have involve America in 2010. Those are not the same things! But they are not totally different. Knowing whether the data we have is “close enough” to the problem we want to solve is at the heart of Wisdom. Yet that was the decision we made at the start of the process, the decision to create a model in the first place. Now that we have created a model, we look to the virtue of Temperance for guidance in using that model. The data we have is never a perfect match for the world we face. We need to temper our confidence and act with humility. Our forecasts will never be as good as a naive use of the model might suggest. Reality will surprise us. We need to take the model’s claims with a family-sized portion of salt.\n\n\n7.4.3 Question 3\n\n\nWhat is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?\n\nBayesian models are beautiful because, via the magic of simulation, we can answer (almost!) any question. Because the question is about four random individuals, we need posterior_predict() to give us four sets of draws from four identical posterior probability distributions. Start with a new newobs:\n\nnewobs <- tibble(.rows = 4)\n\nnewobs\n\n# A tibble: 4 × 0\n\n\nIf you need to predict X individuals, then you need a tibble with X rows, regardless of whether or not those rows are otherwise identical.\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble() \n\npp\n\n# A tibble: 4,000 × 4\n     `1`   `2`   `3`   `4`\n   <dbl> <dbl> <dbl> <dbl>\n 1  177.  170.  169.  164.\n 2  178.  167.  164.  185.\n 3  177.  180.  181.  170.\n 4  174.  169.  166.  169.\n 5  176.  191.  186.  165.\n 6  173.  164.  160.  183.\n 7  181.  199.  172.  157.\n 8  169.  181.  174.  174.\n 9  185.  180.  179.  170.\n10  177.  184.  164.  164.\n# … with 3,990 more rows\n\n\nThe next step is to calculate the number of interest. We can not, directly, draw the height of the tallest or shortest out of 4 random men. However, having drawn 4 random men, we can calculate those numbers, and the difference between them.\n\n# First part of the code is the same as we did above.\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble() |> \n  \n        # Second part of the code requires some trickery.\n  \n        rowwise() |> \n        mutate(tallest = max(c_across())) |> \n        mutate(shortest = min(c_across())) |> \n        mutate(diff = tallest - shortest) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `tallest = max(c_across())`.\nℹ In row 1.\nCaused by warning:\n! Using `c_across()` without supplying `cols` was deprecated in dplyr 1.1.0.\nℹ Please supply `cols` instead.\n\npp        \n\n# A tibble: 4,000 × 7\n# Rowwise: \n     `1`   `2`   `3`   `4` tallest shortest  diff\n   <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> <dbl>\n 1  167.  169.  159.  177.    177.     159.  17.6\n 2  179.  187.  178.  170.    187.     170.  16.8\n 3  169.  183.  168.  172.    183.     168.  15.2\n 4  158.  179.  175.  161.    179.     158.  20.9\n 5  182.  182.  158.  170.    182.     158.  24.5\n 6  166.  153.  189.  165.    189.     153.  36.0\n 7  169.  172.  190.  175.    190.     169.  20.3\n 8  172.  164.  178.  197.    197.     164.  32.1\n 9  190.  184.  177.  171.    190.     171.  18.7\n10  161.  192.  165.  193.    193.     161.  32.4\n# … with 3,990 more rows\n\n\nThese steps serve as a template for much of the analysis we do later. It is often very hard to create a model directly of the thing we want to know. There is no easy way to create a model which estimates this height difference directly. It is easy, however, to create a model which allows for random draws.\nGive us enough random draws, and a tibble in which to store them, and we can estimate the world.\nOnce we have random draws from the posterior distribution we care about, graphing the posterior probability distribution is the same-old, same-old.\n\npp |> \n  ggplot(aes(x = diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Max Height Difference Among Four Men\",\n         subtitle = \"The expected value for this difference would be much more narrow\",\n         x = \"Height Difference in Centimeters\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(breaks = seq(0, 50, 10),\n                       labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\nThere is about an 85% chance that, when meeting 4 random men, the tallest will be at least 10 cm taller than the shortest. Exact calculation:\n\nsum(pp$diff > 10) / length(pp$diff)\n\n[1] 0.847\n\n\n\n\n7.4.4 Question 4\n\nWhat is our posterior probability distribution of the height of the 3rd tallest man out of the next 100 we meet?\n\nThe same approach will work for almost any question.\n\nnewobs <- tibble(.rows =  100)\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(third_tallest = sort(c_across(), \n                              decreasing = TRUE)[3])\n\nExplore the pp object. It has 101 columns: one hundred for the 100 individual heights and one column for the 3rd tallest among them. Having done the hard work, plotting is easy:\n\npp |> \n  ggplot(aes(x = third_tallest, y = after_stat(count / sum(count)))) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Posterior for Height of 3rd Tallest Man from Next 100\",\n         subtitle = \"Should we have more or less certainty about behavior in the tails?\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) \n\n\n\n\n\n7.4.5 Three Levels of Knowledge\nWhen answering questions as we have been, it can be easy to falsely believe that we are delivering the truth. This is not the case. In fact, there are three primary levels of knowledge which we need to understand in order to account for our uncertainty.\nThe three primary levels of knowledge possible knowledge in our scenario include: the Truth (the Preceptor Table), the DGM Posterior, and Our Posterior.\n\n7.4.5.1 The Truth\nIf we know the Truth (with a capital “T”), then we know the Preceptor Table. With that knowledge, we can directly answer our question precisely. We can calculate each individual’s height, and any summary measure we might be interested in, like the average height for different ages or countries.\nThis level of knowledge is possible only under an omniscient power, one who can see every outcome in every individual under every treatment. The Truth would show, for any given individual, their actions under control, their actions under treatment, and each little factor that impacted those decisions.\nThe Truth represents the highest level of knowledge one can have — with it, our questions merely require algebra. There is no need to estimate a treatment effect, or the different treatment effects for different groups of people. We would not need to predict at all — we would know.\n\n7.4.5.2 DGM posterior\nThe DGM posterior is the next level of knowledge, which lacks the omniscient quality of The Truth. This posterior is the posterior we would calculate if we had perfect knowledge of the data generating mechanism, meaning we have the correct model structure and exact parameter values. This is often falsely conflated with “our posterior,” which is subject to error in model structure and parameter value estimations.\nWhat we do with the DGM posterior is the same as our posterior — we estimate parameters based on data and predict the future with the latest and most relevant information possible. The difference is that, when we calculate posteriors for an unknown value in the DGM posterior, we expect those posteriors to be perfect.\n\n7.4.5.3 Our posterior\nUnfortunately, our posterior possesses even less certainty! In the real world, we don’t have perfect knowledge of the DGM: the model structure and the exact parameter values. What does this mean?\nWhen we go to our boss, we tell them that this is our best guess. It is an informed estimate based on the most relevant data possible. From that data, we have created a posterior for the average height of males.\nDoes this mean we are certain that the average height lies is the most probable outcome in our posterior? Of course not! As we would tell our boss, it would not be shocking to find out that the actual average height was less or more than our estimate.\nThis is because a lot of the assumptions we make during the process of building a model, the processes in Wisdom, are subject to error. Perhaps our data did not match the future as well as we had hoped. Ultimately, we try to account for our uncertainty in our estimates. Even with this safeguard, we aren’t surprised if we are a bit off."
  },
  {
    "objectID": "07-two-parameters.html#sec-zero-one-outcomes",
    "href": "07-two-parameters.html#sec-zero-one-outcomes",
    "title": "7  Two Parameters",
    "section": "\n7.5 0/1 Outcomes",
    "text": "7.5 0/1 Outcomes\nVariables with well-behaved, continuous ranges are the easiest to handle. We started with height because it was simple. Sadly, however, many variables are not like height. Consider gender, a variable in nhanes which takes on two possible values: “Male” and “Female”. In the same way that we would like to construct a model which explains or predicts height, we would like to build a model which explains or predicts gender. We want to answer questions like:\nWhat is the probability that a random person who is 180 cm tall is female?\nWisdom suggests we start by looking at the data. Because models use numbers, we need to create a new variable, female, which is 1 for Females and 0 for Males.\n\nch7_b <- nhanes |> \n  select(age, gender, height) |>\n  mutate(female = ifelse(gender == \"Female\", 1, 0)) |> \n  filter(age >= 18) |> \n  select(female, height) |> \n  drop_na()\n\nch7_b\n\n# A tibble: 7,424 × 2\n   female height\n    <dbl>  <dbl>\n 1      0   165.\n 2      0   165.\n 3      0   165.\n 4      1   168.\n 5      1   167.\n 6      1   167.\n 7      1   167.\n 8      0   170.\n 9      0   182.\n10      0   169.\n# … with 7,414 more rows\n\n\n\nch7_b |> \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Men are taller than women\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(0, 1),\n                     labels = c(\"Male\", \"Female\"))\n\n\n\n\nWhy not just fit a linear model, as we did above? Consider:\n\nfit_gender_linear <- stan_glm(data = ch7_b,\n                              formula = female ~ height,\n                              family = gaussian,\n                              refresh = 0,\n                              seed = 82) \n\nRecall that the default value for family is gaussian, so we did not need to include it here. Initially, the fitted model seems OK.\n\nprint(fit_gender_linear, digits = 2)\n\nstan_glm\n family:       gaussian [identity]\n formula:      female ~ height\n observations: 7424\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  6.22   0.07 \nheight      -0.03   0.00 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.36   0.00  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nComparing two individuals who differ in height by 1 cm, we expect the taller individual to have a 3% lower probability of being female. That is not unreasonable. The problems show up at the extremes. Consider the fitted values across the range of our data.\n\nch7_b |> \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  geom_smooth(aes(y = fitted(fit_gender_linear)),\n              method = \"lm\",\n              formula = y ~ x,\n              se = FALSE) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Some fitted values are impossible\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(-0.5, 0, 0.5, 1, 1.5),\n                     labels = c(\"-50%\", \"0% (Male)\", \n                                \"50%\", \"100% (Female)\",\n                                \"150%\"))\n\n\n\n\nUsing 1 for Female and 0 for Male allows us to interpret fitted values as the probability that a person is female or male. That is a handy and natural interpretation. The problem with a linear model arises when, as in this case, the model suggests values outside 0 to 1. Such values are, by definition, impossible. People who are 190 cm tall do not have a -25% chance of being female.\nJustice suggests a different functional form, one which restricts fitted values to the acceptable range. Look closely at the math:\n\\[  p(\\text{Female} = 1) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{height})}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{height})} \\]\nThis is an inverse logistic function, but don’t worry about the details. Mathematical formulas are never more than a Google search away. Instead, note how the range is restricted. Even if \\(\\beta_0 + \\beta_1 \\text{height}\\) is a very large number, the ratio is bound below 1. Similarly, no matter how negative \\(\\beta_0 + \\beta_1 \\text{height}\\) is, the ratio can never be smaller than 0. The model can not, ever, produce impossible values.\nWhenever you have two categories as the outcome, you should use family = binomial.\n\nCourage allows us to use the same tools for fitting this logistic regression as we did above in fitting linear models.\n\nfit_2 <- stan_glm(data = ch7_b,\n                  formula = female ~ height,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 27)\n\n\nprint(fit_2, digits = 3)\n\nstan_glm\n family:       binomial [logit]\n formula:      female ~ height\n observations: 7424\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 43.389  0.999\nheight      -0.257  0.006\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nOne major difference between linear and logistic models is that parameters in the latter are much harder to interpret. What does it mean, substantively, that \\(\\beta_1\\) is -0.26? That is a topic for a more advanced course.\n\nFortunately, parameters are not what we care about. They are epiphenomenon, unicorns of our imagination. Instead, we want answers to our questions, for which Temperance — and the functions in rstanarm — is our guide. Recall our question:\nWhat is the probability that a random person who is 180 cm tall is female?\n\nnewobs <- tibble(height = 180)\n\npe <- posterior_epred(fit_2, newdata = newobs) |> \n  as_tibble()\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for p(female | height = 180 cm)\",\n         subtitle = \"There is a 5-6% chance that a person this tall is female\",\n         x = \"Probability\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\nThere is only about a 1 in 20 chance that a 180 centimeter tall person is female.\nNote that both the x and y axes are probabilities. Whenever we create a posterior probability distribution then, by definition, the y-axis is a probability. The x-axis is the unknown number we do not know. That unknown number can be anything — the weight of the average male, the height of 3rd tallest out of 100 men, the probability that a 180 cm tall person is female. A probability is just another number. The interpretation is the same as always.\nAnother major difference with logistic models is that posterior_epred() and posterior_predict() return different types of objects. posterior_epred() returns probabilities, as above. posterior_predict(), on the other hand, returns predictions, as its name suggests. In other words, it returns zeros and ones. Consider another question:\nIn a group of 100 people who are 180 centimeters tall, how many will be women?\n\nnewobs <- tibble(height = rep(180, 100))\n\npp <- posterior_predict(fit_2, newdata = newobs) |> \n  as_tibble() \n\npp[, 1:4]\n\n# A tibble: 4,000 × 4\n     `1`   `2`   `3`   `4`\n   <int> <int> <int> <int>\n 1     0     0     0     0\n 2     0     0     0     0\n 3     0     0     0     0\n 4     0     0     0     0\n 5     0     0     0     0\n 6     0     0     0     0\n 7     0     0     0     0\n 8     0     0     0     0\n 9     0     0     0     0\n10     1     0     0     0\n# … with 3,990 more rows\n\n\n\nWe show just the first 4 columns for convenience. Each column is 4,000 draws from the posterior predictive distribution for the gender of a person who is 180 cm tall. (Since all 100 people have the same height, all the columns are draws from the same distribution.)\nWe can manipulate this object on a row-by-row basis.\n\npp <- posterior_predict(fit_2, newdata = newobs) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(total = sum(c_across()))\n\npp[, c(\"1\", \"2\", \"100\", \"total\")]\n\n# A tibble: 4,000 × 4\n# Rowwise: \n     `1`   `2` `100` total\n   <int> <int> <int> <int>\n 1     0     0     0     5\n 2     0     0     0     3\n 3     0     0     0     7\n 4     0     0     0     8\n 5     0     0     0     6\n 6     0     0     0     5\n 7     0     0     0     5\n 8     0     0     0     2\n 9     0     0     0     6\n10     0     0     0     4\n# … with 3,990 more rows\n\n\ntotal is the number of women in each row. Manipulating draws on a row-by-row basis is very common.\n\n\npp |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Number of Women among 100 People 180 cm Tall\",\n         subtitle = \"Consistent with probability estimate above\",\n         x = \"Number of Women\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\nThat 5 or 6 women is the most likely number is very consistent with the answer to the first question. There we found that a random person who is 180 cm tall has a 5% or 6% chance of being female. So, with 100 such people, 5 or 6 seems a reasonable total. But the expected value from posterior_epred(), although it does provide a sense of where the center of the predictive distribution will be, does not tell us much about the range of possible outcomes. For that, we need posterior_predict()."
  },
  {
    "objectID": "07-two-parameters.html#summary",
    "href": "07-two-parameters.html#summary",
    "title": "7  Two Parameters",
    "section": "\n7.6 Summary",
    "text": "7.6 Summary\n\nThe next five chapters will follow the same process we have just completed here. We start with a decision we have to make. With luck, we will have some data to guide us. (Without data, even the best data scientist will struggle to make progress.) Wisdom asks us: “Is the data we have close enough to the decision we face to make using that data helpful?” Often times, the answer is “No.”\nOnce we start to build the model, Justice will guide us. Is the model descriptive or causal? What is the mathematical relationship between the dependent variable we are trying to explain and the independent variables we can use to explain it? What assumptions are we making about distribution of the error term?\nHaving set up the model framework, we need Courage to implement the model in code. Without code, all the math in the world is useless. Once we have created the model, we need to understand it. What are the posterior distributions of the unknown parameters? Do they seem sensible? How should we interpret them?\nTemperance guides the final step. With a model, we can finally get back to the decision which motivated the exercise in the first place. We can use the model to make statements about the world, both to confirm that the model is consistent with the world and to use the model to make predictions about numbers which we do not know.\nLet’s practice this process another dozen or so times."
  },
  {
    "objectID": "08-three-parameters.html#wisdom",
    "href": "08-three-parameters.html#wisdom",
    "title": "8  Three Parameters",
    "section": "\n8.1 Wisdom",
    "text": "8.1 Wisdom\n\n8.1.1 Preceptor Table\nWisdom begins with considering the questions we desire to answer and the data set we are given. In this chapter, we are going to ask a series of questions involving train commuters’ ages, party affiliations, incomes, and political ideology, as well as the causal effect of exposure to Spanish-speakers on their attitude toward immigration. These questions will pertain to all train commuters in the US today. Given these types of questions, the Preceptor Table would be:\n\n\n\n\n\n\n\n\n\n\nPreceptor Table\n    \n\nID\n      Age\n      Party\n      Income\n      Liberal\n      Control Ending Attitude\n      Treated Ending Attitude\n    \n\n\n\nCommuter 1\n\n\n23\n\n\nDemocrat\n\n\n50000\n\n\nLiberal\n\n\n3\n\n\n8\n\n\n\n\nCommuter 2\n\n\n18\n\n\nRepublican\n\n\n150000\n\n\nNot Liberal\n\n\n7\n\n\n7\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nCommuter 1000\n\n\n49\n\n\nRepublican\n\n\n100000\n\n\nLiberal\n\n\n4\n\n\n8\n\n\n\n\nCommuter 1001\n\n\n38\n\n\nDemocrat\n\n\n200000\n\n\nLiberal\n\n\n9\n\n\n7\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\n\n\nRecall: a Preceptor Table is the smallest possible table such that, if there is no missing data, all our questions are easy to answer. To answer questions — like “What is, today, the average age of train commuters in the US?” — we need a row for every train commuter.\nNotice that one of our questions is about a causal effect: What change in immigration attitudes is caused by being exposed to Spanish-speakers? Answering causal questions requires (at least) two potential outcomes: immigration attitude for those who receive the treatment of being exposed to Spanish-speakers and for those who do not.\nHaving created the Preceptor Table, we now look at the data we have: the trains data set from the primer.data package.\n\n8.1.2 EDA for trains\n\nAlways explore your data. Recall the discussion from Chapter 4. Enos (2014) randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. Exposure to Spanish-speakers – the treatment – influenced attitudes toward immigration. These reactions were measured through changes in answers to three survey questions. Load the necessary libraries and look at the data.\n\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(skimr)\nlibrary(tidyverse)\n\n\nglimpse(trains)\n\nRows: 115\nColumns: 14\n$ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n$ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n$ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n$ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n$ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n$ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n$ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n$ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n$ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n$ hisp_perc      <dbl> 0.0264, 0.0154, 0.0191, 0.0191, 0.0191, 0.0231, 0.0304,…\n$ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n$ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\n\n\nThe data include information about each respondent’s gender, political affiliations, age, income and so on. treatment indicates whether a subject was in the control or treatment group. The key outcomes are their attitudes toward immigration both before (att_start) and after (att_end) the experiment. Type ?trains to read the help page for more information about each variable.\nLet’s restrict attention to a subset of the variables we need to answer our questions, as specified in the Preceptor Table. age is the age of the respondent; party is their political party affiliation; liberal is whether they are liberal or not; income is the income of the respondent; treatment specifies whether the respondent was given the treatment of being exposed to Spanish-speakers in the train station; and att_end tell us about the respondent’s attitude on immigration after the treatment, .\n\nch8 <- trains |> \n  select(age, att_end, party, income, treatment, liberal)\n\nIt is always smart to look at a some random slices of the data:\n\nch8 |> \n  slice_sample(n = 5)\n\n# A tibble: 5 × 6\n    age att_end party    income treatment liberal\n  <int>   <dbl> <chr>     <dbl> <fct>     <lgl>  \n1    36      11 Democrat 135000 Treated   FALSE  \n2    42      10 Democrat  62500 Treated   TRUE   \n3    44       6 Democrat  62500 Control   TRUE   \n4    22      11 Democrat 135000 Control   FALSE  \n5    50       7 Democrat 135000 Treated   TRUE   \n\n\natt_end is a measure of person’s attitude toward immigration. A higher number means more conservative, i.e., a more exclusionary stance toward immigration into the United States.\n\nch8 |> \n  glimpse()\n\nRows: 115\nColumns: 6\n$ age       <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53, …\n$ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8,…\n$ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat\"…\n$ income    <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 135000…\n$ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contro…\n$ liberal   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,…\n\n\nPay attention to the variable types. Do they make sense? Perhaps. But there are certainly grounds for suspicion. Why is att_end a double rather than an integer? All the values in the data appear to be integers, so there is no benefit to having these variables be doubles. Why is party a character variable and treatment a factor variable? It could be that these are intentional choices made by the creator of the tibble, i.e., us. Or, these could be mistakes. Most likely, these choices are a mixture of sensible and arbitrary. Regardless, it is your responsibility to notice them. You can’t make a good model without looking closely at the data which you are using.\n\nch8 |> \n  skim()\n\n\nData summary\n\n\nName\nch8\n\n\nNumber of rows\n115\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nlogical\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\nparty\n0\n1\n8\n10\n0\n2\n0\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\nCon: 64, Tre: 51\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\nliberal\n0\n1\n0.44\nFAL: 64, TRU: 51\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1\n42.37\n12.20\n20\n33\n43\n52\n68\n▆▇▇▇▃\n\n\natt_end\n0\n1\n9.14\n2.87\n3\n7\n9\n11\n15\n▂▃▇▃▃\n\n\nincome\n0\n1\n141813.04\n74476.64\n23500\n87500\n135000\n135000\n300000\n▅▇▇▁▆\n\n\n\n\n\nskim() shows us what the different values of treatment are because it is a factor. Unfortunately, it does not do the same for character variables like party. The ranges for age and att_end seem reasonable. Recall that participants were asked three questions about immigration issues, each of which allowed for an answer indicated strength of agreement on a scale form 1 to 5, with higher values indicating more agreement with conservative viewpoints. att_end is the sum of the responses to the three questions, so the most liberal possible value is 3 and the most conservative is 15.\n\n8.1.3 Population\n\n\nThe data that we have are very limited. There are only 115 observations, all from 2012 and involving train commuters to Boston. Can we assume that the the data we have and the data in our Preceptor Table are drawn from the same population? Only your judgment, along with advice from your colleagues, can guide you.\nThere is no truth here. The data is real enough, but you created the Preceptor Table. Whether or not there is a population from which you can assume both the data and the Preceptor Table might have been drawn is not a TRUE/FALSE question.\nThe Preceptor Table is all US train commuters today. However, the data we have involves train commuters in the Boston area in 2012. So, what we must ask ourselves is if this data from Boston in 2012 can be drawn from the same population as all train commuters in the US today.\nThe key concept is the idea of a “population.” From which larger population is the data we have being (conceptually) drawn? If we were only interested in the age of individuals in our data set, we would have no need for inference. We know everyone’s ages already. We only need tools like stan_glm() if we seek to understand individuals not in our data.\nThe issue is always: Do the data we have and the data we want to have come from the same population? If there is no connection between the two, progress is impossible. But, in this case, if we are willing to consider the population to be US residents over the last decade (including today), and if we are willing to assume that there is a single population from which the Preceptor Table and our data set is drawn, then we can use our data to create a model to answer our questions.\n\n8.1.4 Population Table\nHaving determined that the Preceptor Table and the data were drawn from the same population, we can produce a Population Table.\nThe Population Table includes rows from three sources: the Preceptor Table, the actual data, and all other members of the population.\nThe rows in the Preceptor Table contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (city and year) but they do not contain any outcome results. We are trying to answer questions about the train commuter population in 2021, so our city entries for these rows will vary and our year entries of these rows will read “2021”.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a study conducted on train commuters around Boston, MA in 2012, so our city entries for these rows will read “Boston, MA” and our year entries of these rows will read “2012”.\nOur other rows contain no data. These are subjects which are members of our desired population, but for which we have no data. As such, all outcomes and covariates are missing. The “Other” rows his may include, for example, data from a range of years before and after 2012, since we considered little change overtime, so we would also consider this data to be representative and drawn from the larger population of train commuters in years near 2012.\nThe population table includes commuters from our Preceptor Table with the information that we would ideally have to answer the questions and those from the data we have that is specific to Boston, MA. Additionally, the population table also includes the groups of people within the population from which both the data we have and the Preceptor table is drawn from that we don’t have.\n\n\n\n\n\n\n\nPopulation Table\n    \n\nSource\n      City\n      Year\n      Age\n      Party\n      Income\n      Liberal\n      Treatment\n      Control\n    \n\n\n\nOther\n\n\n?\n\n\n2008\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nOther\n\n\n?\n\n\n2009\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nData\n\n\nBoston, MA\n\n\n2012\n\n\n43\n\n\nDemocrat\n\n\n150000\n\n\nLiberal\n\n\n6\n\n\n?\n\n\n\n\nData\n\n\nBoston, MA\n\n\n2012\n\n\n52\n\n\nRepublican\n\n\n50000\n\n\nNot Liberal\n\n\n?\n\n\n2\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nOther\n\n\n?\n\n\n2013\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nOther\n\n\n?\n\n\n2014\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPreceptor Table\n\n\nWilmington, DE\n\n\n2021\n\n\n45\n\n\nRepublican\n\n\n45000\n\n\nNot Liberal\n\n\n?\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nAtlanta, GA\n\n\n2021\n\n\n65\n\n\nRepublican\n\n\n15000\n\n\nNot Liberal\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nOther\n\n\n?\n\n\n2025\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nOther\n\n\n?\n\n\n2027\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n\n\n\nOur Preceptor Table rows are chronologically ordered. The “other” rows our greater population for which we are making assumptions — this is why the year at the start is earlier than our data and the year at the end is later than our Preceptor Table. This shows the more expansive population that we are inferring about."
  },
  {
    "objectID": "08-three-parameters.html#age-party",
    "href": "08-three-parameters.html#age-party",
    "title": "8  Three Parameters",
    "section": "\n8.2 age ~ party",
    "text": "8.2 age ~ party\n\nWe want to build a model and then use that model to make claims about the world. Our questions about the relationship between age and party are the following:\nWhat is the expected age of a Democrat at the train station?\nIn a group of three Democrats and three Republicans, what will the age difference be between the oldest Democrat and the youngest Republican?\nWe can answer these and similar questions by creating a model that uses party affiliation to predict age\n\n\n\n\n\n8.2.1 Justice\n\n\n\n\nJustice\n\n\n\n\nJustice consists of four topics: validity, stability, representativeness, and the data generating mechanism (DGM).\nTo understand validity in regards to the Population Table, we must first recognize an inherent flaw in any experiment design: no two units receive exactly the same treatment. If this doesn’t ring true, consider our Spanish speaking train experiment. The units on the Spanish-speaking platform received the same treatment, right? No, actually! Every unit heard Spanish at a different decibel level. Every unit also heard Spanish for a different amount of time, depending on when they arrived to the station. Thus, two units can have the same treatment — that is, hearing Spanish on the platform — while having very different versions of that treatment. This is why it is crucial to define one’s estimand precisely: if we are interested in the difference in potential outcomes between Spanish being spoken for 10 minutes at a 60 dB versus control, we can ignore all the other possible columns in the Population Table.\nWe then must consider the stability of our model. Stability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn. With something like height, it is much easier to assume stability over a greater period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. With something like political ideology, it is much harder to make the assertion that data collected in 2010 would be stable to data collected in 2025. When we are confronted with this uncertainty, we can consider making our timeframe smaller. However, we would still need to assume stability from 2014 (time of data collection) to today. Stability allows us to ignore the issue of time.\nNext, let’s consider representativeness. Representativeness has to do with how well our sample represents the larger population we are interested in generalizing to. Does the train experiment allow us to calculate a causal effect for people who commute by cars? Can we calculate the causal effect for people in New York City? Before we generalize to broader populations we have to consider if our experimental estimates are applicable beyond our experiment. Generally: if there was no chance that a certain type of person would have been in this experiment, we cannot make an assumption for that person.\n\nNow let’s discuss the kind of DGM we will be using. For our purposes, we will be choosing between linear or logistic, however, there are several different types of distributions which we will not be dealing with at the moment. Let’s recall the difference between linear and logistic models, which is dependent upon the outcome variable. If the outcome variable is continuous, we will use the a linear model, and when the outcome variable is binary, or only has two options, we will use a logistic model. In this case, our outcome variable is age which is continous, so it is a linear model.\nWe can also consider the type of DGM. Any DGM with age as its dependent variable will be predictive, not causal, for the simple reason that nothing, other than time, can change your age. You are X years old. It would not matter if you changed your party registration from Democrat to Republican or vice versa. Your age is your age. When dealing with a non-causal DGM, the focus is on predicting things. The underlying mechanism which connects age with party is less important than the brute statistical fact that there is a connection. Predictive models care little about causality.\n\n\nA good way at looking at this is with a Preceptor Table, as seen below. Unlike the previous table in Chapter 7, we now have two columns in addition to ID. Since our data does not include all Republicans and Democrats in the world, not every row is filled in.\n\n\n\n\n\n\n\n\nID\n      \n        Predictor\n      \n      \n        Outcome\n      \n    \n\nPolitical Party\n      Age\n    \n\n\n\n1\nD\n31\n\n\n2\n?\n?\n\n\n...\n...\n...\n\n\n473\nD\n58\n\n\n474\n?\n?\n\n\n...\n...\n...\n\n\n3,258\n?\n?\n\n\n3,259\nR\n49\n\n\n...\n...\n...\n\n\nN\n?\n?\n\n\n\n\n\n\n \nWe now know that we are working with a predictive DGM. Recall:\n\\[\\text{outcome} = \\text{model} + \\text{not in the model}\\]\nIn words, any event depends on our explicitly described model as well as on influences unknown to us. Everything that happens in the world is the result of various factors, and we can only consider some of them in our model (because we do not know about some influences, or because we have no data about them).\n\nLet’s make our DGM. The mathematics:\n\\[ y_i = \\beta_1 republican_i + \\beta_2 democrat_i + \\epsilon_i\\]\nwhere \\[republican_i, democrat_i \\in \\{0,1\\}\\] \\[republican_i +  democrat_i = 1\\] \\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\nDon’t panic dear poets and philosophers, the whole thing is easier than it looks. This will follow the form above in which the outcome is the result of what is in our model and not in our model.\n\nOn the left-hand side we have the outcome, \\(y_i\\), which is the variable to be explained. In our case, this is the age of an individual in the population.\nThe right-hand side contains two parts, that which is contained within the model, and that which isn’t.\n\nFirst, we have the part contained in the model, which consists of the parameter and the data points. The betas are our two parameters: \\(\\beta_1\\) is the average age of Republicans in the population and \\(\\beta_2\\) is the average age of Democrats in the population. \\(republican_i\\) and \\(democrat_i\\) are our explanatory variables and take the values 1 or 0. As shown in our model, \\(\\beta_1 republican_i\\) and \\(\\beta_2 democrat_i\\) are two similar terms which are added together to make our model and each term consists of a parameter and a data point. If person \\(i\\) is a Republican we have \\(republican_i = 1\\) and \\(democrat_i = 0\\). If person \\(i\\) is a Democrat we have \\(republican_i = 0\\) and \\(democrat_i = 1\\). In other words, their values are mutually exclusive – if you are a Democrat, you cannot also be a Republican.\n\n\nThe second part in the right-hand side, \\(\\epsilon_i\\) (“epsilon”), represents the unexplained part of the outcome and is called the error term. This includes all factors that have an influence on someone’s age but are not related to party affiliation. In other words, \\(\\epsilon_i\\) is what influence age that is not factored into our model. We assume that this error follows a normal distribution with an expected value of 0 (meaning it is 0 on average) and it is simply the difference between the outcome and our model predictions.\n\nSome things to note about our model:\n\nThe small \\(i\\)’s are an index to number the observations. It is equivalent to the “ID” column in our Preceptor Table and simply states that the outcome for person \\(i\\) is explained by the modeled and non-modeled factors for person \\(i\\).\nThe model is a claim about how the world works, not just for the 115 individuals for which we have data but for the all the people in the population for which we seek to draw inferences.\nAlthough terminology differs across academic fields, the most common term to describe a model like this is a “regression.” We are “regressing” age on party in order to see if they are associated with each other. The formula above is a “regression formula”, and the model is a “regression model.” This terminology would also apply to our model of height in Chapter 7.\nThe model in Chapter 7 is sometimes called “intercept-only” because the only (interesting) parameter is the intercept. Here we have a “two intercept” model because, instead of estimating an average for the whole population, we are estimating two averages.\n\n8.2.2 Courage\n\n\n\n\nCourage\n\n\n\n\nCourage allows us to translate math to code.\nTo get posterior distributions for our three parameters, we will again use stan_glm(), just as we did in Chapter 7.\n\nfit_1 <- stan_glm(age ~ party - 1, \n                    data = trains, \n                    family = gaussian,\n                    seed = 17,\n                    refresh = 0)\n\n\nThe variable before the tilde, age, is our outcome.\nThe only explanatory variable is party. This variable has only two values, ‘Democrat’ and ‘Republican’.\nRecall that our model is linear. Since we are using a linear model, the family we use will be gaussian.\nWe have also added -1 at the end of the equation, indicating that we do not want an intercept, which would otherwise be added by default.\n\nThe resulting output:\n\nfit_1\n\nstan_glm\n family:       gaussian [identity]\n formula:      age ~ party - 1\n observations: 115\n predictors:   2\n------\n                Median MAD_SD\npartyDemocrat   42.6    1.2  \npartyRepublican 41.2    2.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 12.3    0.8  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\npartyDemocrat corresponds to \\(\\beta_1\\), the average age of Democrats in the population. partyRepublican corresponds to \\(\\beta_2\\), the average age of Republicans in the population. Since we don’t really care about the posterior distribution for \\(\\sigma\\), we won’t discuss it here. Graphically:\n\n\nfit_1 |> \n  as_tibble() |> \n  select(-sigma) |> \n  mutate(Democrat = partyDemocrat, Republican = partyRepublican) |>\n  pivot_longer(cols = Democrat:Republican,\n               names_to = \"parameter\",\n               values_to = \"age\") |> \n  ggplot(aes(x = age, fill = parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Average Age\",\n         subtitle = \"More data allows for a more precise posterior for Democrats\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nThe unknown parameters \\(\\beta_1\\) (partyDemocrat) and \\(\\beta_2\\) (partyRepublican) are still unknown. We can never know the true average age of all Democrats in the population. But we can calculate a posterior probability distribution for each parameter. Comments:\n\nDemocrats seem slightly older than Republicans. That was true in the sample and so, almost (but not quite!) by definition, it will be true in our the posterior probability distributions.\nOur estimate for the average age of Democrats in the population is much more precise than that for Republicans because we have five times as many Democrats as Republicans in our sample. A central lesson from Chapter 6 is that the more data you have related to a parameter, the narrower your posterior distribution will be.\nThere is a great deal of overlap between the two distributions. Would we be surprised if, in truth, the average age of Republicans in the population was greater than that for Democrats? Not really. We don’t have enough data to be sure either way.\nThe phrase “in the population” is doing a great deal of work because we have not said what, precisely, we mean by the “population.” Is it the set of people on those commuter platforms on those days in 2012 when the experiment was done? Is it the set of people on all platforms, including ones never visited? Is it the set of all Boston commuter? All Massachusetts residents? All US residents? Does it include people today, or can we only draw inferences for 2012? We will explore these questions in every model we create.\nThe parameters \\(\\beta_1\\) and \\(\\beta_2\\) can be interpreted in two ways. First, like all parameters, they are a part of the model. We need to estimate them. But, in many cases, we don’t really care what the value of the parameter is. The exact value of \\(\\sigma\\), for example, does not really matter. Second, some parameters have a substantive interpretaion, as with \\(\\beta_1\\) and \\(\\beta_2\\) being the average ages in the population. But this will often not be the case! Fortunately, with such models, we can use functions like posterior_epred() and posterior_predict() to answer our questions.\n\nConsider a table which shows a sample of 8 individuals.\n\n\n\n\n\n\n\n8 Observations from Trains Dataset\n    \n\nAge\n      Party\n      Fitted\n      Residual\n    \n\n\n31\nDemocrat\n42.59\n-11.593526\n\n\n34\nRepublican\n41.19\n-7.186270\n\n\n63\nDemocrat\n42.59\n20.406474\n\n\n45\nDemocrat\n42.59\n2.406474\n\n\n55\nDemocrat\n42.59\n12.406474\n\n\n37\nDemocrat\n42.59\n-5.593526\n\n\n53\nRepublican\n41.19\n11.813730\n\n\n36\nDemocrat\n42.59\n-6.593526\n\n\n\n\n\n\n\n \nThe fitted values are the same for all Republicans and for all Democrats, as the model produces one fitted value for each condition. This table shows how just a sample of 8 individuals captures a wide range of residuals, making it difficult to predict the age of a new individual. We can get a better picture of the unmodeled variation in our sample if we plot these three variables for all the individuals in our data.\nThe following three histograms show the actual outcomes, fitted values, and residuals of all people in trains:\n\noutcome <- ch8 |> \n  ggplot(aes(age)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Age\",\n         y = \"Count\") \n\nfitted <- tibble(age = fitted(fit_1)) |> \n  ggplot(aes(age)) +\n    geom_bar() +\n    labs(x = \"Fitted Values\",\n         y = NULL) +\n    scale_x_continuous(limits = c(20, 70)) \n\nres <- tibble(resids = residuals(fit_1)) |> \n  ggplot(aes(resids)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Residuals\",\n         y = NULL) \n  \n\noutcome + fitted + res +\n  plot_annotation(title = \"Decomposition of Height into Fitted Values and Residuals\")\n\n\n\n\nThe three plots are structured like our equation and table above. A value in the left plot is the sum of one value from the middle plot plus one from the right plot.\n\nThe actual age distribution looks like a normal distribution. It is centered around 43, and it has a standard deviation of about 12 years.\nThe middle plot for the fitted values shows only two adjacent spikes, which represent the estimates for Democrats and Republicans.\nSince the residuals plot represents the difference between the other two plots, its distribution looks like the first plot.\n\n\n8.2.3 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall the first questions with which we began this section:\n\nWhat is the probability that, if a Democrat shows up at the train station, he will be over 50 years old?\n\nSo far we have only tried our model on people from our data set whose real age we already knew. This is helpful to understand the model, but our ultimate goal is to understand more about the real world, about people we don’t yet know much about. Temperance guides us to make meaningful predictions and to become aware of their known and unknown limitations.\nStart with a simple question, what are the chances that a random Democrat is over 50 years old? First, we create a tibble with the desired input for our model. In our case the tibble has a variable named “party” which contains a single observation with the value “Democrat”. This is a bit different than Chapter 7.\n\nnew_obs <- tibble(party = \"Democrat\")\n\n\nUse posterior_predict() to create draws from the posterior for this scenario. Note that we have a new posterior distribution under consideration here. The unknown parameter, call it \\(D_{age}\\), is the age of a Democrat. This could be the age of a randomly selected Democrat from the population or of the next Democrat we meet or of the next Democrat we interview on the train platform. The definition of “population” determines the appropriate interpretation. Yet, regardless, \\(D_{age}\\) is an unknown parameter. But it is not one — like \\(\\beta_1\\), \\(\\beta_2\\), or \\(\\sigma\\) — for which we have already created a posterior probability distribution. That is why we need posterior_predict().\nposterior_predict() takes two arguments: the model for which the simulations should be run, and a tibble indicating for which and how many parameters we want to run these simulations. In this case, the model is the one from Courage and the tibble is the one we just created.\n\npp <- posterior_predict(fit_1, newdata = new_obs) |>\n    as_tibble() \n\nhead(pp, 10)\n\n# A tibble: 10 × 1\n     `1`\n   <dbl>\n 1  46.7\n 2  49.8\n 3  43.9\n 4  55.7\n 5  32.5\n 6  37.0\n 7  44.4\n 8  52.4\n 9  48.5\n10  34.9\n\n\nThe result are draws from the posterior distribution of the age of a Democrat. It is important to understand that this is not a concrete person from the trains dataset - the algorithm in posterior_predict() simply uses the existing data from trains to estimate this posterior distribution.\n\npp |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for a Random Democrat's Age\",\n         subtitle = \"Individual predictions are always more variable than expected values\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nOnce we have the posterior distribution, we can answer (almost) any reasonable question. In this case, the probability that the next Democrat will be over 50 is around 28%.\n\nsum(pp$`1` > 50) / nrow(pp)\n\n[1] 0.2675\n\n\nRecall the second question:\n\nIn a group of three Democrats and three Republicans, what will the age difference be between the oldest Democrat and the youngest Republican?\n\nAs before we start by creating a tibble with the desired input. Note that the name of the column (“party”) and the observations (“Democrat”, “Republican”) must always be exactly as they are in the original data set. This tibble as well as our model can then be used as arguments for posterior_predict():\n\nnewobs <- tibble(party = c(\"Democrat\", \"Democrat\", \"Democrat\", \n                        \"Republican\", \"Republican\",\"Republican\"))\n\nposterior_predict(fit_1, newdata = newobs) |>\n    as_tibble() \n\n# A tibble: 4,000 × 6\n     `1`   `2`   `3`   `4`   `5`   `6`\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  22.2  38.3  43.4  48.8  47.2  48.1\n 2  25.7  51.4  33.8  39.3  42.3  37.4\n 3  47.4  76.3  71.0  27.5  36.7  21.0\n 4  41.2  43.7  35.5  22.2  45.7  40.0\n 5  44.8  53.0  35.7  13.1  22.6  39.5\n 6  60.4  29.9  40.5  34.5  57.6  59.6\n 7  45.9  58.9  34.3  46.5  21.0  40.7\n 8  29.2  39.8  39.0  21.6  40.2  69.1\n 9  72.4  45.4  48.6  29.6  46.3  46.0\n10  59.5  41.2  36.1  54.2  67.0  39.7\n# … with 3,990 more rows\n\n\n\nWe have 6 columns: one for each person. posterior_predict() does not name the columns, but they are arranged in the same order in which we specified the persons in newobs: D, D, D, R, R, R. To determine the expected age difference, we add code which works with these posterior draws:\n\n\n\npp <- posterior_predict(fit_1, newdata = newobs) |>\n    as_tibble() |>\n\n    # We don't need to rename the columns, but doing so makes the subsequest\n    # code much easier to understand. We could just have worked with columns 1,\n    # 2, 3 and so on. Either way, the key is to ensure that you correctly map\n    # the covariates in newobs to the columns in the posterior_predict object.\n  \n    set_names(c(\"dem_1\", \"dem_2\", \"dem_3\", \n                \"rep_1\", \"rep_2\", \"rep_3\")) |> \n    rowwise() |> \n  \n  # Creating three new columns. The first two are the highest age among\n  # Democrats and the lowest age among Republicans, respectively. The third one\n  # is the difference between the first two.\n  \n  mutate(dems_oldest = max(c_across(dem_1:dem_3)),\n         reps_youngest = min(c_across(rep_1:rep_3)),\n         age_diff = dems_oldest - reps_youngest)\n\npp\n\n# A tibble: 4,000 × 9\n# Rowwise: \n   dem_1 dem_2 dem_3 rep_1 rep_2 rep_3 dems_oldest reps_youngest age_diff\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>       <dbl>         <dbl>    <dbl>\n 1  44.0  41.7  70.1  52.7  26.4  38.1        70.1          26.4    43.7 \n 2  22.2  38.2  24.9  39.5  60.4  17.3        38.2          17.3    20.8 \n 3  50.9  44.1  56.7  19.9  64.5  45.2        56.7          19.9    36.8 \n 4  26.1  57.1  65.2  50.3  45.9  41.4        65.2          41.4    23.9 \n 5  57.3  40.8  42.2  41.3  36.0  35.2        57.3          35.2    22.1 \n 6  47.1  35.4  45.5  48.2  38.9  26.1        47.1          26.1    21.0 \n 7  33.8  23.5  56.7  37.8  23.6  45.4        56.7          23.6    33.2 \n 8  52.0  40.6  55.5  39.2  31.7  35.9        55.5          31.7    23.8 \n 9  46.9  32.2  27.1  52.0  46.2  40.9        46.9          40.9     5.99\n10  31.6  36.9  37.3  67.8  48.5  45.9        37.3          45.9    -8.59\n# … with 3,990 more rows\n\n\nThe plotting code is similar to what we have seen before:\n\npp |>  \n  ggplot(aes(x = age_diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Age Difference\",\n         subtitle = \"Oldest of three Democrats compared to youngest of three Republicans\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nIn words, we would expect the oldest Democrat to be about 22 years older than the youngest Republican, but we would not be too surprised if the oldest Democrat was actually younger than the youngest Republican in a group of 6.\n\n8.2.4 Addendum\nInstead of parameterizing this model without an intercept, we could have used one. In that case, the math would be:\n\\[ y_i = \\beta_0  + \\beta_1 democratic_i + \\epsilon_i\\] The interpretations of the parameters are different from the prior model. \\(\\beta_0\\) is now the average age of Republicans. This is the same interpretation as \\(\\beta_1\\) in the original set up. \\(\\beta_1\\) is now the difference between the the average age of Republicans and that of Democrats.\nTo fit this model, we use the exact same code as before, except without the -1 in the formula argument.\n\nstan_glm(age ~ party, \n         data = trains, \n         seed = 98,\n         refresh = 0)\n\nstan_glm\n family:       gaussian [identity]\n formula:      age ~ party\n observations: 115\n predictors:   2\n------\n                Median MAD_SD\n(Intercept)     42.6    1.3  \npartyRepublican -1.5    3.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 12.3    0.8  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe intercept, 42.6, is the same as the partyDemocrat estimate in the first model. The partyRepublican estimate, which was previously 41.0, is now -1.5, meaning it is the difference (allowing for rounding) between the average age of Democrats and Republicans.\nLittle else about the models will be different. They will have the same fitted values and residuals. posterior_predict() will generate the same posterior predictive probabilty distributions. Which parameterization we use does not matter much. But you should be able to interpret the meaning of the coefficients in both."
  },
  {
    "objectID": "08-three-parameters.html#att_end-treatment",
    "href": "08-three-parameters.html#att_end-treatment",
    "title": "8  Three Parameters",
    "section": "\n8.3 att_end ~ treatment",
    "text": "8.3 att_end ~ treatment\nAbove, we created a predictive model: with someone’s party affiliation, we can make a better guess as to what their age is than we could have in the absence of information about their party. There was nothing causal about that model. Changing someone’s party registration can not change their age. In this example, we build a causal model. Consider these two questions:\nWhat is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?\nWhat is the largest causal effect which still has a 1 in 10 chance of occurring?\nModels help us answer questions better than we could without those models, but only if we follow the Cardinal Virtues.\n\n8.3.1 Justice\n\n\n\n\nJustice\n\n\n\n\nThe four elements of Justice in a data science project remain the same: validity, stability, representativeness, and the model.\nFirst, we must consider the validity of the model, and to do so, let’s look at our treatment variable. What does it mean for somebody to receive the treatment? Let’s say somebody was running late on their way to the train in the morning. Well, this person may have heard the Spanish-speakers for a shorter amount of time than a commuter who arrived earlier. Similarly, some Spanish-speakers may have been speaking louder than others or some commuters may have been closer to the speakers than other commuters. Therefore, these commuters would hear the treatment of being exposed to Spanish-speakers less loudly than the other commuters. Additionally, the Spanish-speakers weren’t the same for every train station in our 2012 data and if we were to get data for now, we wouldn’t be able to hire the exact same Spanish-speakers. When considering validity with reference to treatments, what we must determine is if we can assume that the treatments of being exposed to Spanish-speakers that each commuter may experience a bit differently can be assumed to be the same.\nNext, we must consider the stability of our model for the relationship between att_end and treatment between 2012 and 2021. Is this relationship from 2012, four years before Donald Trump’s election as president, still the same? While we might not know for sure, we have to consider this in order to continue and make assumptions with our data. For our purposes, we will consider the relationship to be stable. Even though we know that there may have been some changes, we will consider the model to be the same in both years.\nLet’s now once again consider whether the data from 2012 train commuters around Boston is representative of 2012 train commuters in the US. In our last model, we discussed the issue about how Boston may be different from other cities and therefore not representative of the US, and we will now consider the issue of random sampling that may lead to representativeness issues.\nLet’s say that even though Boston is different from other US cities, we considered Boston to be perfectly representative of the US. Great, but this 2012 data could still not be representative. This is because there could be bias within those who are chosen to give the survey, in that the commuters who are approached and receive the surveys may not be random or representative. What if the individuals giving out the surveys were younger and also tended to choose people to approach with a survey that were similar in age to them? A scenario like this could end up overestimating younger train commuters in the population, which could influence our answers to any of our questions. Specifically, when considering the relationship between att_end and treatment, this could influence the results of the model because younger individuals may have similar attitudes on immigration.\n\n\nNow, let’s determine whether our DGM will be linear or logistic. Since our outcome variable att_end is a continuous variable since it has a range of possible values, we will use a linear model.\nThe math for this model is exactly the same as the math for the predictive model in the first part of this chapter, although we change the notation a bit for clarity.\n\\[ y_i = \\beta_1 treatment_i + \\beta_2 control_i + \\epsilon_i\\]\nwhere \\[treatment_i, control_i \\in \\{0,1\\}\\] \\[treatment_i +  control_i = 1\\] \\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\n\nNothing has changed, except for the meaning of the data items and the interpretations of the parameters.\nOn the left-hand side we still have the outcome, \\(y_i\\), however in this case, this is a person’s attitude toward immigration after the experiment is complete. \\(y_i\\) takes on integer values between 3 and 15 inclusive.\nOn the right-hand side, the part contained in the model will consist of the terms \\(\\beta_1 treatment_i\\) and \\(\\beta_2 control_i\\). These two terms stand for Treated and Control and as before, each term consists of a parameter and a data point. \\(\\beta_1\\) is the average attitude toward immigration for treated individuals — those exposed to Spanish-speakers — in the population. \\(\\beta_2\\) is the average attitude toward immigration for control individuals — those not exposed to Spanish-speakers — in the population.These are both our parameters. The \\(x\\)’s are our explanatory variables and take the values 1 or 0. If person \\(i\\) is Treated, \\(treatment_i = 1\\) and \\(control_i = 0\\). If person \\(i\\) is Control, \\(treatment_i = 0\\) and \\(control_i = 1\\). In other words, these are binary variables and are mutually exclusive – if you are Treated, you cannot also be Control.\nThe last part, \\(\\epsilon_i\\) (“epsilon”), represents the part that is not explained by our model and is called the error term. It is simply the difference between the outcome and our model predictions. In our particular case, this includes all factors that have an influence on someone’s attitude toward immigration but are not explained by treatment status. We assume that this error follows a normal distribution with an expected value of 0.\n\nNote that the formula applies to everyone in the population, not just the 115 people for whom we have data. The index \\(i\\) does not just go from 1 through 115. It goes from 1 through \\(N\\), where \\(N\\) is the number of individuals in the population. Conceptually, everyone has an att_end under treatment and under control.\n\n\nThe small \\(i\\)’s are an index for the data set. It is equivalent to the “ID” column in our Preceptor Table and simply states that the outcome for person \\(i\\) is explained by the predictor variables (\\(treatment\\) and \\(control\\)) for person \\(i\\), along with an error term.\n\n8.3.2 Courage\n\n\n\n\nCourage\n\n\n\n\nWith Justice satisfied, we gather our Courage and fit the model. Note that, except for the change in variable names, the code is exactly the same as it was above, in our predictive model for age. Predictive models and causal models use the same math and the same code. The differences, and they are very important, lie in the interpretation of the results, not in their creation.\n\nfit_2 <- stan_glm(att_end ~ treatment - 1, \n                      data = trains, \n                      seed = 45,\n                      refresh = 0)\n\nfit_2\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ treatment - 1\n observations: 115\n predictors:   2\n------\n                 Median MAD_SD\ntreatmentTreated 10.0    0.4  \ntreatmentControl  8.5    0.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.8    0.2   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nNote that once again, since we are using a linear model, we set the family argument to “gaussian”.\ntreatmentTreated corresponds to \\(\\beta_1\\). As always, R has, behind the scenes, estimated the entire posterior probability distribution for \\(\\beta_1\\). We will graph that distribution in the next section. But the basic print method for these objects can’t show the entire distribution, so it gives us summary numbers: the median and the MAD SD. Speaking roughly, we would expect about 95% of the values in the posterior to be within two MAD SD’s of the median. In other words, we are 95% confident that the true, but unknowable, average attitude toward immigration among the Treated in the population to be between 9.2 and 10.8.\ntreatmentControl corresponds to \\(\\beta_2\\). The same analysis applies. We are about 95% confident that the true value for the average attitude toward immigration for Control in the population is between 7.9 and 9.1.\nUp until now, we have used the Bayesian interpretation of “confidence interval.” This is also the intuitive meaning which, outside of academia, is almost universal. There is a truth out there. We don’t know, and sometimes can’t know, the truth. A confidence interval, and its associated confidence level, tells us how likely the truth is to lie within a specific range. If your boss asks you for a confidence interval, she almost certainly is using this interpretation.\nBut, in contemporary academic research, the phrase “confidence interval” is usually given a “Frequentist” interpretation. (The biggest divide in statistics is between Bayesians and Frequentist interpretations. The Frequentist approach, also known as “Classical” statistics, has been dominant for 100 years. Its power is fading, which is why this textbook uses the Bayesian approach.) For a Frequentist, a 95% confidence interval means that, if we were to apply the procedure we used in an infinite number of future situations like this, we would expect the true value to fall within the calculated confidence intervals 95% of the time. In academia, a distinction is sometimes made between confidence intervals (which use the Frequentist interpretation) and credible intervals (which use the Bayesian interpretation). We won’t worry about that difference in this Primer.\nLet’s look at the full posteriors for both \\(\\beta_1\\) and \\(\\beta_2\\).\n\n\nfit_2 |> \n  as_tibble() |> \n  select(-sigma) |> \n  pivot_longer(cols = treatmentTreated:treatmentControl,\n               names_to = \"Parameter\",\n               values_to = \"attitude\") |> \n  ggplot(aes(x = attitude, fill = Parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Attitude Toward Immigration\",\n         subtitle = \"Treated individuals are more conservative\",\n         x = \"Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_classic()\n\n\n\n\nIt appears that the affect of the treatment is to change people’s attitudes to be more conservative about immigration issues. Which is somewhat surprising!\nWe can decompose the the dependent variable, att_end into two parts: the fitted values and the residuals. There are only two possible fitted values, one for the Treated and one for the Control. The residuals, as always, are simply the difference between the outcomes and the fitted values.\n\n\n\n\n\nThe smaller the spread of the residuals, the better a job the model is doing of explaining the outcomes.\n\n\n8.3.3 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall the first question with which we began this section:\n\nWhat is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?\n\nChapter Chapter 4 defined the average treatment effect. One simple estimator of the average treatment effect is the difference between \\(\\beta_1\\) and \\(\\beta_2\\). After all, the definition of \\(\\beta_1\\) is the average attitude toward immigration, of the population, for anyone, under exposure to the treatment. So, \\(\\beta_1 - \\beta_2\\) is the average treatment effect for the population, roughly 1.5. However, estimating the posterior probability distribution for this parameter is tricky, unless we make use of the posterior distributions of \\(\\beta_1\\) and \\(\\beta_2\\). With that information, the problem is simple:\n\nfit_2 |> \n  as_tibble() |> \n  mutate(ate = treatmentTreated - treatmentControl) |> \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nCould the true value of the average treatment effect be as much as 2 or as little as 1? Of course! The most likely value is around 1.5, but the variation in the data and the smallness of our sample cause the estimate to be imprecise. However, it is quite unlikely that the true average treatment effect is below zero.\n\nWe can use posterior_epred() to answer this question. Create a tibble and use it as we have done before:\n\nnewobs <- tibble(treatment = c(\"Treated\", \"Control\"))\n\npe <- posterior_epred(fit_2, newobs) |> \n    as_tibble() |> \n    mutate(ate = `1` - `2`)\n\npe\n\n# A tibble: 4,000 × 3\n     `1`   `2`   ate\n   <dbl> <dbl> <dbl>\n 1 10.0   8.56 1.47 \n 2  9.94  8.33 1.61 \n 3 10.0   8.48 1.56 \n 4 10.6   8.28 2.36 \n 5 10.6   8.29 2.32 \n 6  9.26  8.81 0.449\n 7 10.4   8.42 1.96 \n 8 10.5   8.28 2.26 \n 9 10.8   8.51 2.25 \n10 10.3   8.37 1.88 \n# … with 3,990 more rows\n\n\nThe posterior probability distribution created with posterior_epred() is the same as the one produced by manipulating the parameters directly.\n\npe |> \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\nOur second question:\n\nWhat is the largest effect size which still has a 1 in 10 chance of occurring?\n\nCreate a tibble which we can pass to posterior_predict(). The variables in the tibble which will be passed in as newdata. Fortunately, the tibble we created above is just what we need for this question also.\nConsider the result of posterior_predict() for two people, one treated and one control. Take the difference.\n\n\npp <- posterior_predict(fit_2, \n                        newdata = newobs) |>\n    as_tibble() |>\n    mutate(te = `1` - `2`)\n  \npp\n\n# A tibble: 4,000 × 3\n     `1`   `2`    te\n   <dbl> <dbl> <dbl>\n 1  9.98  8.22  1.76\n 2  8.93  7.59  1.35\n 3 10.2   3.18  7.07\n 4 12.9   9.91  2.95\n 5 10.2   3.93  6.24\n 6 10.2   7.20  2.97\n 7  8.24 11.2  -2.94\n 8  8.91 10.7  -1.82\n 9 14.7   8.02  6.71\n10 15.9  11.5   4.32\n# … with 3,990 more rows\n\n\nCreate a graphic:\n\npp |> \n  ggplot(aes(x = te)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Treatment Effect for One Person\",\n         subtitle = \"Causal effects are more variable for indvduals\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nIn this case, we are looking at the distribution of the treatment effect for a single individual. This is very different than the average treatment effect. In particular, it is much more variable. We are looking at one row in the Preceptor Table. For a single individual, att_end can be anywhere from 3 to 15, both under treatment and under control. The causal effect — the difference between the two potential outcomes can, in theory, be anywhere from -12 to +12. Such extreme values are rare, but not impossible.\nThe question, however, was interested in the value at the 90th percentile.\n\nquantile(pp$te, prob = 0.9)\n\n     90% \n6.731598 \n\n\nWe would not expect a treatment effect of this magnitude to be common, but, at the same time, effects this big and bigger will occur about 10% of the time."
  },
  {
    "objectID": "08-three-parameters.html#income-age",
    "href": "08-three-parameters.html#income-age",
    "title": "8  Three Parameters",
    "section": "\n8.4 income ~ age",
    "text": "8.4 income ~ age\nSo far, we have only created models in which the predictor variable is discrete, with two possible values. party is either “Democrat” or “Republican”. treatment is either “Treated” or “Control”. Often times, however, the predictor variable will be continuous. Fortunately, the exact same approach works in this case. Consider:\nWhat would you expect the income to be for a 40-year old?\n\n8.4.1 Justice\nOnce again, in Justice, we must consider validity, stability, representativeness, and the data generating mechanism.\nLet’s look at the validity of age and income. Recall that validity refers to whether or not we can consider the columns age and income to have the same meaning in our data set of 2012 Boston train commuters and in our Preceptor Table. While age doesn’t really change meaning over time, income can be impacted by inflation. After all, $100,000 in 2012 doesn’t have the same worth as $100,000 now due to inflation and now would have less purchasing power. This would result in income being underestimated within our model. However, since there hasn’t been drastic inflation that dramatically changed the buying power, we will consider income to be valid. If there had been like 300% inflation, however, our conclusion would probably be different.\nNow, let’s consider the stability of our model and if we believe that our relationship between age and income has changed between 2012 and now. Once again, let’s consider inflation and how that could impact income. If incomes were to increase at the rate of inflation, then the income distribution would be different than that of 2012. However, wages don’t tend to change as quickly as inflation does, so they likely did not change significantly and we can consider this model to be stable.\nNext, let’s consider another issue that we may have with representativeness. What if we were now to assume that Boston train commuters are perfectly representative of US train commuters AND those who were approached to respond to the survey were also random? Even if this were true, we could still not assume representativeness because those who actually complete and submit the survey are not random. Instead of having more young people chosen to respond by those handing out surveys like we discussed in our last model, what if were to assume that when the surveys are handed out randomly, younger people tended to fill out and submit them more than those who are older? Well, this would still skew the age distribution and overestimate younger people in the population, and if younger people also tend to have a lower income than older people, this could also alter our answers to our current questions.\nIf we had reason to believe this is true, one way that we could fix this issue of representativeness is to alter our population to be train commuters in the US who would respond to the survey. In doing so, our population would then accommodate for the skewed age distribution under the assumption that younger individuals tend to respond to surveys at higher rates than older people.\n\n\n\n\nJustice\n\n\n\n\nThe mathematics for a continuous predictor is unchanged from the intercept-including example we explored previously.\n\n\\[y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]\nWhen comparing two people (persons 1 and 2), the first one year older than the second, \\(\\beta_1\\) is the expected difference in their incomes. The algebra is simple. Start with the two individuals.\n\\[y_1 = \\beta_0  + \\beta_1 age_1\\] \\[y_2 = \\beta_0  + \\beta_1 age_2\\] We want the difference between them, so we subtract the second from the first, performing that subtraction on both sides of the equals sign.\n\\[y_1 - y_2 = \\beta_0  + \\beta_1 age_1 - \\beta_0 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 age_1 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 (age_1 - age_2)\\]\nSo, if person 1 is one year older than person 2, we have:\n\\[y_1 - y_2 = \\beta_1 (age_1 - age_2)\\\\\ny_1 - y_2 = \\beta_1 (1)\\\\\ny_1 - y_2 = \\beta_1\\]\nThe algebra demonstrates that \\(\\beta_1\\) is the same for all ages. The difference in expected income between two people aged 23 and 24 is the same as the difference between two people aged 80 and 81. Is that plausible? Maybe. The algebra does not lie. When we create a model like this, this is the assumption we are making.\nNote how careful we are not to imply that increasing age by one year “causes” an increase in income. That is nonsense! No causation without manipulation. Since it is impossible to change someone’s age, there is only one potential outcome. With only one potential outcome, a causal effect is not defined.\n\n8.4.2 Courage\n\n\n\n\nCourage\n\n\n\n\nThe use of stan_glm() is the same as usual.\n\nfit_3 <- stan_glm(income ~ age, \n                  data = trains, \n                  seed = 28,\n                  refresh = 0)\n\nprint(fit_3, details = FALSE)\n\nstan_glm\n family:       gaussian [identity]\n formula:      income ~ age\n observations: 115\n predictors:   2\n------\n            Median   MAD_SD  \n(Intercept) 103288.5  24077.6\nage            907.8    535.8\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74121.3  4897.0\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWhen comparing two individuals, one 30 years old and one 40, we expect the older to earn about $9,000 more. But we are far from certain: the 95% confidence interval ranges from -$3,000 to $20,000.\nThe above is a good summary of the models.\n\nIt is brief! No one wants to listen to too much of your prattle. One sentence gives a number of interest. The second sentence provides a confidence interval.\nIt rounds appropriately. No one wants to hear a bunch of decimals. Use sensible units.\nIt does not just blindly repeat numbers in the printed display. A one year difference in age, which is associated with a $900 difference in income, is awkward. (We think.) A decade comparison is more sensible.\n“When comparing” is a great phrase to start the summary of any non-causal model. Avoid language like “associated with” or “leads to” or “implies” or anything which even hints at a causal claim.\n\nConsider our usual decomposition of the outcome into two parts: the model and the error term.\n\n\n\n\n\nThere are scores of different fitted values. Indeed, there are a greater number of different fitted values than there are different outcome values! This is often true for models which have continuous predictor variables, we have here with age.\n\n\n\n8.4.3 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall our question:\nWhat would you expect the income to be for a random 40 year old?\nGiven that we are looking for an expected value, we use posterior_epred().\n\nnewobs <- tibble(age = 40)\n\npe <- posterior_epred(fit_3, newdata = newobs) |> \n  as_tibble() \n\npe\n\n# A tibble: 4,000 × 1\n       `1`\n     <dbl>\n 1 142106.\n 2 141530.\n 3 141363.\n 4 139289.\n 5 139647.\n 6 142876.\n 7 139427.\n 8 143336.\n 9 145881.\n10 143052.\n# … with 3,990 more rows\n\n\nPlotting is the same as always.\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Expected Income\",\n         subtitle = \"A 40-years old commuter earns around $140,000\",\n         x = \"Income\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"
  },
  {
    "objectID": "08-three-parameters.html#liberal-income",
    "href": "08-three-parameters.html#liberal-income",
    "title": "8  Three Parameters",
    "section": "\n8.5 liberal ~ income",
    "text": "8.5 liberal ~ income\nSo far in this chapter, we have only considered continuous outcome variables. age, att_end and income all take on a variety of values. None of them are, truly, continuous, of course. age is only reported as an integer value. att_end can only, by definition, take on 13 distinct values. However, from a modeling perspective, what matters is that they have more than 2 possible values.\nliberal, however, only takes on two values: TRUE and FALSE. In order to model it, we must use the binomial family. We begin, as always, with some questions:\nAmong all people who have an income $100,000, what proportion are liberal?\nAssume we have a group of eight people, two of whom make $100,000, two $200,000, two $300,000 and two $400,000. How many will be liberal?\n\n8.5.1 Justice\nLet’s now consider Justice for our relationship between liberal and income.\nFirst, let’s look at validity, especially pertaining to liberal. As we know, this column has the values “Liberal” and “Not Liberal” to convey the political ideology of the train commuters. What we must determine if the column liberal and therefore the meaning of being liberal is the same in Boston in 2012 and Boston in 2021. If we can determine these to be the same, then we can assume validity, and in this case, because the core beliefs of being liberal have not changed very much between 2012 and 2021, we can determine that the data is valid.\nNow, let’s consider stability. To do so, we must look at the relationship between liberal and income and determine whether or not we believe that this relationship has changed between 2012 and 2021. With our knowledge of the world, do we have any reason to believe that this has changed? What if between 2012 and 2021, income increased for those who are liberal? Well, then our model for the the relationship between income and liberal would have changed over the years, and therefore so would the model. However, since with our knowledge of the world we have no reason to believe that something of the sorts happened to affect this relationship, we can consider our model to be stable.\nIn our past three models, we have considered 3 possible issues that we could have with the representativeness of our model, such as the difference between Boston and other cities, problems with random sampling, and bias in those who respond, and we will now consider how in this survey there were surveys given before and after treatment, and some people may have filled out only one of the surveys. People who only filled out one of the surveys could affect the representativeness of the data because they could not be included in the data and if those who only filled out one survey tended to be liberal, then this would affect our data because it would underestimate the amount of liberals in the survey. This is something we must consider when looking at representativeness, since we could otherwise not determine if this data from train commuters in Boston in 2012 is representative enough of train commuters in the US now to continue using our data.\nLet’s consider whether this model is linear or logistic. Unlike our previous models this chapter, the outcome variable, liberal, for this model, only has two options, “Liberal” or “Not Liberal”. Therefore, this will be logistic because there are only 2 possible outcomes and the outcome variable isn’t continuous.\nRecall the discussion in Section 7.5 about the logistic regression model which we use whenever the outcome or dependent variable is binary/logical. The math is there, if you care about math. We don’t, at least not too much. Reminder:\n\\[p(\\text{Liberal}_i = \\text{TRUE}) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}\\]\nThis model only has two parameters, \\(\\beta_0\\) and \\(\\beta_1\\). But these parameters do not have simple interpretations, unlike the parameters in a linear (or gaussian) model.\nRecall the fundamental structure of all data science problems:\n\\[\\text{outcome} = \\text{model} + \\text{what is not in the model}\\] The exact mathematics of the model — the parameters, their interpretations — are all just dross in the foundry of our inferences: unavoidable but not worth too much of our time.\nEven if the math is ignorable, the causal versus predictive nature of the model is not. Is this a causal model or a predictive model? It depends! It could be causal if you assume that we can manipulate someone’s income, if, that is, there are at least two potential outcomes: person \\(i\\)’s liberal status if she makes X dollars and person \\(i\\)’s liberal status if she makes Y dollars. Remember: No causation without manipulation. The definition of a causal effect is the difference between two potential outcomes. If you only have one outcome, then your model can not be causal.\nIn many circumstances, we don’t really care if a model is causal or not. We might only want to forecast/predict/explain the outcome variable. In that case, whether we can interpret the influence of a variable as causal is irrelevant to our use of that variable.\n\n8.5.2 Courage\nFitting a logistic model is easy. We use all the same arguments as usual, but with family = binomial added.\n\nfit_4 <- stan_glm(data = ch8,\n                  formula = liberal ~ income,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 365)\n\nHaving fit the model, we can look at a printed summary. Note the use of the digits argument to display more digits in the printout.\n\nprint(fit_4, digits = 6)\n\nstan_glm\n family:       binomial [logit]\n formula:      liberal ~ income\n observations: 115\n predictors:   2\n------\n            Median    MAD_SD   \n(Intercept)  0.562837  0.426033\nincome      -0.000006  0.000003\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nFitted models tell us about the posterior distributions for the parameters in the formula which defines the model we have estimated. We are assuming that the model is true. And, as discussed in Chapter 6, that assumption is always false! Our model is never a perfectly accurate representation of reality. But, if it were perfect, then the posterior distributions which we have created for \\(\\beta_0\\), \\(\\beta_1\\), and so on would be perfect as well.\nWhen working with a linear model, we will often interpret the meaning of the parameters, as we have already done in the first three sections of this chapter. Such interpretations are much harder with logistic models because the math is much less convenient. So, we won’t even bother to try to understand the meaning of these parameters. However, we can note that \\(\\beta_1\\) is negative, suggesting that people with higher incomes are less likely to be liberal.\n\n8.5.3 Temperance\nAmong all people who have an income $100,000, what proportion are liberal?\nAlthough our model is now logistic, all the steps in answering a question like this are the same as with a linear/guassian model.\n\nnewobs <- tibble(income = 100000)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) |> \n  as_tibble()\n\npe is a tibble with a single vector. That vector is 4,000 draws from the posterior distribution of proportion of people, among those who make $100,000, who are liberal. The population proportion is the same thing as the probability for any single individual.\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Proportion Liberal Among $100,000 Earners\",\n         subtitle = \"The population proportion is the same as the probability for any individual\",\n         x = \"Income\",\n         y = \"Probability of Being Liberal\") +\n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\nAssume we have a group of eight people, two of whom make $100,000, two $200,000, two $300,000 and two $400,000. How many will be liberal?\nBecause we are trying to predict the outcome for a small number of units, we use posterior_predict(). The more complex the questions we ask, the more care we need to devote to making the newobs tibble. We use the same rowwise() and c_across() tricks as earlier in the chapter.\n\nnewobs <- tibble(income = c(rep(100000, 2),\n                            rep(200000, 2),\n                            rep(300000, 2),\n                            rep(400000, 2)))\n                 \n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) |> \n  as_tibble() |> \n  rowwise() |> \n  mutate(total = sum(c_across()))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `total = sum(c_across())`.\nℹ In row 1.\nCaused by warning:\n! Using `c_across()` without supplying `cols` was deprecated in dplyr 1.1.0.\nℹ Please supply `cols` instead.\n\npp\n\n# A tibble: 4,000 × 9\n# Rowwise: \n     `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8` total\n   <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     0     1     0     1     1     0     0     0     3\n 2     1     0     0     0     0     1     0     0     2\n 3     1     1     1     0     0     0     0     0     3\n 4     0     1     1     0     0     0     0     0     2\n 5     0     1     0     0     1     0     0     1     3\n 6     0     1     1     0     0     0     0     0     2\n 7     1     0     0     0     0     1     0     0     2\n 8     1     1     1     0     0     0     0     0     3\n 9     1     1     0     1     0     0     0     0     3\n10     0     1     0     0     0     0     0     0     1\n# … with 3,990 more rows\n\n\nStudy the pp tibble. Understand its component parts. The first column, for example, is 4,000 draws from the posterior distribution for the liberal status of a random person with an income of $100,000. Note how all the draws are zeroes or ones. That is very different from the draws we have seen before! But it also makes sense. We are making a prediction about a binary variable, a variable which only have two possible values: zero or one. So, any (reasonable!) predictions will only be zero or one.\nThe second column is the same thing as the first column. Both are 4,000 draws from the posterior distribution for the liberal status of a random person with an income of $100,000. Yet they also have different values. They are both the same thing and different things, in the same way that rnorm(10) and rnorm(10) are the same thing — both are 10 draws from the standard normal distribution — and different things in that the values vary.\nThe third and fourth columns are different from the first two columns. They are both 4,000 draws from the posterior distribution for the liberal status of a random person with an income of $200,000. And so on for later columns. We can answer very difficult questions by putting together simple building blocks, each of them a set of draws from a posterior distribution. Recall the discussion in Section 5.1.\nThe total column is simply the sum of the first eight columns. Having created the building blocks with 8 columns of draws from four different posterior distributions, we can switch our focus to each row. Consider row 2. It has a vector of 8 numbers: 1 1 1 0 0 1 0 0. We can treat that vector as a unit of analysis. This is what might happen with our 8 people. The first three might be liberal, the fourth not liberal and so on. This row is just one example of what might happen, one draw from the posterior distribution of possible outcomes for groups of eight people with these incomes.\nWe can simplify this draw by taking the sum, or doing anything else which might answer the question with which we are confronted. Posterior distributions are as flexible as individual numbers. We can, more or less, just use algebra to work with them.\nGraphically we have:\n\npp |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Number of Liberals in Group with Varied Incomes\",\n         subtitle = \"Two is the most likely number, but values from 0 to 5 are plausible\",\n         x = \"Number of Liberals\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nAs always, there is some truth. If, tomorrow, we were to meet 8 new people, with the specified incomes, a certain number of them would be liberal. If we had the ideal Preceptor Table, we could just look up that number. No data science required. Alas, we don’t know the truth. The bets we can do is to create a posterior distribution for that unknown value, as we have done here. We then need to translate that posterior into English — “The most likely number of liberals is 2 or 3, but a total as low as zero or as high as 5 is also plausible. Having 6 liberals would be really surprising. Having 7 or 8 is almost impossible.”\nAre these two posterior probability distributions perfect? No! This is the central message of the virtue of Temperance. We must demonstrate our humility when we use our models. Recall the distinction between the unknown true distribution and the estimated distribution. The first is the posterior distribution we would create of we understood every detail of the process and could accurately model it. We would still not know the true unknown number, but our posterior distribution for that number would be perfect. Yet, our model is never perfect. We are making all sorts of assumptions behind the scenes. Some of those assumptions are plausible. Others are less so. Either way, the estimated distribution is what we have graphed above.\nThe central lesson of Temperance is: Don’t confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Recognize the unavoidable imperfections in the process. You can still use your estimated posterior — what choice do you have? — but be cautious and humble in doing so. The more that you suspect that your estimated posterior differs from the true posterior, the more humble and cautious you should be."
  },
  {
    "objectID": "08-three-parameters.html#summary",
    "href": "08-three-parameters.html#summary",
    "title": "8  Three Parameters",
    "section": "\n8.6 Summary",
    "text": "8.6 Summary\nIn this chapter, we explored relationships between different variables in the trains data set. We built three predictive models and one causal model.\nSimilar to previous chapters, our first task is to use Wisdom . We judge how relevant our data is to the questions we ask. Is it reasonable to consider the data we have (e.g., income and age data from Boston commuters in 2012) as being drawn from the same population as the data we want to have (e.g., income and age data from today for the entire US)? Probably?\nJustice is necessary to decide the best way to represent the models we make. A little math won’t kill you. We use Courage to translate our models into code. Our goal is to understand, generate posterior distributions for the parameters, and interpret their meaning. Temperance leads us to the final stage, using our models to answer our questions.\nKey commands:\n\nCreate a model with stan_glm().\nUse posterior_epred() to estimate expected values. The e in epred stands for expected.\nUse posterior_predict() to make forecasts for individuals. The variable in predictions is always greater than the variability in expectations because predictions can’t pretend that \\(\\epsilon_i\\) is zero.\n\nOnce we have draws from a posterior distribution for our outcome variable — whether that be an expectation or a prediction — we can manipulate those draws to answer our question.\nRemember:\n\nAlways explore your data.\nPredictive models care little about causality.\nPredictive models and causal models use the same math and the same code.\n“When comparing” is a great phrase to start the summary of any non-causal model.\nDon’t confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Be humble and cautious in your use of the posterior.\n\n\n\n\n\n\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on Exclusionary Attitudes.” Proceedings of the National Academy of Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111."
  },
  {
    "objectID": "09-four-parameters.html#transforming-variables",
    "href": "09-four-parameters.html#transforming-variables",
    "title": "9  Four Parameters",
    "section": "\n9.1 Transforming variables",
    "text": "9.1 Transforming variables\n\nIt is often convenient to transform a predictor variable so that our model makes more sense.\n\n9.1.1 Centering\nRecall our model of income as a function of age.\n\\[ y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]\nWe fit this using the trains data from primer.data. We will also be using a new package, broom.mixed, which allows us to tidy regression data for plotting.\n\n\n\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(broom.mixed)\n\n\nfit_1 <- stan_glm(formula = income ~ age, \n         data = trains, \n         refresh = 0,\n         seed = 9)\n\nprint(fit_1, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 103927.5  23785.9\nage            887.1    555.9\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74153.4  4905.9\n\n\nThere is nothing wrong with this model. Yet the interpretation of \\(\\beta_0\\), the intercept in the regression, is awkward. It represents the average income for people of age zero. That is useless! There are no people of zero age in our data. And, even if there were, it would be weird to think about such people taking the commuter train into Boston and filling out our survey forms.\nIt is easy, however, to transform age into a variable which makes the intercept more meaningful. Consider a new variable, c_age, which is age minus the average age in the sample. Using this centered version of age does not change the predictions or residuals in the model, but it does make the intercept easier to interpret.\n\ntrains_2 <- trains |> \n  mutate(c_age = age - mean(age))\n\nfit_1_c <- stan_glm(formula = income ~ c_age, \n                    data = trains_2, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_c, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 141922.8   6829.9\nc_age          915.0    559.2\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74234.5  4723.3\n\n\nThe intercept, 141,923, is the expected income for someone with c_age = 0, i.e., someone of an average age in the data, which is around 42.\n\n9.1.2 Scaling\nCentering — changing a variable via addition/subtraction — often makes the intercept easier to interpret. Scaling — changing a variable via multiplication/division — often makes it easier to interpret coefficients. The most common scaling method is to divide the variable by its standard deviation.\n\ntrains_3 <- trains |> \n  mutate(s_age = age / sd(age))\n\nfit_1_s <- stan_glm(formula = income ~ s_age, \n                    data = trains_3, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_s, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 103621.9  25156.0\ns_age        10975.5   7115.7\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74432.7  4950.9\n\n\ns_age is age scaled by its own standard deviation. A change in one unit of s_age is the same as a change in one standard deviation of the age, which is about 12. The interpretation of \\(\\beta_1\\) is now:\nWhen comparing two people, one about 1 standard deviation worth of years older than the other, we expect the older person to earn about 11,000 dollars more.\nBut, because we scaled without centering, the intercept is now back to the (nonsensical) meaning of the expected income for people of age 0.\n\n9.1.3 z-scores\nThe most common transformation applies both centering and scaling. The base R function scale() subtracts the mean and divides by the standard deviation. A variable so transformed is a “z-score,” meaning a variable with a mean of zero and a standard deviation of one. Using z-scores makes interpretation easier, especially when we seek to compare the importance of different predictors.\n\ntrains_4 <- trains |> \n  mutate(z_age = scale(age))\n\nfit_1_z <- stan_glm(formula = income ~ z_age, \n                    data = trains_4, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_z, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 141737.9   6877.0\nz_age        11041.8   7035.7\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74306.7  5154.8\n\n\nThe two parameters are easy to interpret after this transformation.\nThe expected income of someone of average age, which is about 42 in this study, is about 142,000 dollars.\nWhen comparing two individuals who differ in age by one standard deviation, which is about 12 years in this study, the older person is expected to earn about 11,000 dollars more than the younger.\nNote that, when using z-scores, we would often phrase this comparison in terms of “sigmas.” One person is “one sigma” older than another person means that they are one standard deviation older. This is simple enough, once you get used to it, but also confusing since we are already using the word “sigma” to mean \\(\\sigma\\), the standard deviation of \\(\\epsilon_i\\). Alas, language is something we deal with rather than control. You will hear the same word “sigma” applied to both concepts, even in the same sentence. Determine meaning by context.\n\n9.1.4 Taking logs\nIt is often helpful to take the log of predictor variables, especially in cases in which their distribution is skewed. You should generally only take the log of variables for which all the values are strictly positive. The log of a negative number is not defined. Consider the number of registered voters (rv13) at each of the polling stations in kenya.\n\nx <- kenya |> \n  filter(rv13 > 0)\n\nrv_p <- x |> \n  ggplot(aes(rv13)) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Registered Voters\",\n         y = NULL) \n\nlog_rv_p <- x |> \n  ggplot(aes(log(rv13))) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Log of Registered Voters\",\n         y = NULL) +\n    expand_limits(y = c(0, 175))\n\nrv_p + log_rv_p +\n  plot_annotation(title = 'Registered Votes In Kenya Communities',\n                  subtitle = \"Taking logs helps us deal with outliers\")\n\n\n\n\nMost experienced data scientists would use the log of rv13 rather than the raw value. Comments:\n\nWe do not know the “true” model. Who is to say that a model using the raw value is right or wrong?\nCheck whether or not this choice meaningfully affects the answer to your question. Much of the time, it won’t. That is, our inferences are often fairly “robust” to small changes in the model. If you get the same answer with rv13 as from log_rv13, then no one cares which you use.\nFollow the conventions in your field. If everyone does X, then you should probably do X, unless you have a good reason not to. If you do have such a reason, explain it prominently.\nMost professionals, when presented with data distributed like rv13, would take the log. Professionals hate (irrationally?) outliers. Any transformation which makes a distribution look more normal is generally considered a good idea.\n\nMany of these suggestions apply to every aspect of the modeling process.\n\n\n9.1.5 Adding transformed terms\nInstead of simply transforming variables, we can add more terms which are transformed versions of a variable. Consider the relation of height to age in nhanes. Let’s start by dropping the missing values.\n\nno_na_nhanes <- nhanes |> \n  select(height, age) |> \n  drop_na() \n\nFit and plot a simple linear model:\n\nnhanes_1 <- stan_glm(height ~ age,\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 47)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_1)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Children are shorter, but a linear fit is poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\nThat is not a very good model, obviously.\nAdding a quadratic term makes it better. (Note the need for I() in creating the squared term within the formula argument.)\n\nnhanes_2 <- stan_glm(height ~ age + I(age^2),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 33)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_2)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Quadratic fit is much better, but still poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\nStill, we have not made use of our background knowledge in creating these variables. We know that people don’t get any taller after age 18 or so. Let’s create variables which capture that break.\n\nnhanes_3 <- stan_glm(height ~ I(ifelse(age > 18, 18, age)),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 23)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_3)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Domain knowledge makes for better models\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\nThe point is that we should not take the variables we receive as given. We are the captains of our souls. We transform variables as needed.\n\n\n\n9.1.6 Transforming the outcome variable\nTransforming predictor variables is uncontroversial. It does not matter much. Change most continuous predictor variables to \\(z\\)-scores and you won’t go far wrong. Or keep them in their original form, and take care with your interpretations. It’s all good.\nTransforming the outcome variable is a much more difficult question. Imagine that we seek to create a model which explains rv13 from the kenya tibble. Should we transform it?\n\nMaybe? There are no right answers. A model with rv13 as the outcome variable is different from a model with log(rv13) as the outcome. The two are not directly comparable.\nMuch of the same advice with regard to taking logs of predictor variables applies here as well.\n\nSee Gelman, Hill, and Vehtari (2020)\n\n9.1.7 Interpreting coefficients\n\nWhen we interpret coefficients, it is important to know the difference between across unit and within unit comparisons. When we compare across unit, meaning comparing Joe and George, we are not looking at a causal relationship. Within unit discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying one unit under two conditions. When we talk about two potential outcomes, we are discussing the same person or unit under two conditions.\nTo put this in terms of the Preceptor tables, a within unit comparison is looking at one row of data: the data for Joe under control and the data for Joe under treatment. We are comparing one unit, or (in this case) one person, to itself under two conditions. An across unit comparison is looking at multiple rows of data, with a focus on differences across columns. We are looking at differences without making any causal claims. We are predicting, but we are not implying causation.\nThe magnitude of the coefficients in linear models are relatively easy to understand. That is not true for logistic regressions. In that case, use the Divide-by-Four rule: Take a logistic regression coefficient (other than the constant term) and divide it by 4 to get an upper bound on the predictive difference corresponding to a unit difference in that variable. All this means is that, when evaluating if a predictor is helpful in a logistic regression only, divide the coefficient by four."
  },
  {
    "objectID": "09-four-parameters.html#selecting-variables",
    "href": "09-four-parameters.html#selecting-variables",
    "title": "9  Four Parameters",
    "section": "\n9.2 Selecting variables",
    "text": "9.2 Selecting variables\nHow do we decide which variables to include in a model? There is no one right answer to this question.\n\n9.2.1 General guidelines for selecting variables\nWhen deciding which variables to keep or discard in our models, our advice is to keep a variable X if any of the following circumstances apply:\n\nThe variable has a large and well-estimated coefficient. This means, roughly, that the 95% confidence interval excludes zero. “Large” can only be defined in the context of the specific model. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.\nUnderlying theory/observation suggests that X has a meaningfully connection to the outcome variable.\nIf the variable has a small standard error relative to the size of the coefficient, it is general practice to include it in our model to improve predictions. The rule of thumb is to keep variables for which the estimated coefficient is more than two standard errors away from zero. Some of these variables won’t “matter” much to the model. That is, their coefficients, although well-estimated, are small enough that removing the variable from the model will not affect the model’s predictions very much.\nIf the standard error is large relative to the coefficient, i.e., if the magnitude of the coefficient is more than two standard errors from zero, and we find no other reason to include it in our model, we should probably remove the variable from your model.\n\n\nThe exception to this rule is if the variable is relevant to answering a question which we have. For example, if we want to know if the ending attitude toward immigration differs between men and women, we need to include gender in the model, even if its coefficient is small and closer to zero than two standard errors.\nIt is standard in your field to include X in such regressions.\nYour boss/client/reviewer/supervisor wants to include X.\n\nLet’s use the trains dataset to evaluate how helpful certain variables are to creating an effective model, based on the guidelines above.\n\n9.2.2 Variables in the trains dataset\nTo look at our recommendations in practice, let’s focus on the trains dataset. The variables in trains include gender, liberal, party, age, income, att_start, treatment, and att_end. Which variables would be best to include in a model?\n\n9.2.3 att_end ~ treatment + att_start\nFirst, let’s look at a model with a left hand variable, att_end, and two right side variables, treatment and att_start.\n\n\n\n\nfit_1_model <- stan_glm(att_end ~ treatment + att_start, \n                    data = trains, \n                    refresh = 0)\n\nfit_1_model\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ treatment + att_start\n observations: 115\n predictors:   3\n------\n                 Median MAD_SD\n(Intercept)       2.3    0.4  \ntreatmentControl -0.9    0.3  \natt_start         0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.3    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nHow do we decide which variables are useful? First, let’s interpret our coefficients.\n\nThe variable before the tilde, att_end, is our outcome.\nThe explanatory variables are treatment, which says whether a commuter relieved treatment or control conditions, and att_start, which measures attitude at the start of the study.\nThe 95% confidence interval for att_end is equal to the coefficient– 2– plus or minus two standard errors. This shows the estimate for the att_end where the commuters were under treatment and were not in the control group.\nThe variable treatmentControl represents the offset in att_end from the estimate for our (Intercept). This offset is for the group of people that were in the Control group. To find the estimated att_end for those in this group, you must add the median for treatmentControl to the (Intercept) median.\nThe variable att_start measures the expected difference in att_end for every one unit increase in att_start.\n\nThe causal effect of the variable treatmentControl is -1. This means that, compared with the predicted att_end for groups under treatment, those in the control group have a predicted attitude that is one entire point lower. As we can see, this is a large and well estimated coefficient. Recall that this means, roughly, that the 95% confidence interval excludes zero. To calculate the 95% confidence interval, we take the coefficient plus or minus two standard errors: -1.3 and -0.5. As we can see, the 95% confidence interval does exclude zero, suggesting that treatment is a worthy variable.\nIn addition to being meaningful, which is enough to justify inclusion in our model, this variable satisfies a number of other qualifications: - The variable has a small standard error. - The variable is considered an indicator variable, which separates two groups of significance (treatment and control) that we would like to study.\nThe variable att_start, with a coefficient of 1, is also meaningful. Due to the fact that the MAD_SD value here is 0, we do not need to find the 95% confidence interval to know –intuitively– that this variable has a large and well estimated coefficient.\nConclusion: keep both variables! treatment and att_start are both significant, as well as satisfying other requirements in our guidelines. They are more than worthy of inclusion in our model.\n\n9.2.4 income ~ age + liberal\nNow, we will look at income as a function of age and liberal, a proxy for political party.\n\nfit_2 <- stan_glm(income ~ age + liberal, \n                    data = trains, \n                    refresh = 0)\n\nfit_2\n\nstan_glm\n family:       gaussian [identity]\n formula:      income ~ age + liberal\n observations: 115\n predictors:   3\n------\n            Median   MAD_SD  \n(Intercept) 110298.3  23690.0\nage           1079.4    534.6\nliberalTRUE -32490.6  13481.3\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 72547.0  4894.7\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nGreat! We have an estimate for income of those who fall into the category of liberalFALSE, as well as data on our right hand side variables of age and liberalTRUE.\nFirst, let’s interpret our coefficients.\n\nThe variable before the tilde, income, is our outcome.\nThe explanatory variables are liberal, which has a resulting value of TRUE or FALSE, and age, a numeric variable.\nThe (Intercept) is estimating income where liberal == FALSE. Therefore, it is the estimated income for commuters that are not liberals and who have age = 0. The estimate for age is showing the increase in income with every additional year of age.\nThe estimate for liberalTRUE represents the offset in predicted income for commuters who are liberal. To find the estimate, we must add the coefficient to our (Intercept) value. We see that, on average, liberal commuters make less money.\n\nIt is important to note that we are not looking at a causal relationship for either of these explanatory variables. We are noting the differences between two groups, without considering causality. This is known as an across unit comparison. When we compare across unit we are not looking at a causal relationship.\nWhen comparing liberalTRUE with our (Intercept), recall that the (Intercept) is calculating income for the case where liberal == FALSE. As we can see, then, the coefficient for liberalTRUE, -32,491, shows that the liberals in our dataset make less on average than non-liberals in our dataset. The coefficient is large relative to the (Intercept) and, with rough mental math, we see that the 95% confidence interval excludes 0. Therefore, liberal is a helpful variable!\nThe variable age, however, does not appear to have a meaningful impact on our (Intercept). The coefficient of age is low and the 95% confidence interval does not exclude 0.\nConclusion: definitely keep liberal! age is less clear. It is really a matter of preference at this point.\n\n9.2.5 liberal ~ I(income/1e+05) + party\nWe will now look at liberal as a function of a transformed income and party. The reason we have transformed income here is due to the fact that, without a transformation, income makes predictions according to very small income disparities. This is not helpful. To enhance the significance of income, we have divided the variable by \\(100000\\). (Note the need for I() in creating the income term within the formula argument.)\n\nfit_3 <- stan_glm(liberal ~ I(income/1e+05) + party, \n                    data = trains, \n                    refresh = 0)\n\nfit_3\n\nstan_glm\n family:       gaussian [identity]\n formula:      liberal ~ I(income/1e+05) + party\n observations: 115\n predictors:   3\n------\n                Median MAD_SD\n(Intercept)      0.7    0.1  \nI(income/1e+05) -0.1    0.1  \npartyRepublican -0.5    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.5    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nRecall that, for logistic models, the (Intercept) is nearly impossible to interpret. To evaluate the impact of variables, we will need to use the Divide-by-Four rule. This instructs us to take a logistic regression coefficient (other than the constant term) and divide it by 4 to get an upper bound on the predictive difference corresponding to a unit difference in that variable. All this means is that, when evaluating if a predictor is helpful, in a logistic regression only, to divide the coefficient by four.\nFor partyRepublican, we take our coefficient, 0, and divide it by 4: -0.125. This is the upper bound on the predictive difference corresponding to a unit difference in this variable. Therefore, we see that partyRepublican is meaningful. We want this variable in our model.\nThe variable income, however, seems less promising. In addition to the resulting value from the Divide-by-Four rule being low (-0.023), the MAD_SD for income is also equal to the posterior median itself, indicating a standard error which is large relative to the magnitude of the coefficient. Though we may choose to include income for reasons unrelated to its impact in our model, it does not appear to be worthy of inclusion on the basis of predictive value alone.\nConclusion: the variable party is significant; we will include it in our model. Unless we have a strong reason to include income, we should exclude it.\n\n9.2.6 Final thoughts\nNow that we have looked at three cases of variables and decided whether they should be included, let’s discuss the concept of selecting variables generally.\nThe variables we decided to keep and discard are not necessarily the variables you would keep or discard, or the variables that any other data scientist would keep or discard. It is much easier to keep a variable than it is to build a case for discarding a variable. Variables are helpful even when not significant, particularly when they are indicator variables which may separate the data into two groups that we want to study.\nThe process of selecting variables – though we have guidelines – is complicated. There are many reasons to include a variable in a model. The main reasons to exclude a variable are if the variable isn’t significant or if the variable has a large standard error."
  },
  {
    "objectID": "09-four-parameters.html#comparing-models-in-theory",
    "href": "09-four-parameters.html#comparing-models-in-theory",
    "title": "9  Four Parameters",
    "section": "\n9.3 Comparing models in theory",
    "text": "9.3 Comparing models in theory\nDeciding which variables to include in a model is a subset of the larger question: How do we decide which model, out of the set of possible models, to choose?\nConsider two models which explain attitudes to immigration among Boston commuters.\n\nfit_liberal <- stan_glm(formula = att_end ~ liberal,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nprint(fit_liberal, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 10.0    0.3  \nliberalTRUE -2.0    0.5  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.7    0.2   \n\n\n\nfit_att_start <- stan_glm(formula = att_end ~ att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 85)\n\nprint(fit_att_start, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 1.6    0.4   \natt_start   0.8    0.0   \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.4    0.1   \n\n\nThey both seem like good models! The results make sense. People who are liberal have more liberal attitudes about immigration, so we would expect their att_end scores to be lower. We would also expect people to provide similar answers in two surveys administered a week or two apart. It makes sense that those with higher (more conservative) values for att_start would also have higher values for att_end.\nHow do we choose between these models?\n\n9.3.1 Better models make better predictions\nThe most obvious criteria for comparing models is the accuracy of the predictions. For example, consider the use of liberal to predict att_end.\n\ntrains |> \n  mutate(pred_liberal = fitted(fit_liberal)) |> \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Add a red circle where our predictions are most accurate (where the x and y\n  # values are the same, which is where our predictions = the true attitudes).\n  # pch = 1 makes the inside of the point translucent to show the number of\n  # correct predictions.\n  \n  geom_point(aes(x = 8, y = 8), \n             size = 20, pch = 1, \n             color = \"red\") +\n  geom_point(aes(x = 10, y = 10), \n             size = 20, pch = 1, \n             color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Liberals are less conservative\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\n\n\n\n\n\nBecause there are only two possible values for liberal — TRUE and FALSE — there are only two predictions which this model will make: about 10 for liberal == FALSE and about 8 for liberal == TRUE. (The points in the above plot are jittered.) For some individuals, these are perfect predictions. For others, they are poor predictions. the red circles on our plot illustrate the areas where our predictions are equal to true values. As we can see, the model isn’t great at predicting attitude end. (Note the two individuals who are liberal == TRUE, and who the model thinks will have att_end == 8, but who have att_end == 15. The model got them both very, very wrong.)\nConsider our second model, using att_start to forecast att_end.\n\ntrains |> \n  mutate(pred_liberal = fitted(fit_att_start)) |> \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Insert red line where our predictions = the truth using geom_abline with an\n  # intercept, slope, and color.\n  \n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Survey responses are somewhat consistent\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\n\n\n\n\nBecause att_end takes on 13 unique values, the model makes 13 unique predictions. Some of those predictions are perfect! But others are very wrong. The red line shows the points where our predictions match the truth. Note the individual with a predicted att_end of around 9 but with an actual value of 15. That is a big miss!\nRather than looking at individual cases, we need to look at the errors for all the predictions. Fortunately, a prediction error is the same thing as a residual, which is easy enough to calculate.\n\ntrains |> \n  select(att_end, att_start, liberal) |> \n  mutate(pred_lib = fitted(fit_liberal)) |> \n  mutate(resid_lib = fitted(fit_liberal) - att_end) |> \n  mutate(pred_as = fitted(fit_att_start)) |> \n  mutate(resid_as = fitted(fit_att_start) - att_end)\n\n# A tibble: 115 × 7\n   att_end att_start liberal pred_lib resid_lib pred_as resid_as\n     <dbl>     <dbl> <lgl>      <dbl>     <dbl>   <dbl>    <dbl>\n 1      11        11 FALSE      10.0    -0.974    10.6    -0.399\n 2      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n 3       5         3 TRUE        8.02    3.02      4.08   -0.916\n 4      11        11 FALSE      10.0    -0.974    10.6    -0.399\n 5       5         8 TRUE        8.02    3.02      8.16    3.16 \n 6      13        13 FALSE      10.0    -2.97     12.2    -0.769\n 7      13        13 FALSE      10.0    -2.97     12.2    -0.769\n 8      11        10 FALSE      10.0    -0.974     9.79   -1.21 \n 9      12        12 FALSE      10.0    -1.97     11.4    -0.584\n10      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n# … with 105 more rows\n\n\nLet’s look at the square root of the average squared error.\n\ntrains |> \n  select(att_end, att_start, liberal) |> \n  mutate(lib_err = (fitted(fit_liberal) - att_end)^2) |> \n  mutate(as_err = (fitted(fit_att_start) - att_end)^2) |> \n  summarize(lib_sigma = sqrt(mean(lib_err)),\n            as_sigma = sqrt(mean(as_err))) \n\n# A tibble: 1 × 2\n  lib_sigma as_sigma\n      <dbl>    <dbl>\n1      2.68     1.35\n\n\nThere are many different measures of the error which we might calculate. The squared difference is most common for historical reasons: it was the mathematically most tractable in the pre-computer age. Having calculated a squared difference for each observation, we can sum them or take their average or take the square root of their average. All produce the same relative ranking, but the last is most popular because it (more or less) corresponds to the estimated \\(\\sigma\\) for a linear model. Note how these measures are the same as the ones produced by the Bayesian models created above.\n\n\nSadly, it is not wise to simply select the model which fits the data best because doing so can be misleading. After all, you are cheating! You are using that very data to select your parameters and then, after using the data once, turning around and “checking” to see how well your model fits the data. It better fit! You used it to pick your parameters! This is the danger of overfitting.\n\n9.3.2 Beware overfitting\nOne of the biggest dangers in data science is overfitting, using a model with too many parameters which fits the data we have too well and, therefore, works poorly on data we have yet to see. Consider a simple example with 10 data points.\n\n\n\n\n\nWhat happens when we fit a model with one predictor?\n\n\n\n\n\nThat is a reasonable model. It does not fit the data particularly well, but we certainly believe that higher values of x are associated with higher values of y. A linear fit is not unreasonable.\n\nBut we can also use some of the lessons from above and try a quadratic fit by adding \\(x^2\\) as a predictor.\n\n\n\n\n\nIs this a better model? Maybe?\nBut why stop at adding \\(x^2\\) to the regression? Why not add \\(x^3\\), \\(x^4\\) and all the way to \\(x^9\\)? When we do so, the fit is much better.\n\nnine_pred <- lm(y ~ poly(x, 9),\n                       data = ovrftng)\n\nnewdata <- tibble(x = seq(1, 10, by = 0.01),\n                  y = predict(nine_pred, \n                              newdata = tibble(x = x)))\n\novrftng |> \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = newdata, \n              aes(x, y)) +\n    labs(title = \"`y` as a 9-Degree Polynomial Function of `x`\") +\n    scale_x_continuous(breaks = seq(2, 10, 2)) +\n    scale_y_continuous(breaks = seq(2, 10, 2)) \n\n\n\n\nIf the only criteria we cared about was how well the model predicts using the data on which the parameters were estimated, then a model with more parameters will always be better. But that is not what truly matters. What matters is how well the model works on data which was not used to create the model.\n\n\n9.3.3 Better models make better predictions on new data\nThe most sensible way to test a model is to use the model to make predictions and compare those predictions to new data. After fitting the model using stan_glm, we would use posterior_predict to obtain simulations representing the predictive distribution for new cases. For instance, if we were to predict how someone’s attitude changes toward immigration among Boston commuters based on political affiliation, we would want to go out and test our theories on new Boston commuters.\nWhen thinking of generalization to new data, it is important to consider what is relevant new data in the context of the modeling problem. Some models are used to predict the future and, in those cases, we can wait and eventually observe the future and check how good our model is for making predictions. Often models are used to obtain insight to some phenomenon without immediate plan for predictions. This is the case with our Boston commuters example. In such cases, we are also interested whether learned insights from on part of the data generalizes to other parts of the data. For example, if we know how political attitudes informed future immigration stances in Boston commuters, we may want to know if those same conclusions could generalize to train commuters in different locations.\nEven if we had detected clear problems with our predictions, this would not necessarily mean that there is anything wrong with the model as fit to the original dataset. However, we would need to understand it further before generalizing to other commuters.\nOften, we would like to evaluate and compare models without waiting for new data. One can simply evaluate predictions on the observed data. But since these data have already been used to fit the model parameters, these predictions are optimistic.\nIn cross validation, part of the data is used to fit the model and the rest of the data—the hold-out set—is used as a proxy for future data. When there is no natural prediction task for future data, we can think of cross validation as a way to assess generalization from one part of the data to another part.\nIn any form of cross validation, the model is re-fit leaving out one part of the data and then the prediction for the held-out part is evaluated. In the next section, we will look at a type of cross validation called leave-one-out (LOO) cross validation."
  },
  {
    "objectID": "09-four-parameters.html#comparing-models-in-practice",
    "href": "09-four-parameters.html#comparing-models-in-practice",
    "title": "9  Four Parameters",
    "section": "\n9.4 Comparing models in practice",
    "text": "9.4 Comparing models in practice\nTo compare models without waiting for new data, we evaluate predictions on the observed data. However, due to the fact that the data has been used to fit the model parameters, our predictions are often optimistic when assessing generalization.\nIn cross validation, part of the data is used to fit the model, while the rest of the data is used as a proxy for future data. We can think of cross validation as a way to assess generalization from one part of the data to another part. How do we do this?\n\nWe can hold out individual observations, called leave-one-out (LOO) cross validation; or groups of observations, called leave-one-group-out cross validation; or use past data to predict future observations, called leave-future-out cross validation. When we perform cross validation, the model is re-fit leaving out one part of the data and then the prediction for the held-out part is evaluated.\nFor our purposes, we will be performing cross validation using leave-one-out (LOO) cross validation.\n\n9.4.1 Cross validation using loo()\n\n\nTo compare models using leave-one-out (LOO) cross validation, one piece of data is excluded from our model. The model is then re-fit and makes a prediction for the missing piece of data. The difference between the predicted value and the real value is calculated. This process is repeated for every row of data in the dataset.\nIn essence: One piece of data is excluded from our model, the model is re-fit, the model attempts to predict the value of the missing piece, we compare the true value to the predicted value, and we assess the accuracy of our model’s prediction. This process occurs for each piece of data, allowing us to assess the model’s accuracy in making predictions.\nTo perform leave-one-out(LOO) cross validation, we will be using the function loo() from an R package. This is how we will determine which model is superior for our purposes.\nFirst, we will refamiliarize ourselves with our first model, fit_liberal.\n\nfit_liberal\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ liberal\n observations: 115\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 10.0    0.3  \nliberalTRUE -2.0    0.5  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.7    0.2   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nNow, we will perform loo() on our model and look at the results.\n\nloo_liberal <- loo(fit_liberal)\n\nloo_liberal\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -279.4  7.5\np_loo         2.9  0.5\nlooic       558.9 15.0\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nWhat does any of this mean?\n\n\nelpd_loo is the estimated log score along with a standard error representing uncertainty due to using only 115 data points.\n\np_loo is the estimated “effective number of parameters” in the model.\n\nlooic is the LOO information criterion, −2 elpd_loo, which we compute for comparability to deviance.\n\nFor our purposes, we mostly need to focus on elpd_loo. Let’s explain, in more depth, what this information means.\nBasically, when we run loo(), we are telling R to take a piece of data out of our dataset, re-estimate all parameters, and then predict the value for the missing piece of data. The value for elpd_loo() is based off of how close our estimate was to the truth. Therefore, elpd_loo() values inform us of the effectiveness of our model in predicting data it has not seen before.\n\nThe higher our value for elpd_loo, the better our model performs. This means that, when comparing models, we want to select the model with the higher value for elpd_loo.\n\nLet’s turn our attention to our second model. To begin, let’s observe the qualities of fit_att_start once again.\n\nfit_att_start\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ att_start\n observations: 115\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 1.6    0.4   \natt_start   0.8    0.0   \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.4    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nGreat! Now, let’s perform loo() on this model.\n\nloo_att_start <- loo(fit_att_start) \n\nloo_att_start\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -201.7 12.5\np_loo         4.2  1.8\nlooic       403.4 25.1\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nThe elpd_loo value for this model is -201.7. This is higher than the elpd_loo for att_liberal, implying that this model is superior. However, we can’t see our estimates together. Is there a simpler way to calculate which model is better?\nActually, yes! Using the function loo_compare(), we can compare the models directly.\n\n9.4.2 Comparing models using loo_compare()\n\nTo compare the two models directly, we can use the function loo_compare with our two loo objects created above. This will calculate the difference in elpd_loo() between our models for us, making our job easier:\n\nloo_compare(loo_att_start, loo_liberal)\n\n              elpd_diff se_diff\nfit_att_start   0.0       0.0  \nfit_liberal   -77.7      12.0  \n\n\nThe value for elpd_diff is equal to the difference in elpd_loo between our two models. These_diff shows that the difference in standard error.\nTo interpret the results directly, it is important to note that the first row will be the superior model. The values of elpd_diff and att_start will be 0, as these columns show the offset in the estimates compared to the better model. To reiterate: when the better model is compared to itself, those values will be 0. The following rows show the offset in elpd and se values between the less effective model, fit_liberal, and the more effective model, fit_att_start.\nThe better model is clear: fit_att_start. Therefore, the attitude at the start of the trains study is more significant to predicting final attitude when compared with the variable liberal, which is an analog for political affiliation.\nAs we have seen, loo_compare is a shortcut for comparing two models. When you are deciding between two models, loo_compare() is a great way to simplify your decision.\nWhat do we do when the value of loo_compare() is small? As a general practice, differences smaller than four are hard to distinguish from noise. In other words: when elpd_diff is less than 4, there is no advantage to one model over the other."
  },
  {
    "objectID": "09-four-parameters.html#testing-is-nonsense",
    "href": "09-four-parameters.html#testing-is-nonsense",
    "title": "9  Four Parameters",
    "section": "\n9.5 Testing is nonsense",
    "text": "9.5 Testing is nonsense\nAs always, it is important to look at the practices of other professionals and the reasons we may choose not to follow those tactics. For instance, our continued problem with hypothesis testing. In hypothesis testing, we assert a null hypothesis \\(H_0\\) about our data and an alternative hypothesis \\(H_a\\).\nWhen performing hypothesis testing, we either reject the hypothesis or we do not reject it. The qualifications for rejecting are met if the 95% confidence interval excludes the null hypothesis. If the hypothesis is included in our 95% confidence interval, we do not reject it. In the case of “insignificant” results, with p > 0.5, we also can’t “reject” the null hypothesis. However, this does not mean that we accept it.\nThe premise of hypothesis testing is to answer a specific question – one that may not even be particularly relevant to our understanding of the world – about our data. So, what are our problems with hypothesis testing? - Rejecting or not rejecting hypotheses doesn’t helps us to answer real questions. - The fact that a difference is not “significant” has no relevance to how we use the posterior to make decisions. - Statistical significance is not equal to practical importance. - There is no reason to test when you can summarize by providing the full posterior probability distribution."
  },
  {
    "objectID": "09-four-parameters.html#parallel-lines",
    "href": "09-four-parameters.html#parallel-lines",
    "title": "9  Four Parameters",
    "section": "\n9.6 Parallel lines",
    "text": "9.6 Parallel lines\nTo conclude this chapter, we will look at a four parameter model. The model we will use will measure att_end as a function of liberal or att_start or treatment or some combination of these variables.\n\n\n\n9.6.1 Wisdom\n\n\n\n\nWisdom\n\n\n\n\nBefore making a model which seeks to explain att_end, we should plot it and the variables we think are connected to it:\n\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  geom_point() +\n  labs(title = \"Attitude End Compared with Attitude Start and Liberal\",\n       x = \"Attitude at Start of Study\",\n       y = \"Attitude at End of Study\",\n       color = \"Liberal?\")\n\n\n\n\nIs that data believable? Maybe? One could imagine that att_end would be predicted fairly well by att_start. This makes sense for most of our data points, which show not much difference between the attitudes. But what about the great disparities in attitude shown for the individual with a starting attitude of 9 and an ending attitude around 15? In a real data science project, this would require further investigation. For now, we ignore the issue and blithely press on.\nAnother component of Wisdom is the population. The concept of the “population” is subtle and important. The population is not the set of commuters for which we have data. That is the dataset. The population is the larger — potentially much larger — set of individuals about whom we want to make inferences. The parameters in our models refer to the population, not to the dataset.\nThere are many different populations, each with its own \\(\\mu\\), in which we might be interested. For instance:\n\nThe population of Boston commuters on the specific train and time included in our dataset.\nThe population of all Boston commuters.\nThe population of commuters in the United States.\n\nAll of these populations are different, so each has a different \\(\\mu\\). Which \\(\\mu\\) we are interested in depends on the problem we are trying to solve. It is a judgment call, a matter of Wisdom, as to whether or not that data we have is “close enough” to the population we are interested in to justify making a model.\nThe major part of Wisdom is deciding what questions you can’t answer because of the data you don’t have.\n\n9.6.2 Justice\n\n\n\n\nJustice\n\n\n\n\nNow that we have considered the connection between our data and the predictions we seek to make, we will need to consider our model.\nFirst: is our model causal or predictive? Recall that our model measures att_end as a function of liberal and att_start. The variables liberal and att_start do not involve a control or treatment dynamic. There is no manipulation with these variables. Given that there is no causation without manipulation, this is a predictive model.\nWe are making inferences about groups of people according to their political affiliation and starting attitude. We are not measuring causality, but we are predicting outcomes.\nWhen creating a parallel slops model, we use the basic equation of a line:\n\\[y_i = \\beta_0  + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}\\]\nIf \\(y = att\\_end\\), \\(x_1 = att\\_start\\), \\(x\\_2 = liberal\\), then the equations are as follows:\nIf liberal = FALSE:\n\\[y_i = \\beta_0  + \\beta_1 x_{1,i}\\] Which equates to, in y = b + mx form:\n\\[y_i = intercept +  \\beta_1 att\\_start_i\\]\nIf liberal = TRUE:\n\\[y_i = (\\beta_0  + \\beta_2) + \\beta_1 x_{1,i}\\]\nWhich equates to, in y = b + mx form:\n\\[y_i = (intercept + liberal\\_true) + \\beta_1 att\\_start_i\\]\n\n\n9.6.3 Courage\n\n\n\n\nCourage\n\n\n\n\nThe use of stan_glm is the same as usual. Using stan_glm, we will create our model, fit_1.\n\nfit_1 <- stan_glm(formula = att_end ~ liberal + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nfit_1\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ liberal + att_start\n observations: 115\n predictors:   3\n------\n            Median MAD_SD\n(Intercept)  1.9    0.5  \nliberalTRUE -0.3    0.3  \natt_start    0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.4    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nTo remind ourselves, recall that the (Intercept) here is representing the att_end value for cases where liberal = FALSE and treatment does not equal Control. The next row, liberalTRUE gives a median value which represents the offset in the prediction compared with the (Intercept). In other words, the true intercept for cases where liberal = TRUE is represented by \\((Intercept) + liberalTRUE\\). The value of treatmentControl is the offset in att_end for those in the Control group.\nTo find our intercepts, we will need to tidy our regression using the tidy() function from the broom.mixed package. We tidy our data and extract values to create a parallel slopes model. The parallel slopes model allows us to visualize multi-variate Bayesian modeling (i.e. modeling with more than one explanatory variable). That is a complicated way of saying that we will visualize the fitted model created above in a way that allows us to see the intercepts and slopes for two different groups, liberalTRUE and liberalFALSE:\n\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_1 |> \n  tidy() |> \n  select(term, estimate)\n\ntidy\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    1.93 \n2 liberalTRUE   -0.300\n3 att_start      0.799\n\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nliberal_true <- tidy$estimate[2]\natt_start <- tidy$estimate[3]\n\nNow, we can define the following terms—- liberal_false_intercept and liberal_false_att_slope; and liberal_true_intercept and liberal_true_att_slope:\n\n# Recall that the (Intercept) shows us the estimate for the case where liberal =\n# FALSE. We want to extract the liberal_false_intercept to indicate where the\n# intercept in our visualization should be. The slope for this case, and for the\n# liberal = TRUE case, is att_start.\n\nliberal_false_intercept <- intercept\nliberal_false_att_slope <- att_start\n\n#  When wanting the intercept for liberal = TRUE, recall that the estimate for\n#  liberalTRUE is the offset from our (Intercept). Therefore, to know the true\n#  intercept, we must add liberal_true to our intercept.\n\nliberal_true_intercept <- intercept + liberal_true\nliberal_true_att_slope <- att_start\n\nAll we’ve done here is extracted the values for our intercepts and slopes, and named them to be separated into two groups. This allows us to create a geom_abline object that takes a unique slope and intercept value, so we can separate the liberalTRUE and liberalFALSE observations.\n\n# From the dataset trains, use att_start for the x-axis and att_end for\n# the y-axis with color as liberal. This will split our data into two color\n# coordinates (one for liberal = TRUE and one for liberal = FALSE)\n\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the liberal false values. Set the intercept\n  # equal to our previously created liberal_false_intercept, while setting slope\n  # equal to our previously created liberal_false_att_slope. The color call is\n  # for coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = liberal_false_intercept,\n              slope = liberal_false_att_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the liberal TRUE values. Set the intercept\n  # equal to our previously created liberal_true_intercept, while setting slope\n  # equal to our previously created liberal_true_att_slope. The color call is\n  # for teal, to match the colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = liberal_true_intercept,\n              slope = liberal_true_att_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Parallel Slopes Model\",\n       x = \"Attitude at Start\", \n       y = \"Attitude at End\", \n       color = \"Liberal\") \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThis is our parallel slopes model. What we have done, essentially, is created a unique line for liberalTRUE and liberalFALSE to observe the differences in the groups as related to attitude start and attitude end.\nAs we can see, commuters who are not liberal tend to start with slightly higher values for att_start. Commuters who are liberal tend to have lower starting values for att_start.\nNow, what if we want to look at another model? To judge whether we can have a superior model? Let’s create another object, using stan_glm, that looks at att_end as a function of treatment and att_start.\n\nfit_2 <- stan_glm(formula = att_end ~ treatment + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 56)\n\nfit_2\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ treatment + att_start\n observations: 115\n predictors:   3\n------\n                 Median MAD_SD\n(Intercept)       2.4    0.4  \ntreatmentControl -1.0    0.2  \natt_start         0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.3    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nTo interpret briefly:\n\n(Intercept) here is representing the att_end value for cases where treatment does not equal Control.\ntreamtmentControl gives a median value which represents the offset in the prediction compared with the (Intercept). In other words, the true intercept for cases where treatment = Control is represented by \\((Intercept) + treatmentControl\\).\natt_start represents the slope for both groups representing a unit change.\n\nAn important point here is that these models are causal. When including the variable treatment, we have a measured causal effect of a condition. This is different from our prior parallel slopes model, where we were modeling for prediction, not causation.\n\nTo see how these models compare in performance, we will perform leave-one-out (LOO) cross validation again.\n\nL1 <- loo(fit_1)\n\nL1\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -202.3 13.1\np_loo         5.5  2.3\nlooic       404.6 26.1\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     114   99.1%   2531      \n (0.5, 0.7]   (ok)         1    0.9%   157       \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nPerform loo() on our second model:\n\nL2 <- loo(fit_2)\n\nL2\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -195.3 12.2\np_loo         5.1  1.8\nlooic       390.5 24.5\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     114   99.1%   1625      \n (0.5, 0.7]   (ok)         1    0.9%   206       \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nRecall that the relevant data is the data from elpd_loo. The estimates for elpd_loo vary quite a bit. Recall that the superior model will have a value of elpd_loo() that is closer to 0. The standard error (SE) for these models also differs some. To compare these directly, we will use loo_compare.\n\nloo_compare(L1, L2)\n\n      elpd_diff se_diff\nfit_2  0.0       0.0   \nfit_1 -7.0       3.8   \n\n\nRecall that, with loo_compare(), the resulting data shows the superior model first, with values of 0 for elpd_diff and se_diff, since it compares the models to the best option. The values of elpd_diff and se_diff for fit_1 show the difference in the models. As we can see, fit_2, the model which looks at treatment + att_start, is better.\nBut how certain can we be that it is better? Note that the difference between the two models is not quite two standard errors. So, there is a reasonable possibility that the difference is due to chance.\n\n\n9.6.4 Temperance\nWhat we really care about is data we haven’t seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn’t change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that are forecasts are more uncertain that a naive use of our model might suggest. Having created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.\nPreceptor’s Posterior is the posterior you would calculate if all the assumptions you made under Wisdom and Justice were correct. Sadly, they never are! So, you can never know Preceptor’s Posterior. Our posterior will, we hope, be a close-ish approximation of Preceptor’s Posterior."
  },
  {
    "objectID": "09-four-parameters.html#summary",
    "href": "09-four-parameters.html#summary",
    "title": "9  Four Parameters",
    "section": "\n9.7 Summary",
    "text": "9.7 Summary\nIn this chapter, we covered a number of topics important to effectively creating models.\nKey commands: - Create a model using stan_glm(). - After creating a model, we can use loo() to perform leave-one-out cross validation. This assesses how effectively our model makes predictions for data it has not seen yet. - The command loo_compare() allows us to compare two models, to see which one performs better in leave-one-out cross validation. The superior model makes better predictions.\nRemember: - We can transform variables – through centering, scaling, taking logs, etc. – to make them more sensible. Consider using a transformation if the intercept is awkward. For instance, if the intercept for age represents the estimate for people of age zero, we might consider transforming age to be easier to interpret. - When selecting variables to include in our model, follow this rule: keep it if the variable has a large and well-estimated coefficient. This means that the 95% confidence interval excludes zero. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model. - When we compare across unit, meaning comparing Joe and George, we are not looking at a causal relationship. Within unit discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying one unit under two conditions. - When we talk about two potential outcomes, we are discussing the same person or unit under two conditions.\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press. https://doi.org/10.1017/9781139161879."
  },
  {
    "objectID": "10-five-parameters.html#wisdom",
    "href": "10-five-parameters.html#wisdom",
    "title": "10  Five Parameters",
    "section": "\n10.1 Wisdom",
    "text": "10.1 Wisdom\n\n\n\n\nWisdom\n\n\n\n\nRecall the most important aspects of Wisdom: the Preceptor Table, the EDA (exploratory data analysis), the population, and the Population Table. As always, we start with the Preceptor Table — the table of data that would make all of our questions answerable with mere arithmetic (no inferences).\n\n10.1.1 The Preceptor Table\nTo create our Preceptor Table, we must first revisit our questions:\nHow many years would we expect two gubernatorial candidates — one male and one female, both 10 years older than the average candidate — to live after the election? How different will their lifespans be? More broadly, how long do candidates, in general, live after the election? Does winning the election affect their longevity?\nWhat (imagined) dataset would make all of these questions easy to solve with a little bit of math? Well, we obviously need data on all gubernatorial candidate elections in the United States. We also need to know their dates of birth, age at time of election, age at time of death, and data for age at time of death minus age at time of election. With these pieces of information, we could answer all of our questions with simple math.\nAlso, because this is an idealized table, we would know age at time of death assuming victory and age at time of death assuming loss. This would not be possible in the real world due to the Fundamental Problem of Causal Inference — we cannot observe a unit under two different conditions (both victory and loss).\nHere is a sample:\n\n\n\n\n\n\n\nID\n      Sex\n      Age at Election\n      Years Lived (Win)\n      Years Lived (Loss)\n      Treatment Effect of Winning\n    \n\n\nCandidate 1\nF\n56\n12\n9\n+3\n\n\nCandidate 2\nM\n72\n7\n5\n+2\n\n\n\n\n\n\nWe would have rows for every gubernatorial election candidate in U.S. history. We may want further details, such as election year. This is merely a sketch of our ideal dataset. Now that we know our “perfect” reality, what data do we actually have to work with?\n\n10.1.2 EDA of governors\n\n\n\nThe primer.data package includes the governors data set which features demographic information about candidates for governor in the United States. Barfort, Klemmensen, and Larsen (2020) gathered this data and concluded that winning a gubernatorial election increases a candidate’s lifespan.\n\nglimpse(governors)\n\nRows: 1,092\nColumns: 14\n$ state        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n$ year         <int> 1946, 1946, 1950, 1954, 1954, 1958, 1962, 1966, 1966, 197…\n$ first_name   <chr> \"James\", \"Lyman\", \"Gordon\", \"Tom\", \"James\", \"William\", \"G…\n$ last_name    <chr> \"Folsom\", \"Ward\", \"Persons\", \"Abernethy\", \"Folsom\", \"Long…\n$ party        <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Republican\", \"Demo…\n$ sex          <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ died         <date> 1987-11-21, 1948-12-17, 1965-05-29, 1968-03-07, 1987-11-…\n$ status       <chr> \"Challenger\", \"Challenger\", \"Challenger\", \"Challenger\", \"…\n$ win_margin   <dbl> 77.334394, -77.334394, 82.206564, -46.748166, 46.748166, …\n$ region       <chr> \"South\", \"South\", \"South\", \"South\", \"South\", \"South\", \"So…\n$ population   <dbl> 2906000, 2906000, 3058000, 3014000, 3014000, 3163000, 332…\n$ election_age <dbl> 38.07255, 78.54894, 48.74743, 46.54620, 46.07255, 33.2703…\n$ death_age    <dbl> 79.11567, 80.66530, 63.31006, 59.88227, 79.11567, 87.8193…\n$ lived_after  <dbl> 41.043121, 2.116359, 14.562628, 13.336071, 33.043121, 54.…\n\n\n\n\nThere are 14 variables and 1,092 observations. In this Chapter, we will only be looking at the variables last_name, year, state, sex, lived_after, election_age, won, and close_race.\n\nch10 <- governors |> \n  mutate(won = if_else(win_margin > 0, TRUE, FALSE)) |> \n  mutate(close_race = if_else(abs(win_margin) < 5, TRUE, FALSE)) |> \n  select(last_name, year, state, sex, lived_after, election_age, won, close_race)\n\nelection_age and lived_after are how many years a candidate lived before and after the election, respectively. As a consequence, only politicians who are already deceased are included in this data set. This means that there are only a handful of observations from elections in the last 20 years. Most candidates from that time period are still alive and are, therefore, excluded. We created the won variable to indicate whether or not the candidate won the election. We define close_race to be true if the winning margin was less than 5%.\nOne subtle issue: Should the same candidate be included multiple times? For example:\n\nch10 |> \n  filter(last_name == \"Cuomo\")\n\n# A tibble: 4 × 8\n  last_name  year state    sex   lived_after election_age won   close_race\n  <chr>     <int> <chr>    <chr>       <dbl>        <dbl> <lgl> <lgl>     \n1 Cuomo      1982 New York Male         32.2         50.4 TRUE  TRUE      \n2 Cuomo      1986 New York Male         28.2         54.4 TRUE  FALSE     \n3 Cuomo      1990 New York Male         24.2         58.4 TRUE  FALSE     \n4 Cuomo      1994 New York Male         20.2         62.4 FALSE TRUE      \n\n\nFor now, we leave in multiple observations for a single person.\nFirst, let’s sample from our dataset.\n\n\n\n\nch10 |> \n  slice_sample(n = 5)\n\n# A tibble: 5 × 8\n  last_name  year state        sex    lived_after election_age won   close_race\n  <chr>     <int> <chr>        <chr>        <dbl>        <dbl> <lgl> <lgl>     \n1 Ristine    1964 Indiana      Male          44.6         44.8 FALSE FALSE     \n2 Sundlun    1986 Rhode Island Male          24.7         66.8 FALSE FALSE     \n3 Richards   1990 Texas        Female        15.9         57.2 TRUE  TRUE      \n4 Turner     1946 Oklahoma     Male          26.6         52.0 TRUE  FALSE     \n5 Williams   1948 Michigan     Male          39.2         37.7 TRUE  FALSE     \n\n\nAs we might expect, sex is most often “Male”. To be more precise in inspecting our data, let’s skim() the dataset.\n\nskim(ch10)\n\n\nData summary\n\n\nName\nch10\n\n\nNumber of rows\n1092\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nlogical\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nlast_name\n0\n1\n3\n11\n0\n615\n0\n\n\nstate\n0\n1\n4\n14\n0\n50\n0\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\nwon\n0\n1\n0.53\nTRU: 576, FAL: 516\n\n\nclose_race\n0\n1\n0.23\nFAL: 838, TRU: 254\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n1964.85\n13.38\n1945.00\n1954.00\n1962.00\n1974.00\n2011.00\n▇▆▃▂▁\n\n\nlived_after\n0\n1\n28.23\n13.38\n0.13\n17.57\n29.60\n38.67\n60.42\n▃▆▇▆▂\n\n\nelection_age\n0\n1\n51.72\n8.71\n31.35\n45.34\n51.36\n57.48\n83.87\n▂▇▆▂▁\n\n\n\n\n\nskim() groups the variable together by type, and provides some analysis for each variable. We are also given histograms of the numeric data.\nLooking at the histogram for year, we see that it is skewed right — meaning that most of the data is bunched to the left and that there is a smaller tail to the right — with half of the observations from election years between 1945 and 1962. This makes sense logically, because we are only looking at deceased candidates, and candidates from more recent elections are more likely to still be alive.\nIn using this data set, our left-side variable will be lived_after. We are trying to understand/predict how many years a candidate will live after the election.\n\nch10 |>\n  ggplot(aes(x = year, y = lived_after)) +\n  geom_point() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Candidates who died more recently can't have lived for long post-election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Year Elected\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \n\n\n\n\nNote that there is a rough line above which we see no observations. Why might this be? When looking at the year elected and years lived post-election, there is missing data in the upper right quadrant due to the fact that it is impossible to have been elected post-2000 and lived more than 21 years. Simply put: this “edge” of the data represents, approximately, the most years a candidate could have lived, and still have died, given the year that they were elected.\nThe reason the data is slanted downward is because the maximum value for this scenario is greater in earlier years. That is, those candidates who ran for governor in earlier years could live a long time after the election and still have died prior to the data set creation, giving them higher lived_after values than those who ran for office in more recent years. The edge of the scatter plot is not perfectly straight because, for many election years, no candidate had the decency to die just before data collection. The reason for so few observations in later years is that fewer recent candidates have died.\nTo begin visualizing our lived_after data, we will inspect the difference in years lived post election between male and female candidates.\n\nch10 |>\n  ggplot(aes(x = sex, y = lived_after)) +\n  geom_boxplot() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Male candidates live much longer after the election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Gender\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \n\n\n\n\nThis plot shows that men live much longer, on average, than women after the election. Is there an intuitive explanation for why this might be?\n\n10.1.3 Population\nThe concept of the “population” is subtle and important. The population is not the set of candidates for which we have data. That is the dataset. The population is the larger — potentially much larger — set of individuals about whom we want to make inferences. The parameters in our models refer to the population, not to the dataset.\nConsider a simple example. Define \\(\\mu\\) as the average number of years lived by candidates for governor after Election Day. Can we calculate \\(\\mu\\) from our data? No! There are many candidates for governor who are still alive, who are not included in our data even though they are part of the “population” we want to study. \\(\\mu\\) can not be calculated. It can only be estimated.\nAnother problem is that we would like to estimate the effect of winning on lifespan in present day. Because our data excludes the most recent candidates (since they are still alive), our predictions will not mirror the future as well as we may hope.\nEven though the original question is about “gubernatorial candidates” in general, and does not specifically refer to the United States, we will assume that the data we have for US governors is representative enough of the population we are interested in (global politicians) that the exercise is useful. If we did not believe that, then we should stop right now. The major part of Wisdom is deciding what questions you can’t answer because of the data you just don’t have.\nThe truth is: in the social sciences, there is never a perfect relationship between the data you have and the question you are trying to answer. Data for gubernatorial candidates in the past is not an analog for gubernatorial candidates today. Nor is it the same as the data for candidates in other countries. Yet, this data is relevant. Right? It is certainly better than nothing.\nGenerally speaking, using not-perfect data is better than using no data at all.\nOf course, this is not always true. If we wanted to predict lifespans of gubernatorial candidates in the U.S., and our data was from lifespans of presidential candidates in France… we would be better off not making any predictions at all. If the data won’t help, don’t use the data."
  },
  {
    "objectID": "10-five-parameters.html#justice",
    "href": "10-five-parameters.html#justice",
    "title": "10  Five Parameters",
    "section": "\n10.2 Justice",
    "text": "10.2 Justice\n\n\n\n\nJustice\n\n\n\n\nAfter inspecting our data and deciding that it is “close enough” to our questions to be useful, we move on to Justice.\nJustice emphasizes a few key concepts:\n\nThe Population Table, a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we want to have (the actual data set), the data for units which we actually have (the Preceptor Table), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\nIs our data representative of the population?\nIs the meaning of the columns consistent, i.e., can we assume validity? We then make an assumption about the data generating mechanism.\n\nWe inspect both representativeness and validity through our Population Table. Representativeness focuses on the rows of the table, while validity focuses on the columns.\n\n10.2.1 Population Table\nBy determining that the data is drawn from the same population which we are analyzing, we can go on to produce a Population Table.\nThe Population Table shows rows from three sources: the Preceptor Table, the actual data, and the population (outside of the data).\nOur Preceptor Table rows contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (sex, election_age, year elected) but they do not contain any outcome results (lived_after). We are trying to answer questions about the train commuter population in 2021, so our year entries of these rows will read “2021”.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from gubernatorial candidates who are deceased. All columns (covariates and outcomes) will be complete.\nOur population rows contain no data. These are subjects which fall under our desired population, but for which we have no data. As such, all rows are missing.\n\ntibble(source = c(\"Population\", \"Population\", \"...\",\n                  \"Data\", \"Data\", \"...\",\n                  \"Population\", \"Population\", \"...\",\n                  \"Preceptor Table\", \"Preceptor Table\", \"...\",\n                  \"Population\", \"Population\"),\n       year_elected = c(\"1912\", \"1924\", \"...\",\n                \"1967\", \"2004\", \"...\",\n                \"2018\", \"2019\", \"...\",\n                \"2021\", \"2021\", \"...\",\n                \"2035\", \"2040\"),\n       election_age = c(\"?\", \"?\", \"...\",\n                        \"43\", \"67\", \"...\",\n                        \"?\", \"?\", \"...\",\n                        \"46\", \"39\", \"...\",\n                        \"?\", \"?\"),\n       sex = c(\"?\", \"?\", \"...\",\n               \"Male\", \"Female\", \"...\",\n               \"?\", \"?\", \"...\",\n               \"Female\", \"Female\", \"...\",\n                 \"?\", \"?\"),\n       lived_after = c(\"?\", \"?\", \"...\", \n                 \"20\", \"19\", \"...\",\n                 \"?\", \"?\", \"...\",\n                 \"?\", \"?\", \"...\",\n                 \"?\", \"?\")) |>\n  \n  # Then, we use the gt function to make it pretty\n  \n  gt() |>\n  cols_label(source = md(\"Source\"),\n             year_elected = md(\"Year\"),\n             election_age = md(\"Election Age\"),\n             sex = md(\"Sex\"),\n             lived_after = md(\"Years Lived After\")) \n\n\n\n\n\n\nSource\n      Year\n      Election Age\n      Sex\n      Years Lived After\n    \n\n\nPopulation\n1912\n?\n?\n?\n\n\nPopulation\n1924\n?\n?\n?\n\n\n...\n...\n...\n...\n...\n\n\nData\n1967\n43\nMale\n20\n\n\nData\n2004\n67\nFemale\n19\n\n\n...\n...\n...\n...\n...\n\n\nPopulation\n2018\n?\n?\n?\n\n\nPopulation\n2019\n?\n?\n?\n\n\n...\n...\n...\n...\n...\n\n\nPreceptor Table\n2021\n46\nFemale\n?\n\n\nPreceptor Table\n2021\n39\nFemale\n?\n\n\n...\n...\n...\n...\n...\n\n\nPopulation\n2035\n?\n?\n?\n\n\nPopulation\n2040\n?\n?\n?\n\n\n\n\n\n\nAgain, the Population Table shows the more expansive population for which we are making assumptions — this includes data from our “population”, our actual data, and the Preceptor Table.\n\n10.2.2 Validity\nValidity, on the other hand, involves our columns. To put it simply, does the column for lifespan in our Preceptor Table equate to the column for lifespan from our dataset. Again, we look to the source of our data: Barfort, Klemmensen, and Larsen (2020).\nThe collection of birth and death dates for winning candidates is well documented. The birth and death dates for losing candidates, however, is not as easily gathered. In fact, Barfort, Klemmensen, and Larsen (2020) had to perform independent research for this information:\n\n“For losing candidates, we use information gathered from several online sources, including Wikipedia, The Political Graveyard…, Find a Grave… and Our Campaigns.”\n\nThis is not nearly as reliable as the data collection for candidates who won their election. And, there was a further complication:\n\n“In a few cases, we are only able to identify the year of birth or death, not the exact date of the event. For these candidates, we impute the date as July 1 of the given year.”\n\nFor these candidates, then, our estimate for longevity will be inaccurate. We also have to hope that the birth and death dates listed on unreliable internet sources are accurate. It is possible that they are not, especially for older candidates.\nThe mission of this exploration is to ensure validity as much as possible — that is, to equate our columns when they are not equated themselves. In this case, because we cannot fix the issues with data collection, we accept that our estimates may be slightly skewed.\n\n10.2.3 Stability\nStability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn.\nWith an outcome variable such as height, it is easier to assume stability over a greater period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. Can we say the same for this example, where we are looking at years lived post-election?\nLifespan changes over time. In fact, between 1960 and 2015, life expectancy for the total population in the United States increased by almost 10 years from 69.7 years in 1960 to 79.4 years in 2015. Therefore, our estimates for the future may need some adjustment — that is, to add years to our predicted life expectancy to account for a global change in lifespan over time.\nWhen we are confronted with this uncertainty, we can consider making our timeframe smaller. After all, if we confined the data to candidates post-1980, we would expect more stability in lifespan. This modification may be appropriate, but it limits our data. Stability, in essence, allows us to ignore the issue of time.\nAlternatively, if we believe that it is unlikely that our columns are stable, we have two choices. First, we abandon the experiment. If we believe our data is useless, so is our experiment. Second, we can choose to provide a sort of warning message with our conclusions: this is based on data from ten years ago, but that was the most recent data available to us.\n\n10.2.4 Representativeness\nThe external validity of a study is often directly related to the representativeness of our sample. Representativeness has to do with how well our sample represents the larger population we are interested in generalizing to.\nAfter looking at Barfort, Klemmensen, and Larsen (2020), the source for our dataset, we see that:\n“We collect data… for all candidates running in a gubernatorial election from 1945 to 2012. We limit attention to the two candidates who received the highest number of votes.”\nThis data is, then, highly representative of gubernatorial candidates, as it includes every candidate from 1945 to 2012. However, there is one large caveat: only the two candidates with the most votes are included in the dataset. This is unfortunate, as we would ideally look at all gubernatorial candidates (regardless of votes). Regardless, we still deem the dataset to be representative enough of our larger population.\nGenerally: if there was no chance that a certain type of person would have been in this experiment, we cannot make an assumption for that person."
  },
  {
    "objectID": "10-five-parameters.html#courage",
    "href": "10-five-parameters.html#courage",
    "title": "10  Five Parameters",
    "section": "\n10.3 Courage",
    "text": "10.3 Courage\n\n\n\n\nCourage\n\n\n\n\n\n10.3.1 sex\nLet’s regress lived_after on sex to see how candidates’ post-election lifespans differ by sex.\n\nfit_2 <- stan_glm(data = ch10,\n                       formula = lived_after ~ sex,\n                       refresh = 0,\n                       seed = 76)\n\n\nprint(fit_2, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 16.1    2.9  \nsexMale     12.3    2.9  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 13.3    0.3  \n\n\nWe do not have a value for female. However we do have an intercept. In this regression, our mathematical formula is:\n\\[ lived\\_after_i = \\beta_0  + \\beta_1 male_i + \\epsilon_i\\]\n\\(\\beta_0\\) is our intercept, around 16 years. In this type of model, our intercept represents the the variable which is not represented in the model. Therefore, the intercept value represents those who are not male (females).\n\\(\\beta_1\\) only affects the outcome when the candidate is male. When the candidate is a male, we add the coefficient for male to the intercept value, which gives us the average lifespan of a male gubernatorial candidate after an election.\nThe posterior distribution for \\(\\beta_0 + \\beta_1\\) can be constructed via simple addition.\n\nfit_2 |> \n  as_tibble() |> \n  mutate(male_intercept = `(Intercept)` + sexMale) |> \n  ggplot(aes(male_intercept)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate Years Left\",\n         y = \"Probability\",\n         x = \"Years To Live After the Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nBut is that actually so “simple?” Not really! Manipulating parameters directly is bothersome. Dealing with variables named (Intercept) is error-prone. Using posterior_epred() and associated functions is much easier.\n\nposterior_epred(fit_2,\n                tibble(sex = \"Male\")) |> \n  as_tibble() |> \n  ggplot(aes(`1`)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate\",\n         y = \"Probability\",\n         x = \"Expected Lifespan Post-Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nThe interpretation of this parameter is the same as we have seen before. There is a true average, across the entire population, of the number of years that male candidates live after the election. We can never know what that true average is. But, it seems very likely that the true average is somewhere between 27.5 and 29.5 years.\n\nposterior_epred(fit_2,\n                tibble(sex = \"Female\")) |> \n  as_tibble() |> \n  ggplot(aes(`1`)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Female Candidate\",\n         y = \"Probability\",\n         x = \"Expected Lifespan Post-Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nWe see that the expected lifespan post-election is significantly lower for female candidates, with the average being around 15 years.\n\n10.3.2 election_age\nTo begin, let’s model candidate lifespan after the election as a function of candidate lifespan prior to the election. The data:\n\nch10 |> \n  ggplot(aes(x = election_age, y = lived_after)) +\n    geom_point() +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Younger candidates live longer\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Years on Election Day\",\n         y = \"Years Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\n\n\n\n\nThe math is fairly simple:\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). - \\(lived\\_after_i\\) is the number of years lived after the election for candidate \\(i\\). - \\(election\\_age_i\\) is the number of years lived before the election for candidate \\(i\\). - \\(\\epsilon_i\\) is the “error term,” the difference between the actual years-lived for candidate \\(i\\) and the modeled years-lived. \\(\\epsilon_i\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\).\nThe key distinction is between:\n\nVariables, always subscripted with \\(i\\), whose values (potentially) vary across individuals.\nParameters, never subscripted with \\(i\\), whose values are constant across individuals.\n\nWhy do we use \\(lived\\_after_i\\) in this formula instead of \\(y_i\\)? The more often we remind ourselves about the variable’s actual substance, the better. But there is another common convention: to always use \\(y_i\\) as the symbol for the dependent variable. It would not be unusual to describe this model as:\n\\[ y_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i\\]\nBoth mean the same thing.\n\nEither way, \\(\\beta_0\\) is the “intercept” of the regression, the average value for the population of \\(lived\\_after\\), among those for whom \\(election\\_age = 0\\).\n\\(\\beta_1\\) is the “coefficient” of \\(election\\_age\\). When comparing two individuals, the first with an \\(election\\_age\\) one year older than the second, we expect the first to have a \\(lived\\_after\\) value \\(\\beta_1\\) different from the second. In other words, we expect the older to have fewer years remaining, because \\(\\beta_1\\) is negative. Again, this is the value for the population from which our data is drawn.\nThere are three unknown parameters — \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\) — just as with the models we used in Chapter 8. Before we get to the five parameter case, it is useful to review this earlier material.\nYou may recall from middle school algebra that the equation of a line is \\(y = m x + b\\). There are two parameters: \\(m\\) and \\(b\\). The intercept \\(b\\) is the value of \\(y\\) when \\(x = 0\\). The slope coefficient \\(m\\) for \\(x\\) is the increase in \\(y\\) for every one unit increase in \\(x\\). When defining a regression line, we use slightly different notation but the fundamental relationship is the same.\nRather repeat the process in Chapter 8, we will look at the resulting plot using a model that predicts years lived after as a factor of age at election.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nAs we discussed in Chapter 8, the most common term for a model like this is a “regression.” We have “regressed” lived_after, our dependent variable, on election_age, our (only) independent variable.\nConsider someone who is about 40 years old on Election Day. We have a score or more data points for candidates around that age. This area is highlighted by the red box on our plot. As we can see, two died soon after the election. Some of them lived for 50 or more years after the election. Variation fills the world. However, the fitted line tells us that, on average, we would expect a candidate that age to live about 37 years after the election.\nThis is a descriptive model, not a causal model. Remember our motto from Chapter Chapter 4: No causation without manipulation. There is no way, for person \\(i\\), to change the years that she has been alive on Election Day. On the day of this election, she is X years old. So, there are not two (or more) potential outcomes. Without more than one potential outcome, there can not be a causal effect.\nGiven that, it is important to monitor our language. We do not believe that that changes in election_age “cause” changes in lived_after. That is obvious. But there are some words and phrases — like “associated with” and “change by” — which are too close to causal. (And which we are guilty of using just a few paragraphs ago!) Be wary of their use. Always think in terms of comparisons when using a predictive model. We can’t change election_age for an individual candidate. We can only compare two candidates (or two groups of candidates).\n\nLet’s look at the posterior of \\(\\beta_1\\), the coefficient of \\(election\\_age_i\\):\n\n\nfit_1 |> \n  as_tibble() |> \n  ggplot(aes(election_age)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of the Coefficient of `election_age`\",\n         y = \"Probability\",\n         x = \"Coefficient of `election_age`\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n10.3.3 election_age and sex\nIn this model, our outcome variable continues to be lived_after, but now we will have two different explanatory variables: election_age and sex. Note that sex is a categorical explanatory variable and election_age is a continuous explanatory variable. This is the same type of model — parallel slopes — as we saw in Chapter 9.\nMath:\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\epsilon_i \\]\nBut wait! The variable name is sex, not male. Where does male come from?\n\nThe answer is that male is an indicator variable, meaning a 0/1 variable. male takes a value of one if the candidate is “Male” and zero otherwise. This is the same as the \\(male_i\\) variable used in the previous two examples.\n\nThe outcome variable is \\(lived\\_after_i\\), the number of years a person is alive after the election. \\(male_i\\) is one of our explanatory variables. If we are predicting the number of years a male candidate lives after the election, this value will be 1. When we are making this prediction for female candidates, this value will be 0. \\(c\\_election\\_age_i\\) is our other explanatory variable. It is the number of years a candidate has lived before the election, scaled by subtracting the average number of years lived by all candidates.\n\\(\\beta_0\\) is the average number of years lived after the election for women, who on the day of election, have been alive the average number of years of all candidates (i.e. both male and female). \\(\\beta_0\\) is also the intercept of the equation. In other words, \\(\\beta_0\\) is the expected value of \\(lived\\_after_i\\), if \\(male_i = 0\\) and \\(c\\_election\\_age_i = 0\\).\n\\(\\beta_1\\) is almost meaningless by itself. The only time it has meaning is when its value is connected to our intercept (i.e. \\(\\beta_0 + \\beta_1\\)). When the two are added together, you get the average number of years lived after the election for males, who on the day of election, have been alive the average number of years for all candidates.\n\\(\\beta_2\\) is, for the entire population, the average difference in \\(lived\\_after_i\\) between two individuals, one of whom has an \\(c\\_election\\_age_i\\) value of 1 greater than the other.\n\nLet’s translate the model into code.\n\nfit_3 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex + election_age,\n                      refresh = 0,\n                      seed = 12)\n\n\nprint(fit_3, detail = FALSE)\n\n             Median MAD_SD\n(Intercept)  66.0    3.2  \nsexMale       6.1    2.4  \nelection_age -0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 11.1    0.2  \n\n\nLooking at our results, you can see that our intercept value is around 66. The average female candidate, who had been alive the average number of years of all candidates, would live another 66 years or so after the election.\nNote that sexMale is around 6. This is our coefficient, \\(\\beta_1\\). We need to connect this value to our intercept value to get something meaningful. Using the formula \\(\\beta_0 + \\beta_1\\), we find out that the number of years the average male candidate — who, on the day of election, is the average age of all candidates — would live is around 72 years.\nNow take a look at the coefficient for \\(c\\_election\\_age_i\\), \\(\\beta_2\\). The median of the posterior, -0.8, represents the slope of the model. When comparing two candates who differ by one year in election_age, we expect that they will differ by -0.8 years in lived_after. It makes sense that this value is negative. The more years a candidate has lived, the fewer years the candidate has left to live. So, for every extra year a candidate is alive before an election, their lifespan after the election will be 0.8 years lower, on average.\nWe will now show you the parallel slopes model, which was created using the same process explained in the prior chapter. All we’ve done here is extracted the values for our intercepts and slopes, and separated them into two groups. This allows us to create a geom_abline object that takes a unique slope and intercept value, so we can separate the male and female observations.\n\n\n# A tibble: 3 × 2\n  term         estimate\n  <chr>           <dbl>\n1 (Intercept)    66.0  \n2 sexMale         6.15 \n3 election_age   -0.847\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.4 election_age, sex and election_age*sex\nLet’s create another model. This time, however, the numeric outcome variable of lived_after is a function of the two explanatory variables we used above, election_age and sex, and of their interaction. To look at interactions, we need 5 parameters, which is why we needed to wait until this chapter to introduce the concept.\nMath:\n\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\\\ \\beta_3 male_i *  c\\_election\\_age_i + \\epsilon_i \\]\n\n\nOur outcome variable is still \\(lived\\_after_i\\). We want to know how many years a candidate will live after an election. Our explanatory variables as the same as before. \\(male_i\\) is one for male candidates and zero for female candidates. \\(c\\_election\\_age_i\\) the number of years a candidate has lived before the election, relative to the average value for all candidates. In this model, we have a third predictor variable: the interaction between \\(male_i\\) and \\(c\\_election\\_age_i\\).\n\\(\\beta_0\\) is the average number of years lived after the election for women, who on the day of election, have been alive the average number of years of all candidates. In a sense, this is the same meaning as in the previous model, without an interaction term. But, always remember that the meaning of a parameter is conditional on the model in which it is embedded. Even if a parameter is called \\(\\beta_0\\) in two different regressions does necessitate that it means the same thing in both regressions. Parameter names are arbitrary, or at least simply a matter of convention.\n\\(\\beta_1\\) does not have a simple interpretation as a stand-alone parameter. It is a measure of how different women are from men. However, \\(\\beta_0 + \\beta_1\\) has a straightforward meaning exactly analogous to the meaning of \\(\\beta_0\\). The sum is the average number of years lived after the election for men, who on the day of election, have been alive the average number of years of all candidates.\n\\(\\beta_2\\) is the coefficient of \\(c\\_election\\_age_i\\). It it just the slope for women. It is the average difference in \\(lived\\_after_i\\) between two women, one of whom has an \\(c\\_election\\_age_i\\) value of 1 greater than the other. In our last example, \\(\\beta_2\\) was the slope for the whole population. Now we are different slopes for different genders.\n\\(\\beta_3\\) alone is difficult to interpret. However, when it is added to \\(\\beta_2\\), the result in the slope for men.\n\nCode:\n\nfit_4 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex*election_age,\n                      refresh = 0,\n                      seed = 13)\n\n\nprint(fit_4, detail = FALSE)\n\n                     Median MAD_SD\n(Intercept)          20.4   20.7  \nsexMale              52.0   20.8  \nelection_age         -0.1    0.4  \nsexMale:election_age -0.8    0.4  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 11.1    0.2  \n\n\n\n\nThe intercept has increased. \\(\\beta_0\\) is around 20. This is the intercept for females. It still means the average number of years lived after the election for women is 20 or so. Our sexMale coefficient, \\(\\beta_1\\), refers to the value that must be added to the intercept in order to get the average for males. When calculated, the result is 72. Keep in mind, however, that these values only apply if \\(c\\_election\\_age_i = 0\\), if, that is, candidate \\(i\\) is around 52 years old.\nThe coefficient for \\(c\\_election\\_age_i\\), \\(\\beta_2\\), is -0.1. What does this mean? It is the slope for females. So, when comparing two female candidates who differ by one year in age, we expect that the older candidate will live 0.1 years less. Now direct your attention below at the coefficient of sexMale:election_age, \\(\\beta_3\\), which is -0.8. This is the value that must be added to the coefficient of \\(c\\_election\\_age_i\\) (recall \\(\\beta_2 + \\beta_3\\)) in order to find the slope for males. When the two are added together, this value, or slope, is about -0.9. When comparing two male candidates who differ in age by one year, we expect the older candidate to live about 0.9 years less.\nKey point: The interpretation of the intercepts only apply to candidates for whom \\(c\\_election\\_age_i = 0\\). Candidates who are not 52 years-old will have a different expected number of years to live. The interpretation of the slope applies to everyone. In other words, the relationship between \\(lived\\_after_i\\) and \\(c\\_election\\_age_i\\) is the same, regardless of your gender or how old you are.\nThe posterior:\n\nfit_4 |> \n  as_tibble() |> \n  mutate(male_years = `(Intercept)` + sexMale) |> \n  rename(female_years = `(Intercept)`) |> \n  select(female_years, male_years) |> \n  pivot_longer(cols = female_years:male_years, \n               names_to = \"parameters\",\n               values_to = \"years\") |> \n  ggplot(aes(years, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Years Lived After the Election\",\n         subtitle = \"Men live longer\",\n         x = \"Average Years Lived Post Election\",\n         y = \"Probability\", \n         fill = \"Parameters\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nAgain, we do not recommend working directly with parameters. The above analysis would be much easier with posterior_epred(), as we will see in the Temperance Section.\nMale candidates live longer on average than female candidates. Note, also, that the average years to live after the election for females is about 20 with this model. With the previous model, it was 66 years. Why the difference? The interpretation of “average” is different! In the previous model, it was the average for all women. In this model, it is the average for all 52 years-old women. Those are different things, so we should hardly be surprised by different posteriors.\n\n\nSlope posteriors:\n\nfit_4 |> \n  as_tibble() |> \n  mutate(slope_men = election_age + `sexMale:election_age`) |> \n  rename(slope_women = election_age) |> \n  select(slope_women, slope_men) |> \n  pivot_longer(cols = slope_women:slope_men, \n               names_to = \"parameters\",\n               values_to = \"slope\") |> \n  ggplot(aes(slope, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Slope of Years-Lived on Years-to-Live\",\n         subtitle = \"Men have a steeper slope\",\n         x = \"Slope\",\n         y = \"Probability\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic() \n\n\n\n\nThis posterior distribution shows the average slope values for men and women. You can see that men have a steeper slope, while the slope for women is practically 0! If you are trying to forecast the number of years that a women will live after the election, you may ignore the number of years that she has already lived. This is definitely not true for men. Why the difference?\n\n10.3.5 Interaction model\nRecall the parallel slopes model that we created in Chapter 9. Another visualization we can create, one that also uses slopes and intercepts for our model, is the interaction model. In this model, the slopes for our two groups are different, creating a non-parallel visualization.\nThe process for creating the interaction model is similar to creating the parallel slopes model. Let us begin the same way — by tidying our data and inspecting it.\n\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_4 |> \n  tidy() |> \n  select(term, estimate)\n\ntidy\n\n# A tibble: 4 × 2\n  term                 estimate\n  <chr>                   <dbl>\n1 (Intercept)           20.4   \n2 sexMale               52.0   \n3 election_age          -0.0753\n4 sexMale:election_age  -0.781 \n\n\nAfter tidying our data, we will extract values and assign sensible names for later use. Note that this is identical to the process from Chapter 9, with the addition of a fourth term (the interaction term):\n\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nsex_male <- tidy$estimate[2]\nelection_age <- tidy$estimate[3]\ninteraction_term <- tidy$estimate[4]\n\nNow that we have extracted our values, we will create the intercept and slope values for our two different groups, females and males. Recall the following details about finding slopes and intercepts in an interaction model:\n\nThe intercept is the intercept for females. It represents the average number of years lived after the election for females.\nOur sexMale coefficient refers to the value that must be added to the intercept in order to get the average years lived post-election for males.\nThe coefficient for \\(c\\_election\\_age_i\\) is the slope for females.\nThe coefficient of sexMale:election_age is the value that must be added to the coefficient of \\(c\\_election\\_age_i\\) in order to find the slope for males.\n\n\n# Recall that the intercept and the estimate for election_age act as the\n# estimates for female candidates only. Accordingly, we have assigned those\n# values (from the previous code chunk) to more sensible names: female_intercept\n# and female_slope.\n\nfemale_intercept <- intercept\nfemale_slope <- election_age\n\n# To find the male intercept, we must add the intercept for the estimate for\n# sex_male. To find the male slope, we must add election_age to our\n# interaction term estimate.\n\nmale_intercept <- intercept + sex_male\nmale_slope <- election_age + interaction_term\n\nAfter creating objects for our different intercepts and slopes, we will now create the interaction model using geom_abline() for a male and female line.\n\n# From the ch10 data, create a ggplot object with election_age as the x-axis\n# and lived_after as the y-axis. We will use color = sex.\n\nggplot(ch10, aes(x = election_age, y = lived_after, color = sex)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the female intercept and slope. Set the\n  # intercept qual to our previously created female_intercept, while setting\n  # slope equal to our previously created female_slope. The color call is for\n  # coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = female_intercept,\n              slope = female_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the male values. Set the intercept equal to\n  # our previously created male_intercept, while setting slope equal to our\n  # previously created male_slope. The color call is for teal, to match the\n  # colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = male_intercept,\n              slope = male_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Interaction Model\",\n       subtitle = \"Comparing post election lifespan across sex\",\n       x = \"Average Age at Time of Election\", \n       y = \"Years Lived Post-Election\", \n       color = \"Sex\") +\n  theme_classic()\n\n\n\n\nThis is our final interaction model! There are some interesting takeaways. First, we may note that there are far fewer data points for female candidates — a concern we previously mentioned. It makes sense, then, that the slope would be less dramatic when compared with male candidates. We also see that most female candidates run when they are older, as compared with male candidates. This might explain why our intercept for years lived post-election is lower for female candidates.\nThe male line seems more sensible, as we might expect with far more datapoints. For male candidates, we see a clear (logical) pattern: the older candidates are at the time of election, the less years post-election they live. This makes sense, as we are limited by the human lifespan."
  },
  {
    "objectID": "10-five-parameters.html#temperance",
    "href": "10-five-parameters.html#temperance",
    "title": "10  Five Parameters",
    "section": "\n10.4 Temperance",
    "text": "10.4 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall the questions with which we began the chapter:\nHow long will two political candidates — one male and one female, both 10 years older than the average candidate — live after the election? How different will their lifespans be?\nThese questions are, purposely, less precise than the ones we tackled in Chapters Chapter 7 and Chapter 8, written more in a conversational style. This is how normal people talk.\nHowever, as data scientists, our job is to bring precision to these questions. There are two commonsense interpretations. First, we could be curious about the expected values for these questions. If we averaged the data for a thousand candidates like these, what would the answer be? Second, we could be curious about two specific individuals. How long will they live? Averages involve questions about parameters. The fates of individuals require predictions. Those are general claims, violated too often to be firm rules. Yet, they highlight a key point: expected values are less variable than individual predictions.\nTo calculate expected values, use posterior_epred(). To forecast for individuals, use posterior_predict().\n\n10.4.1 Expected values\nConsider the “on average” interpretation first. The answer begins with the posterior distributions of the parameters in fit_4.\n\n\nnewobs = tibble(sex = c(\"Male\", \"Female\"), \n                 election_age = 10)\n\npe <- posterior_epred(object = fit_4, \n                      newdata = newobs) |> \n  as_tibble() |> \n  rename(\"Male\" = `1`,\n         \"Female\" = `2`)\n\n\npe |> \n pivot_longer(cols = Male:Female, \n               names_to = \"Gender\",\n               values_to = \"years\") |> \n  ggplot(aes(years, fill = Gender)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Years Lived Post-Election\",\n         subtitle = \"Male candidates live longer\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = \n                         scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\n\nLooking at our posterior probability distributions above, we can see that male candidates are expected to live longer. But how much longer? As in previous chapters, we can manipulate distributions in, more or less, the same way that we manipulate simple numbers. If we want to know the difference between two posterior distributions, we can simply subtract.\n\npe <- posterior_epred(object = fit_4, \n                      newdata = newobs) |> \n  as_tibble() |> \n  mutate(diff = `1` - `2`)\n\n\npe |> \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Additional Male Years Lived\",\n         subtitle = \"Male candidates live about 4 years longer\",\n         x = \"Expected Additional Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nThe average value of the difference in years-to-live is probably positive, with the most likely value being around 45 years. But there still a 1% chance the true value is less than zero, i.e., that we should expect female candidates to live longer.\nInstead of using posterior_epred(), we could have answered these questions by using the posterior probability distributions for the parameters in the model, along with some simple math. Don’t do this! First, you are much more likely to make a mistake. Second, this approach does not generalize well to complex models with scores of parameters and their interactions.\n\n10.4.2 Individual predictions\nIf, instead, we interpret the question as asking for a prediction for a small number of individuals, then we need to use posterior_predict().\n\nUse posterior_predict() to create draws from the posterior probability distribution for our prediction for these cases. posterior_predict() takes two arguments: the model for which the simulations should be run, and a tibble indicating the covariate values for the individual(s) we want to predict. In this case, we are using the fit_4 model and the tibble is the one we just created above. In other words, the inputs for posterior_predict() and posterior_epred() are identical.\n\npp <- posterior_predict(object = fit_4, \n                        newdata = newobs) |>\n  as_tibble() |> \n  rename(\"Male\" = `1`,\n         \"Female\" = `2`)\n\npp  \n\n# A tibble: 4,000 × 2\n    Male Female\n   <dbl>  <dbl>\n 1  80.0 41.1  \n 2  58.3 15.4  \n 3  65.2 17.0  \n 4  74.0 38.4  \n 5  46.0 67.8  \n 6  56.6 20.4  \n 7  54.4  0.763\n 8  74.7  2.93 \n 9  46.5 12.5  \n10  69.1 -1.22 \n# … with 3,990 more rows\n\n\nThe resulting tibble has 2 columns, the first for a male candidate and the second for female candidate. Both columns are draws from the posterior predictive distributions. In both cases, the forecasts depend on the values of all the covariates. That is, we would provide a different forecast if the candidates were younger or older.\nLet’s look at the posterior predictive distribution for each candidate.\n\npp |> \n  pivot_longer(cols = Male:Female, \n               names_to = \"Gender\",\n               values_to = \"years\") |> \n  ggplot(aes(years, fill = Gender)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for a Candidate's Years Lived Post-Election\",\n         subtitle = \"Individual lifespans have a great deal of variation\",\n         x = \"Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nThere is a big overlap in the predictions for individuals while, at the same time, there is much less overlap in the averages. Random stuff happens to an individual all the time. Random stuff cancels out when you take the average for many individuals. Consider the difference in the posterior predictive distributions for the two individuals.\n\n\npp |> \n  mutate(diff = Male - Female) |> \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for a Male Candidate's Extra Years Lived\", \n         subtitle = \"Any random male candidate may die before a random female candidate\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\nIn words, we would predict that the male candidate would live longer than the female candidate. By how much? Well, that number is an unknown parameter. By looking at our posterior above, our best estimate is about 44.6 years. However, it is quite possible that, for any given male/female candidates, the female will live longer.\n\npp |> \n  mutate(diff = Male - Female) |> \n  summarize(f_live_longer = sum(diff < 0),\n            total = n(),\n            f_live_longer / total)\n\n# A tibble: 1 × 3\n  f_live_longer total `f_live_longer/total`\n          <int> <int>                 <dbl>\n1           107  4000                0.0268\n\n\nIn fact, there is a 4 in 10 chance that the female candidate lives longer.\nNote what is the same and what is different when we move from a question about averages to a question about individuals. In both cases, the most likely value is about the same. That is, the average behavior is the same as our expected value for any given individual. But the uncertainty is much greater for an individual prediction. The chance of the true average for male candidates being less than that for female candidates is low. Yet, for any individual pair of candidates, it would not even be slightly surprising for the female candidate to outlive the male candidate. Individuals vary. Averages never tell the whole story.\n\n10.4.3 Expectation versus individual variation\nLet’s compare the results from posterior_epred() and posterior_predict() for this scenario directly. Most of this code is the same as what we have shown you above, but we think it is useful to look at everything together.\n\nnewobs <- tibble(sex = c(\"Male\", \"Female\"),\n                  election_age = 10)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) |>\n  as_tibble() |> \n  mutate(diff = `1` - `2`)\n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) |> \n  as_tibble() |> \n  mutate(diff = `1` - `2`)\n\ntibble(Expectation = pe$diff,\n       Prediction = pp$diff) |> \n  pivot_longer(cols = Expectation:Prediction, \n               names_to = \"Type\",\n               values_to = \"years\") |> \n  ggplot(aes(years, fill = Type)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected and Individual Male Advantage\",\n         subtitle = \"Expected male advantage is much more precisely estimated\",\n         x = \"Additional Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nExpected values vary much less than predictions. The above chart makes that easy to see. We are somewhat sure that the true underlying average for the numbers of years that male candidates live post-election is more than female candidates. But, for any two individual candidates, there is a good chance that that the female candidate will live longer. We can not ignore \\(\\epsilon\\) when predicting the outcome for individuals. When estimating expected values or long-run averages, the \\(\\epsilon\\)’s cancel out.\n\n\n\n10.4.4 Testing\n“Tests,” “testing,” “hypothesis tests,” “tests of significance,” and “null hypothesis significance testing” all refer to the same concept. We will refer to this collection of approaches as NHST, a common abbreviation derived from the initials of the last phrase. Wikipedia provides an overview.\nIn hypothesis testing, we have a null hypothesis — this hypothesis represents a particular probability model. We also have an alternative hypothesis, which is typically the alternative to the null hypothesis. Let’s look at an example that is unrelated to statistics first.\nImagine a criminal trial held in the United States. Our criminal justice system assumes “the defendant is innocent until proven guilty.” That is, our initial assumption is that the defendant is innocent.\nNull hypothesis (\\(H_0\\)): Defendent is not guilty (innocent) Alternative hypothesis (\\(H_a\\)): Defendant is guilty\nIn statistics, we always assume the null hypothesis is true. That is, the null hypothesis is always our initial assumption.\nWe then collect evidence — such as finger prints, blood spots, hair samples — with the hopes of finding “sufficient evidence” to make the assumption of innocence refutable.\nIn statistics, the data are the evidence.\nThe jury then makes a decision based on the available evidence:\nIf the jury finds sufficient evidence — beyond a reasonable doubt — to make the assumption of innocence refutable, the jury rejects the null hypothesis and deems the defendant guilty. We behave as if the defendant is guilty. If there is insufficient evidence, then the jury does not reject the null hypothesis. We behave as if the defendant is innocent.\nIn statistics, we always make one of two decisions. We either reject the null hypothesis or we fail to reject the null hypothesis. Rather than collect physical evidence, we test our hypothesis in our model. For example, say that we have a hypothesis that a certain parameter equals zero. The hypotheses are:\n\\(H_0\\): The parameter equals 0. \\(H_a\\): The parameter does not equal 0.\nThe hypothesis that a parameter equals zero (or any other fixed value) can be directly tested by fitting the model that includes the parameter in question and examining the corresponding 95% interval. If the 95% interval excludes zero (or the specified fixed value), then the hypothesis is said to be rejected. If the 95% interval inclues zero, we do not reject the hypothesis. We also do not accept the hypothesis.\nIf this sounds nonsensical, it’s because it is. Our view: Amateurs test. Professionals summarize.\nA Yes/No question throws away too much information to (almost) ever be useful. There is no reason to test when you can summarize by providing the full posterior probability distribution.\nThe same arguments apply in the case of “insignificant” results when we can’t “reject” the null hypothesis. In simple terms: who cares!? We have the full posterior probability distribution for that prediction — also known as the posterior predictive distribution — as graphed above. The fact that result is not “significant” has no relevance to how we use the posterior to make decisions.\nThe same reasoning applies to every parameter we estimate, to every prediction we make. Never test — unless your boss demands a test. Use your judgment, make your models, summarize your knowledge of the world, and use that summary to make decisions."
  },
  {
    "objectID": "10-five-parameters.html#summary",
    "href": "10-five-parameters.html#summary",
    "title": "10  Five Parameters",
    "section": "\n10.5 Summary",
    "text": "10.5 Summary\nThe major part of Wisdom is deciding what questions you can’t answer because of the data you don’t have.\nAvoid answering questions by working with parameters directly. Use posterior_epred() instead.\nGood data science involves an intelligent tour of the space of possible models.\nAlways think in terms of comparisons when using a predictive model.\nSpend less time thinking about what parameters mean and more time using posterior_epred() and posterior_predict() to examine the implications of your models.\n\n\n\n\nBarfort, Sebastian, Robert Klemmensen, and Erik Gahner Larsen. 2020. “Longevity Returns to Political Office.” Political Science Research and Methods. https://doi.org/10.1017/psrm.2019.63."
  },
  {
    "objectID": "11-n-parameters.html#wisdom",
    "href": "11-n-parameters.html#wisdom",
    "title": "11  N Parameters",
    "section": "\n11.1 Wisdom",
    "text": "11.1 Wisdom\n\n\n\n\nWisdom\n\n\n\n\nAs you research ways to increase voting, you come across a large-scale experiment showing the effect of sending out a voting reminder that “shames” citizens who do not vote. You are considering sending out a “shaming” voting reminder yourself.\nWe will be looking at the shaming dataset from the primer.data package. This is dataset is from “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment” by Gerber, Green, and Larimer (2008). Check out the paper here. You can, and should, familiarize yourself with the data by typing ?shaming.\nRecall our initial question: how can we encourage voters to go out to the polls on election day? We now need to translate this into a more precise question, one that we can answer with data.\nOur question:\nWhat is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?\n\n11.1.1 Ideal Preceptor Table\nRecall the ideal Preceptor Table. What rows and columns of data do you need such that, if you had them all, the calculation of the number of interest would be trivial? If you want to know the average height of an adult in India, then the ideal Preceptor Table would include a row for each adult in India and a column for their height.\nOne key aspect of this ideal Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. Mainly: do we need a causal model, one which estimates that attitude under both treatment and control? The Preceptor Table would require two columns for the outcome. In this case, we are trying to see the causal effect of mailed voting reminders on voting.\nAre we are modeling (just) for prediction or are we (also) modeling for causation? Predictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model. Here, we are looking at causation.\nSo, what would our ideal table look like? Assuming we are running for governor in the United States, we would ideally have data for every citizen of voting age. This means we would have approximately 200 million rows.\nBecause there is no missing data in an ideal Preceptor Table, we would also know the outcomes under both treatment (receiving a reminder) and control (not receiving a reminder). Here is a sample row from our table:\n\n\n\n\n\n\n\n\nID\n      \n        Outcomes\n      \n    \n\nBehavior in Treatment\n      Behavior in Control\n      Treatment effect\n    \n\n\n\nCitizen 1\n0\n1\n+1\n\n\nCitizen 2\n1\n1\n0\n\n\n\n\n\n\nIn our ideal table, we have rows for all American citizens of voting age. This is a good start! However, we may want even more information in our ideal Preceptor Table. Perhaps a column for sex would be informative. A column for age? Political affiliation? In a perfect world, we would know all of these pieces of information. In a perfect world, we could measure the exact causal effect of voting reminders for different subsets of the US population.\nWe may also want to narrow our ideal Preceptor Table. If we are running for governor in Florida, we may only want to study citizens in Florida. If we are running as a Democrat, we may only want to study citizens who are registered Democrats.\nHowever, the main point of this exercise is to see what we want to know compared with what we actually do know.\n\n11.1.2 EDA of shaming\n\nAfter loading the packages we need, let’s perform an EDA, starting off by running glimpse() on the shaming tibble from the primer.data package.\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(ggthemes)\nlibrary(ggdist)\nlibrary(gt)\nlibrary(janitor)\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\n\nglimpse(shaming)\n\nRows: 344,084\nColumns: 15\n$ cluster       <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ primary_06    <int> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,…\n$ treatment     <fct> Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne,…\n$ sex           <chr> \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\", \"F…\n$ age           <int> 65, 59, 55, 56, 24, 25, 47, 50, 38, 39, 65, 61, 57, 37, …\n$ primary_00    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ general_00    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ primary_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ general_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ primary_04    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ general_04    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ hh_size       <int> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1,…\n$ hh_primary_04 <dbl> 0.0952381, 0.0952381, 0.0476190, 0.0476190, 0.0476190, 0…\n$ hh_general_04 <dbl> 0.8571429, 0.8571429, 0.8571429, 0.8571429, 0.8571429, 0…\n$ neighbors     <int> 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n\n\nglimpse() gives us a look at the raw data contained within the shaming data set. At the very top of the output, we can see the number of rows and columns, or observations and variables respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. This summary provides an idea of some of the variables we will be working with.\nVariables of particular interest to us are sex, hh_size, and primary_06. The variable hh_size tells us the size of the respondent’s household, sex tells us the sex of the respondent, and primary_06 tells us whether or not the respondent voted in the 2006 Primary election. There are a few things to note while exploring this data set. You may – or may not – have noticed that the only response to the general_04 variable is “Yes”. In their published article, the authors note that “Only registered voters who voted in November 2004 were selected for our sample” (Gerber, Green, Larimer, 2008). After this, the authors found their history then sent out the mailings. Thus, non-registered voters are excluded from our data.\nIt is also important to identify the dependent variable and its meaning. In this shaming experiment, the dependent variable is primary_06, which is a variable coded either 0 or 1 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the effect that the treatments have on voting behavior in the 2006 general election.\n\nWe have not yet discussed the most important variable of them all: treatment. The treatment variable is a factor variable with 5 levels, including the control. Since we are curious as to how sending mailings affects voter turnout, the treatment variable will tell us about the impact each type of mailing can make. Let’s start off by taking a broad look at the different treatments.\n\n\nshaming |>\n  count(treatment)\n\n# A tibble: 5 × 2\n  treatment        n\n  <fct>        <int>\n1 No Postcard 191243\n2 Civic Duty   38218\n3 Hawthorne    38204\n4 Self         38218\n5 Neighbors    38201\n\n\nFour types of treatments were used in the experiment, with voters receiving one of the four types of mailing. All of the mailing treatments carried the message, “DO YOUR CIVIC DUTY - VOTE!”.\nThe first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote.” This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.\nIn the second treatment, Hawthorne, households received a mailing which told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.\nIn the third treatment, Self, the mailing includes the recent voting record of each member of the household, placing the word “Voted” next to their name if they did in fact vote in the 2004 election or a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart” with the voting record of the household members after the 2006 primary. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.\nThe fourth treatment, Neighbors, provides the household members’ voting records, as well as the voting records of those who live nearby. This mailing also told recipients, “we intend to mail an updated chart” of who voted in the 2006 election to the entire neighborhood.\n\n11.1.3 Population\nOne of the most important components of Wisdom is the concept of the “population”. Recall the questions we asked earlier:\nAs we have discussed before, the population is not the set of people, or voters, for which we have data. This is the dataset. Nor is it the set of voters about whom we would like to have data. Those are the rows in the ideal Preceptor Table. The population is the larger — potentially much larger — set of individuals which include both the data we have and the data we want. Generally, the population will be much larger than either the data we have and the data we want.\nIn this case, we are viewing the data from the perspective of someone running for Governor this year that wants to increase voter turnout. We want to increase turnout now, not for people voting in 2006! We also may want to increase turnout in those citizens who are not registered to vote, a group that is excluded from our dataset. Is it reasonable to generate conclusions for this group? Most likely, no. However, we have limited data to work with and we have to determine how far we are willing to generalize to other groups.\nIt is a judgment call, a matter of Wisdom, as to whether or not we may assume that the data we have and the data we want to have (i.e., the ideal Preceptor Table) are drawn from the same population.\n\n\nEven though the original question is about “voters” in general, and does not specifically refer to specific states in which we might be interested, we will assume that the data we have for random voters is, uh, representative enough of the population we are interested in. If we did not believe that, then we should stop right now. The major part of Wisdom is deciding what questions you can’t answer because of the data you just don’t have."
  },
  {
    "objectID": "11-n-parameters.html#justice",
    "href": "11-n-parameters.html#justice",
    "title": "11  N Parameters",
    "section": "\n11.2 Justice",
    "text": "11.2 Justice\n\n\n\n\nJustice\n\n\n\n\nJustice emphasizes a few key concepts:\n\nThe actual Preceptor Table, a structure which includes a row for every unit in the population. We generally break the rows in the actual Preceptor Table into three categories: the data for units we want to have, the data for units which we actually have, and the data for units we do not care about.\nIs our data representative of the population?\nIs the meaning of the columns consistent, i.e., can we assume validity?\n\nWe then make an assumption about the data generating mechanism.\n\n11.2.1 Preceptor Table\nRecall that in an actual Preceptor Table, we will have a bunch of missing data! We can not use simple arithmetic to calculate the causal effect of voting reminders on voting behavior. Instead, we will be required to estimate it. This is our estimand, a variable in the real world that we are trying to measure. An estimand is not the value you calculated, but is rather the unknown variable you want to estimate.\nLet’s build a basic visualization for the actual Preceptor Table for this scenario:\n\n\n\n\n\n\n\n\nID\n      \n        Outcomes\n      \n      \n        $$\\text{Estimand}$$\n      \n    \n\nTreatment\n      Control\n      Treatment - Control\n    \n\n\n\n\nCitizen 1\n\n\nVoted\n\n\n?\n\n\n?\n\n\n\n\nCitizen 23\n\n\nDid not vote\n\n\n?\n\n\n?\n\n\n\n\nCitizen 40\n\n\n?\n\n\nVoted\n\n\n?\n\n\n\n\nCitizen 53\n\n\n?\n\n\nDid not vote\n\n\n?\n\n\n\n\nCitizen 80\n\n\nVoted\n\n\n?\n\n\n?\n\n\n\n\n\n\n\nHere, there are two possible outcomes: did vote or did not vote. What we really want to know is the Average Treatment Effect (ATE) of the treatment, the voting reminder. We want to estimate how much the voting reminder impacts the odds of someone voting.\nNote that this is a simplified version of the actual Preceptor Table. In this dataset, we have a number of other columns that we know about each of our subjects: age, sex, past voting history. In an expanded actual Preceptor Table, these columns would be included.\nNow, how can we fill in the question marks? Because of the Fundamental Problem of Causal Inference, we can never know the missing values. Because we can never know the missing values, we must make assumptions. “Assumption” just means that we need a “model,” and all models have parameters.\n\n11.2.2 The Population Table\nThe Population Table shows the data that we actually have in our desired population. It shows rows from three sources: the ideal Preceptor Table, the data, and the population outside of the data (rows which exist but for which we have no data).\nIn our ideal Preceptor rows, we have information for our covariates of sex, year, and state. However, because those rows are not included in our data, we do not have any outcome results. Since our scenario pertains to an upcoming election in Texas, the state column will read Texas and the year column will read 2021.\nThe rows from our data have everything: the covariates and the outcomes. The covariates here will be Michigan for state and 2006 for year, since these are the pieces of information that are included in our data. Of course, we still do not have values for Treatment minus Control, since we cannot observe one subject under two conditions.\nThe rows from the population have no data. These are subjects which fall under our desired population, but for which we have no data. As such, all rows are missing.\n\n\n\n\n\n\n\n\nSource\n      Sex\n      Year\n      State\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n\nTreatment\n      Control\n      Treatment - Control\n    \n\n\n\n\nPopulation\n\n\n?\n\n\n1990\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n1995\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nData\n\n\nMale\n\n\n2006\n\n\nMichigan\n\n\nDid not vote\n\n\n?\n\n\n?\n\n\n\n\nData\n\n\nFemale\n\n\n2006\n\n\nMichigan\n\n\n?\n\n\nVoted\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n2010\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2012\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPreceptor Table\n\n\nFemale\n\n\n2021\n\n\nTexas\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nFemale\n\n\n2021\n\n\nTexas\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n2026\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2030\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n\n\n\n\n11.2.3 Validity\nTo understand validity in regards to the Population Table, we must first recognize an inherent flaw in any experiment design: no two units receive exactly the same treatment.\nWe might be thinking: well, surely two postcards are the same? But, they aren’t! The postcards sent from our data were sent with information relevant to 2006 — a different candidate, different language, different syntax. A postcard sent in 2021, even if we used the exact same language, would be encouraging a new candidate with new reform and differing policies.\nThus, despite the fact that two units are in the same treatment — that is, receiving a postcard — they have very different versions of that treatment. Indeed, there are an infinite number of possible treatments. This is why it is so important to define our estimand clearly.\n\n11.2.4 Stability\nStability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn.\nWith something like height, it is much easier to assume stability over a greater period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. Can we say the same for this example, where we are looking at voting behavior?\nIs data collected in 2006 on voting behavior likely to be the same in 2021? Frankly, we don’t know! We aren’t sure what would impact someone’s response to a postcard encouraging them to vote. It is possible, for instance, that a postcard informing neighbors of voting status would have more of an effect on voting behavior during a pandemic, when you are more closely interacting with neighbors.\nWhen we are confronted with this uncertainty, we can consider making our timeframe smaller. However, we would still need to assume stability from 2006 (time of data collection) to today. Stability allows us to ignore the issue of time.\n\n11.2.5 Representativeness\nThis is a good time to consider what it really means to accept that our data is representative of our population. With that in mind, let’s break down our real, current question:\n\nWe are running for governor in Texas in the year 2021. In this year and in the United States, we consider sending out a voting reminder postcard to citizens of voting age. Will this reminder encourage voting, and by how much?\n\nNow, let’s break down our data from the shaming dataset:\n\nThe data was gathered in Michigan prior to the August 2006 primary election. The population for the experiment was 180,002 households in the state of Michigan. The data only included those who had voted in the 2004 general election. Therefore, it did not include non-voters. The reminders were mailed to households at random.\n\nSo, how similar are these groups? Let’s start with some differences. * The data is from 2006. Our question is asking for answers from 2021. This is not a small gap in time. A lot changes in a decade and a half! * The data excludes all non-voters in the last election. Our question, which seeks to increase voting turnout in all citizens, would want for non-voters to be included. So, can we make any claims about those citizens? Probably not. * The data only includes voters from Michigan. We want to make inferences about Texas, or perhaps the United States as a whole. Is it within reason to do that?\nGenerally: if there was no chance that a certain type of person would have been in this experiment, we cannot make an assumption for that person.\nThe purpose of this section is to make us think critically about the assumptions we are making and whether those assumptions can be reasonably made. Though we will continue using this dataset in the remainder of the chapter, it is clear that we must make our predictions with caution.\n\n11.2.6 Functional form"
  },
  {
    "objectID": "11-n-parameters.html#courage",
    "href": "11-n-parameters.html#courage",
    "title": "11  N Parameters",
    "section": "\n11.3 Courage",
    "text": "11.3 Courage\n\n\n\n\nCourage\n\n\n\n\n\n11.3.1 Set-up\nNow, we will create an object named object_1 that includes a 3-level factor classifying voters by level of civic engagement.\n\nConvert all primary and general election variables that are not already 1/0 binary to binary format.\nCreate a new column named civ_engage that sums up each person’s voting behavior up to, but not including, the 2006 primary.\nCreate a column named voter_class that classifies voters into 3 bins: “Always Vote” for those who voted at least 5 times, “Sometimes Vote” for those who voted between 3 or 4 times, and “Rarely Vote” for those who voted 2 or fewer times. This variable should be classified as a factor.\nCreate a column called z_age which is the z-score for age.\n\n\nobject_1 <- shaming |> \n  \n  # Converting the Y/N columns to binaries with the function we made \n  # note that primary_06 is already binary and also that we don't \n  # need it to predict construct previous voter behavior status variable.\n  \n  mutate(p_00 = (primary_00 == \"Yes\"),\n         p_02 = (primary_02 == \"Yes\"),\n         p_04 = (primary_04 == \"Yes\"),\n         g_00 = (general_00 == \"Yes\"),\n         g_02 = (general_02 == \"Yes\"),\n         g_04 = (general_04 == \"Yes\")) |> \n  \n  # A sum of the voting action records across the election cycle columns gives\n  # us an idea (though not weighted for when across the elections) of the voters\n  # general level of civic involvement.\n  \n  mutate(civ_engage = p_00 + p_02 + p_04 + \n                      g_00 + g_02 + g_04) |> \n  \n  # If you look closely at the data, you will note that g_04 is always Yes, so\n  # the lowest possible value of civ_engage is 1. The reason for this is that\n  # the sample was created by starting with a list of everyone who voted in the\n  # 2004 general election. Note how that fact makes the interpretation of the\n  # relevant population somewhat subtle.\n  \n  mutate(voter_class = case_when(civ_engage %in% c(5, 6) ~ \"Always Vote\",\n                                 civ_engage %in% c(3, 4) ~ \"Sometimes Vote\",\n                                 civ_engage %in% c(1, 2) ~ \"Rarely Vote\"),\n         voter_class = factor(voter_class, levels = c(\"Rarely Vote\", \n                                                      \"Sometimes Vote\", \n                                                      \"Always Vote\"))) |> \n  \n  # Centering and scaling the age variable. Note that it would be smart to have\n  # some stopifnot() error checks at this point. For example, if civ_engage < 1\n  # or > 6, then something has gone very wrong.\n  \n  mutate(z_age = as.numeric(scale(age))) |> \n  select(primary_06, treatment, sex, civ_engage, voter_class, z_age)\n\nLet’s inspect our object:\n\nobject_1 |> \n  slice(1:3)\n\n# A tibble: 3 × 6\n  primary_06 treatment  sex    civ_engage voter_class    z_age\n       <int> <fct>      <chr>       <int> <fct>          <dbl>\n1          0 Civic Duty Male            4 Sometimes Vote 1.05 \n2          0 Civic Duty Female          4 Sometimes Vote 0.638\n3          1 Hawthorne  Male            4 Sometimes Vote 0.361\n\n\nGreat! Now, we will create our first model: the relationship between primary_06, which represents whether a citizen voted or not, against sex and treatment.\n\n11.3.2 primary_06 ~ treatment + sex\nIn this section, we will look at the relationship between primary voting and treatment + sex.\nThe math:\nWithout variable names:\n\\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i, 1} + \\beta_{2}x_{i,2} ... + \\beta_{n}x_{i,n} + \\epsilon_{i} \\] With variable names:\n\\[ y_{i} = \\beta_{0} + \\beta_{1}civic\\_duty_i + \\beta_{2}hawthorne_i + \\beta_{3}self_i + \\beta_{4}neighbors_i + \\beta_{5}male_i + \\epsilon_{i} \\]\nThere are two ways to formalize the model used in fit_1: with and without the variable names. The former is related to the concept of Justice as we acknowledge that the model is constructed via the linear sum of n parameters times the value for n variables, along with an error term. In other words, it is a linear model. The only other model we have learned this semester is a logistic model, but there are other kinds of models, each defined by the mathematics and the assumptions about the error term. The second type of formal notation, more associated with the virtue Courage, includes the actual variable names we are using. The trickiest part is the transformation of character/factor variables into indicator variables, meaning variables with 0/1 values. Because treatment has 5 levels, we need 4 indicator variables. The fifth level — which, by default, is the first variable alphabetically (for character variables) or the first level (for factor variables) — is incorporated in the intercept.\nLet’s translate the model into code.\n\nfit_1 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ treatment + sex,\n                  refresh = 0,\n                  seed = 987)\n\n\nprint(fit_1, digits = 3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      primary_06 ~ treatment + sex\n observations: 344084\n predictors:   6\n------\n                    Median MAD_SD\n(Intercept)         0.291  0.001 \ntreatmentCivic Duty 0.018  0.003 \ntreatmentHawthorne  0.026  0.003 \ntreatmentSelf       0.048  0.002 \ntreatmentNeighbors  0.081  0.003 \nsexMale             0.012  0.002 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.464  0.001 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWe will now create a table that nicely formats the results of fit_1 using the tbl_regression() function from the gtsummary package. It will also display the associated 95% confidence interval for each coefficient.\n\ntbl_regression(fit_1, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |>\n  \n  # Using Beta as the name of the parameter column is weird.\n  \n  as_gt() |>\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Age Predict Likelihood of Voting\") |>\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |> \n  cols_label(estimate = md(\"**Parameter**\"))\n\n\n\n\n\n\n\nLikelihood of Voting in the Next Election\n    \n\nHow Treatment Assignment and Age Predict Likelihood of Voting\n    \n\n\nCharacteristic\n      Parameter\n      \n95% CI1\n\n    \n\n\n(Intercept)\n0.291\n0.288, 0.293\n\n\ntreatment\n\n\n\n\n    No Postcard\n—\n—\n\n\n    Civic Duty\n0.018\n0.013, 0.023\n\n\n    Hawthorne\n0.026\n0.021, 0.031\n\n\n    Self\n0.048\n0.044, 0.054\n\n\n    Neighbors\n0.081\n0.076, 0.087\n\n\nsex\n\n\n\n\n    Female\n—\n—\n\n\n    Male\n0.012\n0.009, 0.015\n\n\n\nSource: Gerber, Green, and Larimer (2008)\n    \n\n\n1 CI = Credible Interval\n    \n\n\n\n\nInterpretation: * The intercept of this model is the expected value of the probability of someone voting in the 2006 primary given that they are part of the control group and are female. In this case, we estimate that women in the control group will vote ~29.1% of the time. * The coefficient for sexMale indicates the difference in likelihood of voting between a male and female. In other words, when comparing men and women, the 0.012 implies that men are ~1.2% more likely to vote than women. Note that, because this is a linear model with no interactions between sex and other variables, this difference applies to any male, regardless of the treatment he received. Because sex can not be manipulated (by assumption), we should not use a causal interpretation of the coefficient. * The coefficients of the treatments, on the other hand, do have a causal interpretation. For a single individual, of either sex, being sent the Self postcard increases your probability of voting by 4.8%. It appears that the Neighbors treatment is the most effective at ~8.1% and Civic Duty is the least effective at ~1.8%.\n\n11.3.3 primary_06 ~ z_age + sex + treatment + voter_class + voter_class*treatment\nIt is time to look at interactions! Create another model named fit_2 that estimates primary_06 as a function of z_age, sex, treatment, voter_class, and the interaction between treatment and voter classification.\nThe math: \\[y_{i} = \\beta_{0} + \\beta_{1}z\\_age + \\beta_{2}male_i + \\beta_{3}civic\\_duty_i + \\\\ \\beta_{4}hawthorne_i + \\beta_{5}self_i + \\beta_{6}neighbors_i + \\\\ \\beta_{7}Sometimes\\ vote_i + \\beta_{8}Always\\ vote_i + \\\\ \\beta_{9}civic\\_duty_i Sometimes\\ vote_i + \\beta_{10}hawthorne_i Sometimes\\ vote_i + \\\\ \\beta_{11}self_i Sometimes\\ vote_i + \\beta_{11}neighbors_i Sometimes\\ vote_i + \\\\ \\beta_{12}civic\\_duty_i Always\\ vote_i + \\beta_{13}hawthorne_i Always\\ vote_i + \\\\ \\beta_{14}self_i Always\\ vote_i + \\beta_{15}neighbors_i Always\\ vote_i + \\epsilon_{i}\\] Translate into code:\n\nfit_2 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ z_age + sex + treatment + voter_class + \n                            treatment*voter_class,\n                  family = gaussian,\n                  refresh = 0,\n                  seed = 789)\n\n\nprint(fit_2, digits = 3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      primary_06 ~ z_age + sex + treatment + voter_class + treatment * \n       voter_class\n observations: 344084\n predictors:   17\n------\n                                              Median MAD_SD\n(Intercept)                                    0.153  0.003\nz_age                                          0.035  0.001\nsexMale                                        0.008  0.002\ntreatmentCivic Duty                            0.010  0.007\ntreatmentHawthorne                             0.007  0.007\ntreatmentSelf                                  0.023  0.007\ntreatmentNeighbors                             0.044  0.007\nvoter_classSometimes Vote                      0.114  0.003\nvoter_classAlways Vote                         0.294  0.004\ntreatmentCivic Duty:voter_classSometimes Vote  0.014  0.008\ntreatmentHawthorne:voter_classSometimes Vote   0.019  0.007\ntreatmentSelf:voter_classSometimes Vote        0.030  0.007\ntreatmentNeighbors:voter_classSometimes Vote   0.042  0.008\ntreatmentCivic Duty:voter_classAlways Vote    -0.001  0.009\ntreatmentHawthorne:voter_classAlways Vote      0.025  0.009\ntreatmentSelf:voter_classAlways Vote           0.026  0.008\ntreatmentNeighbors:voter_classAlways Vote      0.047  0.009\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.451  0.001 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nAs we did with our first model, create a regression table to observe our findings:\n\ntbl_regression(fit_2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |>\n  as_gt() |>\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Other Variables Predict Likelihood of Voting\") |>\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |> \n  cols_label(estimate = md(\"**Parameter**\"))\n\n\n\n\n\n\n\nLikelihood of Voting in the Next Election\n    \n\nHow Treatment Assignment and Other Variables Predict Likelihood of Voting\n    \n\n\nCharacteristic\n      Parameter\n      \n95% CI1\n\n    \n\n\n(Intercept)\n0.153\n0.147, 0.159\n\n\nz_age\n0.035\n0.034, 0.037\n\n\nsex\n\n\n\n\n    Female\n—\n—\n\n\n    Male\n0.008\n0.005, 0.011\n\n\ntreatment\n\n\n\n\n    No Postcard\n—\n—\n\n\n    Civic Duty\n0.010\n-0.004, 0.023\n\n\n    Hawthorne\n0.007\n-0.005, 0.021\n\n\n    Self\n0.023\n0.010, 0.036\n\n\n    Neighbors\n0.044\n0.031, 0.057\n\n\nvoter_class\n\n\n\n\n    Rarely Vote\n—\n—\n\n\n    Sometimes Vote\n0.114\n0.108, 0.121\n\n\n    Always Vote\n0.294\n0.287, 0.301\n\n\ntreatment * voter_class\n\n\n\n\n    Civic Duty * Sometimes Vote\n0.014\n0.000, 0.030\n\n\n    Hawthorne * Sometimes Vote\n0.019\n0.004, 0.033\n\n\n    Self * Sometimes Vote\n0.030\n0.016, 0.044\n\n\n    Neighbors * Sometimes Vote\n0.042\n0.028, 0.056\n\n\n    Civic Duty * Always Vote\n-0.001\n-0.018, 0.015\n\n\n    Hawthorne * Always Vote\n0.025\n0.008, 0.042\n\n\n    Self * Always Vote\n0.026\n0.009, 0.042\n\n\n    Neighbors * Always Vote\n0.047\n0.029, 0.064\n\n\n\nSource: Gerber, Green, and Larimer (2008)\n    \n\n\n1 CI = Credible Interval\n    \n\n\n\n\nNow that we have a summarized visual for our data, let’s interpret the findings: * The intercept of fit_2 is the expected probability of voting in the upcoming election for a woman of average age (~ 50 years old in this data), who is assigned to the No Postcard group, and is a Rarely Voter. The estimate is 15.3%. * The coefficient of z_age, 0, implies a change of ~3.5% in likelihood of voting for each increment of one standard deviation (~ 14.45 years). For example: when comparing someone 50 years old with someone 65, the latter is about 3.5% more likely to vote. * Exposure to the Neighbors treatment shows a ~4.4% increase in voting likelihood for someone in the Rarely Vote category. Because of random assignment of treatment, we can interpret that coefficient as an estimate of the average treatment effect. * If someone were from a different voter classification, the calculation is more complex because we need to account for the interaction term. For example, for individuals who Sometimes Vote, the treatment effect of Neighbors is 0.1%. For Always Vote Neighbors, it is 0.1%."
  },
  {
    "objectID": "11-n-parameters.html#temperance",
    "href": "11-n-parameters.html#temperance",
    "title": "11  N Parameters",
    "section": "\n11.4 Temperance",
    "text": "11.4 Temperance\n\n\n\n\nTemperance\n\n\n\n\nFinally, let’s remember the virtue of Temperance. The gist of temperance is: be humble with our inferences, as our inferences are always, certainly, and unfortunately not going to match the real world. How does this apply to our shaming scenario?\nRecall our initial question: What is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?\nTo answer the question, we want to look at different average treatment effects for each treatment and type of voting behavior. In the real world, the treatment effect for person A is almost always different than the treatment effect for person B.\nIn this section, we will create a plot that displays the posterior probability distributions of the average treatment effects for men of average age across all combinations of 4 treatments and 3 voter classifications. This means that we are making a total of 12 inferences.\nImportant note: We could look at lots of ages and both Male and Female subjects. However, that would not change our estimates of the treatment effects. The model is linear, so terms associated with z_age and sex disappear when we do the subtraction. This is one of the great advantages of linear models.\nTo begin, we will need to create our newobs object.\n\n# Because our model is linear, the terms associated with z_age and sex disappear\n# when we perform subtraction. The treatment effects calculated thereafter will\n# not only apply to males of the z-scored age of ~ 50 years. The treatment\n# effects apply to all participants, despite calling these inputs.\n\n\nsex <- \"Male\"\nz_age <- 0\ntreatment <- c(\"No Postcard\",\n               \"Civic Duty\",\n               \"Hawthorne\",\n               \"Self\",\n               \"Neighbors\")\nvoter_class <- c(\"Always Vote\",\n                 \"Sometimes Vote\",\n                 \"Rarely Vote\")\n\n# This question requires quite the complicated tibble! Speaking both\n# hypothetically and from experience, keeping track of loads of nondescript\n# column names after running posterior_epred() while doing ATE calculations\n# leaves you prone to simple, but critical, errors. expand_grid() was created\n# for cases just like this - we want all combinations of treatments and voter\n# classifications in the same way that our model displays the interaction term\n# parameters.\n\nnewobs <- expand_grid(sex, z_age, treatment, voter_class) |> \n  \n  # This is a handy setup for the following piece of code that allows us to\n  # mutate the ATE columns with self-contained variable names. This is what\n  # helps to ensure that the desired calculations are indeed being done. If you\n  # aren't familiar, check out the help page for paste() at `?paste`.\n  \n  mutate(names = paste(treatment, voter_class, sep = \"_\"))\n\npe <- posterior_epred(fit_2,\n                        newdata = newobs) |> \n  as_tibble() |> \n  \n  # Here we can stick the names that we created in newobs onto the otherwise\n  # unfortunately named posterior_epred() output. \n  \n  set_names(newobs$names)\n\nNow that we have our newobs to work with, we will need to create an object named plot_data that collects the treatment effect calculations.\nRecall that, when calculating a treatment effect, we need to subtract the estimate for each category from the control group for that category. For example, if we wanted to find the treatment effect for the Always Vote Neighbors group, we would need: Always Vote Neighbors - Always Vote No Postcard.\nTherefore, we will use mutate() twelve times, for each of the treatments and voting frequencies. After, we will pivot_longer in order for the treatment effects to be sensibly categorized for plotting. If any of this sounds confusing, read the code comments carefully.\n\nplot_data <- pe |> \n  \n  # Using our cleaned naming system, ATE calculations are simple enough. Note\n  # how much easier the code reads because we have taken the trouble to line up\n  # the columns.\n  \n  mutate(`Always Civic-Duty`    = `Civic Duty_Always Vote`     - `No Postcard_Always Vote`,\n         `Always Hawthorne`     = `Hawthorne_Always Vote`      - `No Postcard_Always Vote`,\n         `Always Self`          = `Self_Always Vote`           - `No Postcard_Always Vote`,\n         `Always Neighbors`     = `Neighbors_Always Vote`      - `No Postcard_Always Vote`,\n         `Sometimes Civic-Duty` = `Civic Duty_Sometimes Vote`  - `No Postcard_Sometimes Vote`,\n         `Sometimes Hawthorne`  = `Hawthorne_Sometimes Vote`   - `No Postcard_Sometimes Vote`,\n         `Sometimes Self`       = `Self_Sometimes Vote`        - `No Postcard_Sometimes Vote`,\n         `Sometimes Neighbors`  = `Neighbors_Sometimes Vote`   - `No Postcard_Sometimes Vote`,\n         `Rarely Civic-Duty`    = `Civic Duty_Rarely Vote`     - `No Postcard_Rarely Vote`,\n         `Rarely Hawthorne`     = `Hawthorne_Rarely Vote`      - `No Postcard_Rarely Vote`,\n         `Rarely Self`          = `Self_Rarely Vote`           - `No Postcard_Rarely Vote`,\n         `Rarely Neighbors`     = `Neighbors_Rarely Vote`      - `No Postcard_Rarely Vote`) |> \n  \n  # This is a critical step, we need to be able to reference voter\n  # classification separately from the treatment assignment, so pivoting in the\n  # following manner reconstructs the relevant columns for each of these\n  # individually. \n  \n  pivot_longer(names_to = c(\"Voter Class\", \"Group\"),\n               names_sep = \" \",\n               values_to = \"values\",\n               cols = `Always Civic-Duty`:`Rarely Neighbors`) |> \n  \n    # Reordering the factors of voter classification forces them to be displayed\n    # in a sensible order in the plot later.\n  \n    mutate(`Voter Class` = fct_relevel(factor(`Voter Class`),\n                                     c(\"Rarely\",\n                                       \"Sometimes\",\n                                       \"Always\")))\n\nFinally, we will plot our data! Read the code comments for explanations on aesthetic choices, as well as a helpful discussion on fct_reorder().\n\nplot_data  |> \n  \n  # Reordering the y axis values allows a smoother visual interpretation - \n  # you can see the treatments in sequential ATE.\n  \n  ggplot(aes(x = values, y = fct_reorder(Group, values))) +\n  \n  # position = \"dodge\" is the only sure way to see all 3 treatment distributions\n  # identity, single, or any others drop \"Sometimes\" - topic for further study\n  \n    stat_slab(aes(fill = `Voter Class`),\n              position = 'dodge') +\n    scale_fill_calc() +\n  \n    # more frequent breaks on the x-axis provides a better reader interpretation\n    # of the the shift across age groups, as opposed to intervals of 10%\n    \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1),\n                       breaks = seq(-0.05, 0.11, 0.01)) +\n    labs(title = \"Treatment Effects on The Probability of Voting\",\n         subtitle = \"Postcards work less well on those who rarely vote\",\n         y = \"Postcard Type\",\n         x = \"Average Treatment Effect\",\n         caption = \"Source: Gerber, Green, and Larimer (2008)\") +\n    theme_clean() +\n    theme(legend.position = \"bottom\")\n\n\n\n\nThis is interesting! It shows us a few valuable bits of information:\n\nWe are interested in the average treatment effect of postcards. There are 4 different postcards, each of which can be compared to what would have happened if the voter did not receive any postcard.\nThese four treatment effects, however, are heterogeneous. They vary depending on an individual’s voting history, which we organize into three categories: Rarely Vote, Sometimes Vote and Always Vote. So, we have 12 different average treatment effects, one for each possible combination of postcard and voting history.\nFor each of these combinations, the graphic shows our posterior distribution.\n\nWhat does this mean for us, as we consider which postcards to send? * Consider the highest yellow distribution, which is the posterior distribution for the average treatment effect of receiving the Neighbors postcard (compared to not getting a postcard) for Always Voters. The posterior is centered around 9% with a 95% confidence interval of, roughly, 8% to 10%. * Overall, the Civic Duty and Hawthorne postcards had small average treatment effects, across all three categories of voter. The causal effect on Rarely Voters was much smaller, regardless of treatment. It was also much less precisely estimated because there were many fewer Rarely Voters in the data. *The best way to increase turnover, assuming there are limits to how many postcards you can send, is to focus on Sometimes/Always voters and to use the Neighbors postcard.\nConclusion: If we had a limited number of postcards, we would send the Neighbors postcard to citizens who already demonstrate a tendency to vote.\nHow confident are we in these findings? If we needed to convince our boss that this is the right strategy, we need to explain how confident we are in our assumptions. To do that, we must understand the three levels of knowledge in the world of posteriors.\n\n11.4.1 The Three Levels of Knowledge\nThere exist three primary levels of knowledge possible knowledge in our scenario: the Truth (the ideal Preceptor Table), the DGM Posterior, and Our Posterior.\n\n11.4.1.1 The Truth\nIf we know the Truth (with a capital “T”), then we know the ideal Preceptor Table. With that knowledge, we can directly answer our question precisely. We can calculate each individual’s treatment effect, and any summary measure we might be interested in, like the average treatment effect.\nThis level of knowledge is possible only under an omniscient power, one who can see every outcome in every individual under every treatment. The Truth would show, for any given individual, their actions under control, their actions under treatment, and each little factor that impacted those decisions.\nThe Truth represents the highest level of knowledge one can have — with it, our questions merely require algebra. There is no need to estimate a treatment effect, or the different treatment effects for different groups of people. We would not need to predict at all — we would know.\n\n11.4.1.2 DGM posterior\nThe DGM posterior is the next level of knowledge, which lacks the omniscient quality of The Truth. This posterior is the posterior we would calculate if we had perfect knowledge of the data generating mechanism, meaning we have the correct model structure and exact parameter values. This is often falsely conflated with “Our posterior”, which is subject to error in model structure and parameter value estimations.\nWith the DGM posterior, we could not be certain about any individual’s causal effect, because of the Fundamental Problem of Causal Inference. In other words, we can never measure any one person’s causal effect because we are unable to see a person’s resulting behavior under treatment and control; we only have data on one of the two conditions.\nWhat we do with the DGM posterior is the same as Our posterior — we estimate parameters based on data and predict the future with the latest and most relevant information possible. The difference is that, when we calculate posteriors for an unknown value in the DGM posterior, we expect those posteriors to be perfect.\nIf we go to our boss with our estimates from this posterior, we would expect our 95% confidence interval to be perfectly calibrated. That is, we would expect the true value to lie within the 95% confidence interval 95% of the time. In this world, we would be surprised to see values outside of the confidence interval more than 5% of the time.\n\n11.4.1.3 Our posterior\nUnfortunately, Our posterior possesses even less certainty! In the real world, we don’t have perfect knowledge of the DGM: the model structure and the exact parameter values. What does this mean?\nWhen we go to our boss, we tell them that this is our best guess. It is an informed estimate based on the most relevant data possible. From that data, we have created a 95% confidence interval for the treatment effect of various postcards. We estimate that the treatment effect of the Neighbors postcard to be between 8% to 10%.\nDoes this mean we are certain that the treatment effect of Neighbors is between these values? Of course not! As we would tell our boss, it would not be shocking to find out that the actual treatment effect was less or more than our estimate.\nThis is because a lot of the assumptions we make during the process of building a model, the processes in Wisdom, are subject to error. Perhaps our data did not match the future as well as we had hoped. Ultimately, we try to account for our uncertainty in our estimates. Even with this safeguard, we aren’t surprised if we are a bit off.\nFor instance, would we be shocked if the treatment effect of the Neighbors postcard to be 7%? 12%? Of course not! That is only slightly off, and we know that Our posterior is subject to error. Would we be surprised if the treatment effect was found to be 20%? Yes. That is a large enough difference to suggest a real problem with our model, or some real world change that we forgot to factor into our predictions.\nBut, what amounts to a large enough difference to be a cause for concern? In other words, how wrong do we have to be in a one-off for our boss to be suspicious? When is “bad luck” a sign of stupidity? We will delve into this question in the next section of our chapter.\n\n11.4.1.4 Bad luck or bad work?\nIn any one problem, it is hard to know if we were “right,” if our posterior was similar to the DGM posterior. After all, 5% of the time the answer is outside the 95% confidence interval. But if the truth ends up very, very far away from the median of our posterior, our boss will be rightly suspicious. How many MAD SDs or standard errors away do we have to be from the truth before we are obviously a fool?\nThere are many ways to judge a forecast. Here, we’re looking at two main things: the calibration of a forecast — that is, whether events that we said would happen 30 percent of the time actually happened about 30 percent of the time — and how our forecast compared with an unskilled estimate that relies solely on historical averages. We can answer those questions using calibration plots and skill scores, respectively. These concepts are a bit too advanced for this course, but their foundations are important to understand.\nCalibration plots compare what we predicted with what actually happened. Single predictions can be difficult to judge on their own, so we often want to group many predictions together in bins and plot the averages of each bin’s forecasted increase in voting against the actual increase in voting. If our forecasts are well-calibrated, then all of the bins on the calibration plot will be close to the 45 degree line; if our forecast was poorly calibrated, the bins will be further away. Our second tool, skill scores, lets us evaluate our forecasts even further, combining accuracy and an appetite for risk into a single number.\nBrier skill scores tell us how much more valuable our forecasts are than an unskilled estimate, one that is informed by historical averages — e.g., a guess that a postcard will increase voting by 5%.\nThese are the technical ways that we can judge our own work’s accuracy. Our boss will likely judge using other methods.\nFor instance, if we answer many questions (by creating many posteriors for different problems) then, over time, our boss will get a sense of our actual skill, both because our median should be above and below the truth about the same proportion and because our confidence intervals should be correctly calibrated.\nWe know, from experience, that our posteriors are often too narrow. They assume that we know the DGM when, in fact, we know that we do not. What do we do with that knowledge? First, we prepare our boss for this fact. This is the humility in Temperance. Second, we estimate dozens of different models and combine their posteriors. The result might very well have the same median as your correct posterior, but the confidence intervals would be much wider. These concepts are more advanced than the Primer, but they are important to consider when making predictions."
  },
  {
    "objectID": "11-n-parameters.html#summary",
    "href": "11-n-parameters.html#summary",
    "title": "11  N Parameters",
    "section": "\n11.5 Summary",
    "text": "11.5 Summary\n\nUse the tidy() function from the broom.mixed package to make models with \\(N\\) parameters easier to interpret.\nA function we are familiar with, stan_glm(), is used to create models with \\(N\\) parameters.\nIt is important to remember that the data does not equal the truth.\nThe population we would like to make inferences about is not the population for which we have data. It is a matter of wisdom whether the data we do have maps closely enough to the population we are studying.\nWhen dealing with models with many parameters, double check that you know how to find the true slope and intercepts — often, this requires adding numerous values to the coefficient you are studying."
  },
  {
    "objectID": "set-up.html#computer-set-up",
    "href": "set-up.html#computer-set-up",
    "title": "Set Up for Working on The Primer",
    "section": "Computer Set Up",
    "text": "Computer Set Up\n\nRead the Getting Started chapter from The Primer. Follow the instructions, including installing Rtools if you are using Windows. Read (and watch the videos from) Getting Used to R, RStudio, and R Markdown by Chester Ismay and Patrick C. Kennedy. Note that R Markdown is the predecessor to Quarto. Check out RStudio Essentials Videos. Most relevant for us are “Writing code in RStudio”, “Projects in RStudio” and “Github and RStudio”. The best reference for R/RStudio/Git/Github issues is always Happy Git and GitHub for the useR.\nMake sure that your Git/Github connections are good. If you have gone through the key chapters in Happy Git with R — as you should have — then these may already be OK. If not (or, even if you have), then you need to run usethis::git_sitrep().\n\n> library(usethis)   \n> git_sitrep()    \nGit config (global)   \n● Name: 'David Kane'   \n● Email: 'dave.kane@gmail.com'   \n● Vaccinated: FALSE   \nℹ See `?git_vaccinate` to learn more   \nℹ Defaulting to https Git protocol   \n● Default Git protocol: 'https'   \nGitHub   \n● Default GitHub host: 'https://github.com'   \n● Personal access token for 'https://github.com': '<discovered>'   \n● GitHub user: 'davidkane9'   \n● Token scopes: 'delete_repo, gist, notifications, repo, user, workflow'   \n● Email(s): 'dave.kane@gmail.com (primary)', 'dkane@fas.harvard.edu'   \n...   \nI left out the end of the output.\nIf the first part — Git config — seems messed up, execute (with your information):\n\nuse_git_config(user.name = \"David Kane\", user.email = \"dave.kane@gmail.com\")\n\nIf the second part seems messed up, try:\n\nusethis::create_github_token()\n\nand read about Github credentials. After you do, restart R and then run git_sitrep() again to make sure that things look like mine, more or less.\n\nInstall the renv package. You can read about the renv package here.\n\nIt is not critical to understand all the details of how renv works. The big picture is that it creates a set of libraries which will be used just for this project and whose versions are kept in sync between you and me.\n\nAt this point, you should have all the tools you need to contribute. If you have never done a pull request, however, you will need to learn more. Start by reading the help page. Read the whole thing! Don’t just skim it. These are important concepts for professional-level workflow. The usethis package is mostly providing wrappers around the underlying git commands. If you want to understand what is happening at a lower level, read this, but doing so is optional.\n\nAgain, with luck, you will only have to do these steps once.\n\nProve to yourself (and to me) that your set up is working by submitting a pull request to me which simply adds your name to the top of the all.primer.tutorials TODO.txt file. (See below for how to do this.) Email me to set up our next meeting after you do this."
  },
  {
    "objectID": "set-up.html#project-set-up",
    "href": "set-up.html#project-set-up",
    "title": "Set Up for Working on The Primer",
    "section": "Project Set Up",
    "text": "Project Set Up\nYou will need to do the below steps at least one time. It is more likely, however, that you will do them dozens of times. If things are working, great! If they start not working, you can try to diagnose the problem. But, if you can’t, then you are in a nuke it from orbit scenario, which means that you start by deleting the current version of the package from two places: your computer, and your Github account. To delete a project from your computer, put the R Studio project directory in the Trash. Make sure to also close out of the R Studio session after you delete it. If for some reason you cannot completely remove it, consider using the command $sudo rm -r dirname where you replace dirname with the path to the project on your computer. sudo and rm can be extremely dangerous when used together, so make sure to double check the command and/or do additional research. After you successfully remove the R project from your computer, go to your Github account and then go to Settings to delete the repo.\nKey steps:\n\nFork/download the target repo:\n\n\nlibrary(usethis)  \ncreate_from_github(\"PPBDS/primer\",   \n                    fork = TRUE,   \n                    destdir = \"/Users/davidkane/Desktop/projects/\",   \n                    protocol = \"https\")  \n\nThat is the repo for working on the book. If you are working on PPBDS/primer.data or PPBDS/all.primer.tutorials, you need to create_from_github() using those repos. You must change destdir to be a location on your computer. Indeed, professionals will generally have several different RStudio sessions open, each working on a different R project/package, each of which is connected to its own Github repo.\nFor your education, it is worth reading the help page for create_from_github(). The fork and protocal arguments may not really be necessary and, obviously, you should place the project in the location on your computer in which your other projects live. The command first forks a copy of PPBDS/primer to your Github account and then clone/downloads that fork to your computer.\nThis may seem like overkill, but, as Pro Git explains, it is how (essentially) all large projects are organized. With luck, you only have to issue this command once. After that, you are always connected, both to your fork and to the true repos, which live at github/com/PPBDS. Also, note that, if something ever gets totally messed up on your computer, you can just delete the project folder on your computer and the repo on your Github account and then start again. (If you have made changes that you don’t want to lose, just save the files with those changes to one side and then move them back after you have recreated the project.)\nNote that this command should automatically put you in a new RStudio session with the primer (or all.primer.tutorials or primer.data) RStudio project which resides on your computer\n\nThe next step is to get renv setup so that you are running the same package versions as everyone else. (This does not apply to the all.primer.tutorials package, which does not use renv.) Run this once:\n\n\nlibrary(renv)\nrenv::restore()\n\nThis will install all the packages you need in the directory of this project. (This has no effect on your main library of R packages.) Restart your R session. Again, this means that you now have two separate installations of, for example, ggplot2. One is in the default place which your R sessions is by default pointed to. (In a different project without a renv directory, you can run .libPaths() to see where that is.) The second place that ggplot2 is installed is in the renv directory which lives in this project.\nNote that, for the most part, you won’t do anything with renv after this initial use. If you use error = TRUE in any code chunk, you will also need renv.ignore = TRUE in that code chunk, or you will get an annoying warning because renv can’t parse the code in that chunk.\nHowever, there are three other renv commands you might issue:\nrenv::status() just reports if anything is messed up. It won’t hurt anything.\nrenv::restore() looks at the renv.lock file and installs the packages it specifies. You will need to do this when I make a change to renv.lock, e.g., if I upgrade our version of ggplot2 or add a new package.\nrenv::snapshot() should only be issued if you know what you are doing. This changes the renv.lock file, which is something that, usually, only I do. Most common case for use would be if you need to add a new package to the project.\nBecause the all.primer.tutorials does not use renv, we need to take care that we are using up to date versions of all the packages.\n\nCreate a branch to work from:\n\n\npr_init(branch = \"chapter-9\")\n\nMake sure the branch name is sensible. Again, this is a command that you only need to issue once, at least for our current workflow. You should always be “on” this branch, never on the default (master) branch. You can check this in the upper right corner of the git panel on R Studio.\nIn more professional settings, you will often work on several different branches at once. So, if you are comfortable, you should feel free to create more than one branch, use it, delete it and so on. Never work on the default branch, however. And, if you use multiple branches, be careful where you are and what you are doing."
  },
  {
    "objectID": "set-up.html#daily-work",
    "href": "set-up.html#daily-work",
    "title": "Set Up for Working on The Primer",
    "section": "Daily Work",
    "text": "Daily Work\n\nPull regularly:\n\n\npr_merge_main()\n\nIssue this command all the time. This is how you make sure that your repo and your computer is updated with the latest changes that have been made in the book. The word “upstream” is associated with the repos at PPBDS. The word “origin” is associated with the fork at your Github account. But, in general, you don’t need to worry about this. Just pull every time you sit down. (Just clicking the pull button is not enough. That only pulls from your repo, to which no changes have been made. It does not pull from PPBDS/primer, et al.) You issue this command multiple times a day.\n\nMake changes in the file you are editing. Knit to make sure the changes work. Commit with a message. Push to the repo on your Github account. And so on.\n\nAt some point, you will be ready to push to the PPBDS organization. However, you can’t do this directly. Instead, you must submit a pull request (PR). Because you are part of a larger project, these commands are slightly different than what you have done before, which has usually just been clicking on the pull (blue) and push (green) arrows in the Git pane in RStudio.\n\nIssue pull requests every few days, depending on how much work you have done and/or whether other people are waiting for something you have done.\n\n\npr_push()\n\nThis command bundles up a bunch of git commands (which you could do by hand) into one handy step. This command does everything needed to create a “pull request” — a request from you to me that I accept the changes you are proposing into the repo at PPBDS/primer — and then opens up the web page to show you. But your are not done! You must PRESS the green button on that web page, sometimes twice. Until then, the PR has not actually been created. pr_push() just does everything before that. The “pr” in pr_push() stands for pull request.\n\nI will leave aside for now issues associated with the back-and-forth discussions we might have around your pull request. I will probably just accept it. Your changes will go into the repos at PPBDS and then be distributed to everyone else when they run pr_merge_main().\nYou can now continue on. There is no need to wait for me to deal with your pull request. There is no need to fork/clone/download again. You don’t need to create a new branch, although many people do, with a branch name which describes what they are working on now. You just keep editing your files, knitting, and committing then pushing to your forked repo. When you feel you have completed another chunk of work, just run pr_push() again.\nRead the usethis setup help page at least once, perhaps after a week or two of working within this framework. It has lots of good stuff!"
  },
  {
    "objectID": "set-up.html#common-problems",
    "href": "set-up.html#common-problems",
    "title": "Set Up for Working on The Primer",
    "section": "Common Problems",
    "text": "Common Problems\n\n\n\nIn the immediate aftermath of this creation process, the blue/green arrows (in the Git panel) for pulling/pushing may be grayed out. This is a sign that the connection between your computer and your forked repo has not “settled in.” (I am not sure of the cause or even if this is the right terminology.) I think that just issuing your first pr_merge_main() fixes it. If not, it always goes away. Until it does, however, you can’t pull/push to your repo. That doesn’t really matter, however, since the key commands you need are pr_merge_main() and pr_push(), both of which always work immediately.\nAfter running pr_merge_main(), you will often see a bunch of files in your Git tab in the top right corner of Rstudio marked with an M (for Modified), including files which you know you did not edit. These are the files that have been updated on the “truth” — on PPBDS/primer — since your last pr_merge_main(). Since you pulled them directly from the PPBDS/primer repo, your forked repo sees all the changes other people have made and thinks that you made them. This is easily fixed, however — just commit all the changes to your forked repo. (Strangely, this seems to not always happen. If you don’t see this effect, don’t worry.)\nAlways run pr_merge_main() before committing a file. Otherwise, you may create lots of merge conflicts. If this happens, save a copy of the file(s) you personally were editing off to the side. Then, nuke it from orbit, following the instructions above. Repeat the Project Set Up process. Then move in your file(s) by hand into the new repo, and commit/push them as normal.\nWhen you submit a pull request to merge your work with the PPBDS repo, it won’t always be smiles and sunshine — every once in a while, you’ll run into merge conflicts. When these arise, it is because two parties work on a file separately and submit conflicting changes. This makes it hard for GitHub to “merge” your version with the other version. When this happens, find multiple adjacent “>”, “<”, and “=” signs in your document — these will show you where the conflicts occur. For more background on merge conflicts, read this.\n\nIf you see the above-mentioned conflicts in your document, do not submit a pull request. This will mess things up. Instead, first, go through your document, and make sure all the weird conflict indicators (<, >, and =) are removed. Second, decide what goes in that space. It might be the stuff you wrote. It might be the other stuff. It might be some combination of the two which you decide on. Whatever happens, you are making an affirmative choice about what should appear in the file at that location. Once all the merge conflicts are fixed, run pr_push() again.\n\npr_push() can be tricky. First, note that, if I have not accepted a (p)ull (r)equest which you have submitted, then your PR is still open. You can see it on Github. In fact, you can see all the closed/completed pull requests as well. If, while one PR is still open, you submit another pr_push(), then this will just be added to your current PR. And that is OK! We don’t need it to be separate.\n\nBut even if there is not an open PR, pr_push() can be tricky. The key thing to remember is that you must press a green button on Github for a new PR to be created. Normally, this is easy. Running pr_push() automatically (or perhaps after you run pr_view()) puts you in a browser and brings you to the correct Github page. Press the button and – presto! – you have created a PR. But, sometimes, the web page is different. It actually sends you back to an old pull request. When this happens, you need to click on the “Pull Request” tab above. This will take you to a new page, with a green button labeled “Compare & Pull Request”. Press that button.\n\nIf you end up needing to install a new package — which should be rare — just install it with renv::install()and then type renv::status() to confirm than renv is aware of the change. Then, type renv::snapshot(). This will update the renv.lock file to include the new package. You just commit/push the new version of renv.lock, and that shares the information with everyone else on the project. Never commit/push a modified renv.lock unless you know why it has changed. But, for the most part, leave changes in renv.lock to me.\nBe careful of committing garbage files like “.DS_Store”, which is a file created sometimes. Only commit changes which you understand. In the vast majority of cases your PRs will only involve one or two files.\nIf using Windows, make sure you have RTools installed.\nSometimes, your renv library becomes messed up. Note that nuking from orbit may not fix this because renv installs your packages in some private common area which is not impacted when you reinstall the PPBDS package you are working with. In this case, running renv::rebuild() may help. If not, run renv::diagnostics(), find the path to the renv cache directory and delete everything there by hand. Then, run renv::repair()'."
  },
  {
    "objectID": "set-up.html#style-guide",
    "href": "set-up.html#style-guide",
    "title": "Set Up for Working on The Primer",
    "section": "Style Guide",
    "text": "Style Guide\n\nNever use just a single # after using it for the chapter title. The first subpart uses a ##. There should be 5 to 8 subparts for each chapter. Within each subpart, you may have sub-subparts, indicated with ###. There should be 3 to 10 of those. You may use #### if you like.\nSection headings (other than Chapter titles) are in sentence case (with only the first word capitalized, unless it is something always capitalized) rather than title case (in which all words except small words like “the” and “of” are capitalized). Chapter titles are in title case. Headings do not end with a period.\nNever hard code stuff like “A tibble with 336,776 rows and 19 columns.” What happens when you update the data? Instead, calculate all numbers on the fly, with “r scales::comma(x)” whenever x is a number in the thousands or greater. Example: “A tibble with ‘r scales::comma(nrow(x))’ rows and ‘r ncol(x)’ columns.”\n“We” are writing this book.\nPackage names are in bold: ggplot2 is a package for doing graphics. In general, we reserve bolding for package names. Use italics for emphasis in other contexts.\nR code, anything you might type in the console, is always within backticks. Example: mtcars is a built-in dataset.\nFunction names always include the parentheses: we write pivot_wider(), not pivot_wider.\nAdd lots of memes and videos and cartoons.\nMake ample use of comments, placed with the handy CMD-Shift-/ shortcut. These are notes for everyone else working on the chapter, and for future you.\nAll tables should be created with the gt package.\nAll images and gifs are loaded with knitr::include_graphics().\nInterim data sets should be called x or something sensible to the situation, like ch7 for a data set you are working with in Chapter 7. Do not use names like data and df, both of which are R commands.\nStudents are sometimes tentative. Don’t be! Edit aggressively. If you don’t like what is there, delete it. (If I disagree with your decision, I can always get the text back from Github.) Move things around. Make the chapter yours, while keeping to the style of the other chapters. Note that 90% of the prose here was not written by me. Cut anything you don’t like.\nIf you make an mp4, you can convert it to .gif using https://convertio.co/mp4-gif.\nEverything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potential outcomes to define precisely what “true value” you are talking about. And so on.\n\n\nStray thoughts\nEvery chapter 5+ begins with a problem, and the decision we must make. These are often toy, highly stylized problems. The decisions are not realistic. But, in structure, these problems parallel the real problems that people face, the actual decisions which they must make.\nThe problem is specified at the end of the “preamble,” the untitled part of the chapter after the title and before the first subpart. Example from Chapter 8:\n\nA person arrives at a Boston commuter station. The only thing you know is their political party. How old are they? Two people arrive: A Democrat and a Republican. What are the odds that the Democrat is 10% older than the Republican?\n\n\nA different person arrives at the station. You know nothing about them. What will their attitude toward immigration be if they are exposed to Spanish-speakers on the platform? What will it be if they are not? How certain are you?\n\nIs this an actual problem that someone might face? No! But it is like such problems. The first requires the creation of a predictive model. The second necessitates a causal model. The rest of the chapter teaches the reader how to create such models. The end of the chapter harkens back to the questions from the beginning.\nMight it be nice to put more meat on the story than that? Perhaps. In an ideal world, the “decision” you faced would be more complex than just playing the prediction game. Begin with a decision. What real world problem are you trying to solve? What are the costs and benefits of different approaches? What unknown thing are you trying to estimate? With Sampling, it might be: How many people should I call? With estimating one parameter — like vote share as the ballots come in — it might be: How much should I bet on the election outcome?\nThe data we have might not be directly connected to our problem. For example, we might be running a Senate campaign and trying to decide what to spend money on. The Spanish-speakers-on-a-train-platform data set is not directly related to that problem, but it isn’t unrelated. Indeed, the first theme of “validity” is directly related to this issue: Is the data we have relevant to the problem we want to solve?"
  },
  {
    "objectID": "style-guide.html#comments",
    "href": "style-guide.html#comments",
    "title": "Style Guide",
    "section": "Comments",
    "text": "Comments\nInclude comments in your code. Easy-to-understand chunks of code should not have comments. The code is the comment. But other code will merit many, many lines of comments, more lines than the code itself. In a given file, you should have about as many total lines of comments as you have lines of code.\nMake your comments meaningful. They should not be a simple description of what your code does. The best comments are descriptions about why you did what you did and which other approaches you tried or considered. (The code already tells us what you did.) Good comments often have a “Dear Diary” quality: “I did this. Then I tried that. I finally chose this other thing because of reasons X, Y and Z. If I work on this again, I should look into this other approach.” Because of this, the structure is often a paragraph of comments followed by several lines of code.\nEach line of a comment should begin with the comment symbol (a “hash”) followed by a single space: #. Code comments must be separated from code by one empty line on both sides. Format your code comments neatly. Place your cursor in the comment block and hit Ctrl-Shift-/ to format the comment automatically. label your R code chunks, without using weird characters or spaces. download_data is a good R code chunk label. Plot #1 is not.\nSpelling matters. Comments should be constructed as sentences, with appropriate capitalization and punctuation."
  },
  {
    "objectID": "style-guide.html#graphics",
    "href": "style-guide.html#graphics",
    "title": "Style Guide",
    "section": "Graphics",
    "text": "Graphics\nUse titles, subtitles, axis labels, captions and so on to make it clear what your graphics mean.\nAnytime you make a graphic without a title (explaining what the graphic is), a subtitle (highlighting a key conclusion to draw), a caption (with some information about the source of the data) and axis labels (with information about your variables), you should justify that decision in a code comment. We (try to) always include these items but there are situations in which doing so makes less sense. Ultimately, these decisions are yours, but the readers of your code (including future-you) should understand your reasoning.\nUse your best judgment. For example, sometimes axis labels are unnecessary. Read Data Visualization: A practical introduction by Kieran Healy for guidance on making high quality graphics."
  },
  {
    "objectID": "style-guide.html#formating",
    "href": "style-guide.html#formating",
    "title": "Style Guide",
    "section": "Formating",
    "text": "Formating\nLong Lines\nLimit your code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font. When calling functions, you can omit the argument names for very common arguments (i.e. for arguments that are used in almost every invocation of the function). Short unnamed arguments can also go on the same line as the function name, even if the whole function call spans multiple lines.\nWhitespace\n|> should always have a space before it, and should usually be followed by a new line. After the first step in the pipe, each line should be indented by two spaces. This structure makes it easier to add new steps (or rearrange existing steps) and harder to overlook a step.\n\n# Good\n\niris |>\n  select(Species, Sepal.Length) |> \n  summarise(avg = mean(Sepal.Length),\n            .by = Species) |> \n  arrange(avg)\n\n# Bad\n\niris |> select(Species, Sepal.Length) |> summarise(avg = mean(Sepal.Length),\n            .by = Species) |> arrange(avg)\n\nggplot2 code is handled in a similar fashion. All commands after the initial invocation of ggplot() are indented.\n\n# Good\n\ndiamonds |> \n  ggplot(aes(x = depth)) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\n# Bad\n\ndiamonds |> \nggplot(aes(x = depth)) +\ngeom_histogram(bins = 100) + labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\nCommas\nAlways put a space after a comma, never before, just like in regular English.\n\n# Good\n\nx[, 1]\n\n# Bad\n\nx[,1]\nx[ ,1]\nx[ , 1]\n\nParentheses\nDo not put spaces inside or outside parentheses for regular function calls.\n\n# Good\n\nmean(x, na.rm = TRUE)\n\n# Bad\n\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )\n\nInfix operators\nMost infix operators (=, ==, +, -, <-, ~, et cetera) should be surrounded by one space.\n\n# Good\n\nheight <- (feet * 12) + inches\nmean(x, na.rm = TRUE)\ny ~ a + b\n\n\n# Bad\n\nheight<-feet*12+inches\nmean(x, na.rm=TRUE)\ny~a + b\n\nOther operators — like ::, :::, $, @, [, [[, ^, and : — should never be surrounded by spaces.\n\n# Good\n\nsqrt(x^2 + y^2)\ndf$z\nx <- 1:10\n\n# Bad\n\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx <- 1 : 10\n\nYou may add extra spaces if it improves alignment of = or <-.\n\nlist(total = a + b + c,\n     mean = (a + b + c) / n)\n\nDo not add extra spaces to places where space is not usually allowed."
  },
  {
    "objectID": "style-guide.html#messageswarningserrors",
    "href": "style-guide.html#messageswarningserrors",
    "title": "Style Guide",
    "section": "Messages/Warnings/Errors",
    "text": "Messages/Warnings/Errors\nR messages/warnings/errors should never appear in a submitted document. The right way to deal with these issues is to find out their cause and then fix the underlying problem. Students sometimes use “hacks” to make these messages/warnings/errors disappear. The most common hacks involve using code chunk options like message = FALSE, warning = FALSE, results = \"hide\", include = FALSE and others. Don’t do this, in general. A message/warning/error is worth understanding and then fixing. Don’t close your eyes (metaphorically) and pretend that the problem doesn’t exist. There are some situations, however, in which, no matter what you try, you can’t fix the problem. In those few cases, you can use one of these hacks, but you must make a code comment directly below it, explaining the situation. The only exception is the “setup” chunk (included by default in every new Rmd) which comes with include = FALSE. In that chunk, no explanation is necessary, by convention."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barfort, Sebastian, Robert Klemmensen, and Erik Gahner Larsen. 2020.\n“Longevity Returns to Political Office.” Political\nScience Research and Methods. https://doi.org/10.1017/psrm.2019.63.\n\n\nBryan, Jenny. 2019. STAT 545: Data Wrangling, Exploration, and\nAnalysis with r. https://stat545.com/.\n\n\nDiez, David M, Christopher D Barr, and Mine Çetinkaya-Rundel. 2014.\nIntroductory Statistics with Randomization and Simulation.\nFirst. Scotts Valley, CA: CreateSpace Independent Publishing Platform.\nhttps://www.openintro.org/stat/textbook.php?stat_book=isrs.\n\n\nDowney, Allen. 2012. Think Bayes: Bayesian Statistics Made\nSimple. Green Tea Press.\n\n\nEnos, Ryan D. 2014. “Causal Effect of Intergroup Contact on\nExclusionary Attitudes.” Proceedings of the National Academy\nof Sciences 111 (10): 3699–3704. https://doi.org/10.1073/pnas.1317670111.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data\nScience. First. Sebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science: Data\nAnalysis and Prediction Algorithms with r. First. Boca Raton, FL:\nCRC Press.\n\n\nKim, Albert Y., and Chester Ismay. 2019. Statistical Inference via\nData Science: A ModernDive into r and the Tidyverse. First. Boca\nRaton, FL: CRC Press.\n\n\nKuhn, Max, and Julia Silge. 2020. Tidy Modeling with r.\n\n\nLegler, Julie, and Paul Roback. 2019. Broadening Your Statistical\nHorizons: Generalized Linear Models and Multilevel Models.\n\n\nRobbins, Naomi. 2013. Creating More Effective Graphs. First.\nNew York, NY: Chart House.\n\n\nTimbers, Tiffany-Anne, Trevor Campbell, and Melissa Lee. 2021. Data\nScience: A First Introduction. https://ubc-dsci.github.io/introduction-to-datascience/.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  }
]